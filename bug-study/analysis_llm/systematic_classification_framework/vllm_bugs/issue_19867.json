{
  "issue_type": "bug",
  "extracted_from": "/root/yunwei37/vllm-exp/bug-study/analysis_llm/sample_results/vllm/label_based/bug/issues.json",
  "issue": {
    "number": 19867,
    "title": "[Bug]: AiterFlashAttentionImpl.__init__() got multiple values for argument 'use_irope' for llama4 model",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWe hit an exception on running llama4 models with latest code on ROCm V1:\n\n```\n(VllmWorker rank=2 pid=267) ERROR 06-19 01:00:39 [multiproc_executor.py:488] TypeError: AiterFlashAttentionImpl.__init__() got multiple values for argument 'use_irope'\n```\nCurrent work-around:\nTo turn off AITER_MHA, with VLLM_ROCM_USE_AITER_MHA=0\n\n\nProposal:\n\n- [ ] Fix the bug (the team is working on it)\n- [ ] Add a end-to-end test for one of the small llama4 models\n- [ ] \n\nThe motivation for adding an end to end test for a small version of llama4 models, is that we have seen issues of breaking llama4 models in the past because of lacking such tests.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "rocm"
    ],
    "state": "closed",
    "created_at": "2025-06-19T14:36:59+00:00",
    "closed_at": "2025-07-14T17:39:11+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19867/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/19867"
  }
}