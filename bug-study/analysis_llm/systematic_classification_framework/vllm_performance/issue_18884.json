{
  "issue_type": "performance",
  "extracted_from": "/root/yunwei37/vllm-exp/bug-study/analysis_llm/sample_results/vllm/label_based/performance/issues.json",
  "issue": {
    "number": 18884,
    "title": "[Performance]: The Unstable Performance Difference between CUDA and PyTorch",
    "body": "### Proposal to improve performance\n\n_No response_\n\n### Report of performance regression\n\n_No response_\n\n### Misc discussion on performance\n\nI have encountered such a problem\uff1aI implemented a custom CUDA operator for matrix multiplication and compared its time performance with PyTorch\u2019s einsum method. In a standalone Python test script, the execution time of the CUDA operator was significantly less than that of the einsum method. The code and results for the test time are as follows:\nimport pytest\nimport torch\nimport time\nfrom vllm._custom_ops import decode_matrix as decode_matrix_cuda\nfrom vllm.platforms import current_platform\n\ndef decode_matrix_torch(\n        a_sm: torch.Tensor,  # [NUM_TOKENS, NUM_HEADS, HEAD_SIZE]\n        q: torch.Tensor,  # [NUM_HEADS,NUM_TOKENS, HEAD_SIZE]\n        k_cache: torch.Tensor,  # [NUM_HEADS,NUM_TOKENS, HEAD_SIZE]\n        window_factors: torch.Tensor,  # [NUM_HEADS,1, 1]  \n):\n    a_sm = torch.einsum('hmd,hnd->hmn', q, k_cache) * (k_cache.shape[-1] ** -0.5)\n    return a_sm\n\n\nNUM_BATCH_TOKENS = [3, 256, 512, 613, 1024, 1536, 4096]\nNUM_QUERY_HEADS = [4, 8, 12, 32, 48, 64]\nHEAD_SIZES = [32, 48, 64, 96, 128, 256]\nDTYPES = [torch.float32, torch.half, torch.bfloat16]\n\n@pytest.mark.parametrize(\"num_tokens\", NUM_BATCH_TOKENS)\n@pytest.mark.parametrize(\"num_query_heads\", NUM_QUERY_HEADS)\n@pytest.mark.parametrize(\"head_size\", HEAD_SIZES)\n@pytest.mark.parametrize(\"output_dtype\", DTYPES)\n@torch.inference_mode()\ndef test_merge_attn_states(num_tokens: int, num_query_heads: int,\n                           head_size: int, output_dtype: torch.dtype):\n    if not current_platform.is_cuda():\n        pytest.skip('Currently only support compare triton merge_attn_states '\n                    'with custom cuda merge_attn_states kernel')\n\n    NUM_TOKENS = num_tokens\n    NUM_HEADS = num_query_heads\n    HEAD_SIZE = head_size\n\n    print(f\"\\nNUM_TOKENS:{NUM_TOKENS}, NUM_HEADS:{NUM_HEADS}, \"\n          f\"HEAD_SIZE:{HEAD_SIZE}, DTYPE: {output_dtype}, \"\n          f\"Device: {current_platform.get_device_name()}\")\n\n    # prefix_lse and suffix_lse contain inf and normal values\n    q = torch.randn(NUM_HEADS,1,HEAD_SIZE,\n                             dtype=torch.float32,\n                             device=\"cuda\")\n    \n    k_cache = torch.randn(NUM_HEADS,\n                             NUM_TOKENS,HEAD_SIZE,\n                             dtype=torch.float32,\n                             device=\"cuda\")\n    \n    window_factors = torch.randn(NUM_HEADS,\n                             1,1,\n                             dtype=torch.float32,\n                             device=\"cuda\")\n\n    output = torch.randn(NUM_HEADS,\n                             1,NUM_TOKENS,\n                             dtype=torch.float32,\n                             device=\"cuda\")\n\n\n    warmup_times = 2\n    repeat_times = 20\n\n    output_torch = output.clone()\n    total_time_torch_kernel = 0\n    total_time_torch = 0\n    start = torch.cuda.Event(enable_timing=True)\n    end = torch.cuda.Event(enable_timing=True)\n\n    # 0. Run the Torch kernel\n    q_torch = q.clone()\n    k_cache_torch = k_cache.clone()\n    window_factors_torch = window_factors.clone()\n    for _ in range(warmup_times):\n        output_torch = decode_matrix_torch(\n            output_torch, q_torch, k_cache_torch, window_factors_torch)\n    torch.cuda.synchronize()\n\n    for _ in range(repeat_times):\n        start.record()\n        start_time = time.time()\n        output_torch = decode_matrix_torch(\n            output_torch, q_torch, k_cache_torch, window_factors_torch)\n        \n\n        end_time = time.time()\n        end.record()\n        torch.cuda.synchronize()\n        total_time_torch_kernel += start.elapsed_time(end)\n        total_time_torch += (end_time-start_time)\n\n    avg_time_torch_kernel = total_time_torch_kernel / repeat_times\n    avg_time_torch = total_time_torch / repeat_times\n\n\n    # 2. Run the CUDA kernel\n    total_time_cuda_kernel = 0\n    total_time_cuda = 0\n    output_cuda = output.clone()\n    \n    for _ in range(warmup_times):\n        decode_matrix_cuda(q, k_cache,\n                               window_factors, output_cuda)\n    torch.cuda.synchronize()\n\n    for _ in range(repeat_times):\n        start.record()\n        start_time = time.time()\n        decode_matrix_cuda(q, k_cache,\n                               window_factors, output_cuda)\n        end_time = time.time()\n        end.record()\n        torch.cuda.synchronize()\n        total_time_cuda_kernel += start.elapsed_time(end)\n        total_time_cuda += (end_time-start_time)\n\n    avg_time_cuda_kernel = total_time_cuda_kernel / repeat_times\n    avg_time_cuda = total_time_cuda / repeat_times\n\n    # 3. Performance compare\n    performance_improved = avg_time_torch_kernel / avg_time_cuda_kernel\n    print(f\" Torch_kernel time: {avg_time_torch_kernel:.6f}ms, \"\n          f\"Torch time: {avg_time_torch:.6f}ms \")\n\n    print(f\"  CUDA_kernel time: {avg_time_cuda_kernel:.6f}ms, \"\n          f\"CUDA time: {avg_time_cuda:.6f}ms, \"\n          f\"Performance: {performance_improved:.5f}x\")\n    print(\"-\" * 100)\n\ntest_merge_attn_states(256,12,128,torch.float32)\n\n![Image](https://github.com/user-attachments/assets/50bf468c-f922-4187-a63b-43de1b0fc77a)\n\nHowever, when integrating this CUDA operator into the forward function of the model file in VLLM, its performance became unstable: sometimes the execution time of the CUDA operator was longer than that of the einsum method, and sometimes it was shorter. The results for the test time are as follows:\n\n![Image](https://github.com/user-attachments/assets/169f41cc-76a5-425c-bed3-418973f8a1ab)\n\nWhat factors could be causing this phenomenon?\n\n### Your current environment (if you think it is necessary)\n\n```text\nThe output of `python collect_env.py`\n```\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "performance"
    ],
    "state": "open",
    "created_at": "2025-05-29T06:52:56+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18884/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/18884"
  }
}