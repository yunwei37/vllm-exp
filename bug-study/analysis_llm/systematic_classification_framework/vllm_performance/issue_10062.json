{
  "issue_type": "performance",
  "extracted_from": "/root/yunwei37/vllm-exp/bug-study/analysis_llm/sample_results/vllm/label_based/performance/issues.json",
  "issue": {
    "number": 10062,
    "title": "[Performance]: Throughput and Latency degradation with a  single LoRA adapter on A100 40 GB",
    "body": "### Proposal to improve performance\n\n_No response_\n\n### Report of performance regression\n\n_No response_\n\n### Misc discussion on performance\n\n\r\n\r\n---\r\n\r\n**Setup Summary for vLLM Benchmarking with Llama-2 Model:**\r\n\r\n- **Hardware**: A100 40 GB (a2-highgpu-2g) on Google Kubernetes Engine (GKE)\r\n- **Model**: `meta-llama/Llama-2-7b-hf`\r\n- **GPU Count**: 1\r\n- **Experiments**:\r\n  - **Experiment 1**: Requests using the base model `meta-llama/Llama-2-7b-hf`.\r\n  - **Experiment 2**: vLLM deployed with LoRA adapter `vineetsharma/qlora-adapter-Llama-2-7b-hf-TweetSumm` (size 160 MB).\r\n  - **Experiment 3**: vLLM deployed with LoRA adapter `xtuner/Llama-2-7b-qlora-moss-003-sft` (size 640 MB).\r\n  \r\n  For all three experiments, we used the same input prompt (ShareGPT) and observed a similar output length.\r\n\r\n**Settings**:\r\n- **Eager Mode**: Not enabled.\r\n- **Max GPU Utilization**: Default at 90%.\r\n\r\n**Benchmark Metrics**:\r\nWe measured:\r\n  - **Latency per output token**\r\n  - **Throughput** (output tokens per second)\r\n\r\nYou can view detailed results in the benchmark document: [Benchmark 1 server - Sheet7.pdf](https://github.com/user-attachments/files/17640153/Benchmark.1.server.-.Sheet7.pdf).\r\n\r\n---\r\n\r\n**Observations and Questions**:\r\n\r\n- Using LoRA adapters led to a notable degradation in throughput and latency compared to the base model. Specifically, we observed up to a 50% drop in maximum throughput with LoRA compared to the base model.  \r\n- **Is this performance degradation expected with LoRA adapters?** \r\n- **Are there parameters or tuning options that could improve LoRA performance?**\r\n\r\n**Deployment Command**:\r\n\r\n```yaml\r\ncommand: [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\r\nargs:\r\n  - \"--model\"\r\n  - \"meta-llama/Llama-2-7b-hf\"\r\n  - \"--tensor-parallel-size\"\r\n  - \"1\"\r\n  - \"--port\"\r\n  - \"8000\"\r\n  - \"--disable-log-requests\"\r\n  - \"--enable-lora\"\r\n  - \"--max-loras\"\r\n  - \"3\"\r\n  - \"--max-cpu-loras\"\r\n  - \"15\"\r\n  - \"--max-lora-rank\"\r\n  - \"64\"\r\n  - \"--gpu-memory-utilization\"\r\n  - \"0.9\"\r\n  - \"--lora-modules\"\r\n  - xtuner/Llama-2-7b-qlora-moss-003-sft\r\n```\r\n\r\n--- \r\n\n\n### Your current environment (if you think it is necessary)\n\n---\r\n\r\n**Sample Query**:\r\n\r\n```bash\r\ncurl -i ${IP}:${PORT}/v1/completions -H 'Content-Type: application/json' -d '{\r\n  \"model\": \"tweet-summary\",\r\n  \"prompt\": \"Write as if you were a critic: San Francisco\",\r\n  \"max_tokens\": 100,\r\n  \"temperature\": 0\r\n}'\r\n```\r\n\r\n**Deployment YAML Configuration**:\r\n\r\n```yaml\r\n---\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: vllm-llama2-7b-pool\r\nspec:\r\n  selector:\r\n    app: vllm-llama2-7b-pool\r\n  ports:\r\n    - protocol: TCP\r\n      port: 8000\r\n      targetPort: 8000\r\n  type: LoadBalancer\r\n\r\n---\r\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: vllm-llama2-7b-pool\r\nspec:\r\n  replicas: 1\r\n  selector:\r\n    matchLabels:\r\n      app: vllm-llama2-7b-pool\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: vllm-llama2-7b-pool\r\n    spec:\r\n      containers:\r\n        - name: lora\r\n          image: \"vllm/vllm-openai:latest\"\r\n          imagePullPolicy: Always\r\n          command: [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\r\n          args:\r\n            - \"--model\"\r\n            - \"meta-llama/Llama-2-7b-hf\"\r\n            - \"--tensor-parallel-size\"\r\n            - \"1\"\r\n            - \"--port\"\r\n            - \"8000\"\r\n            - \"--disable-log-requests\"\r\n            - \"--enable-lora\"\r\n            - \"--max-loras\"\r\n            - \"3\"\r\n            - \"--max-cpu-loras\"\r\n            - \"15\"\r\n            - \"--max-lora-rank\"\r\n            - \"64\"\r\n            - \"--gpu-memory-utilization\"\r\n            - \"0.9\"\r\n            - \"--lora-modules\"\r\n            - \"tweet-summary-0=/adapters/vineetsharma/qlora-adapter-Llama-2-7b-hf-TweetSumm_0\"\r\n          env:\r\n            - name: PORT\r\n              value: \"8000\"\r\n            - name: HUGGING_FACE_HUB_TOKEN\r\n              valueFrom:\r\n                secretKeyRef:\r\n                  name: hf-token\r\n                  key: token\r\n          ports:\r\n            - containerPort: 8000\r\n              name: http\r\n              protocol: TCP\r\n          livenessProbe:\r\n            failureThreshold: 240\r\n            httpGet:\r\n              path: /health\r\n              port: http\r\n              scheme: HTTP\r\n            initialDelaySeconds: 5\r\n            periodSeconds: 5\r\n            successThreshold: 1\r\n            timeoutSeconds: 1\r\n          readinessProbe:\r\n            failureThreshold: 600\r\n            httpGet:\r\n              path: /health\r\n              port: http\r\n              scheme: HTTP\r\n            initialDelaySeconds: 5\r\n            periodSeconds: 5\r\n            successThreshold: 1\r\n            timeoutSeconds: 1\r\n          resources:\r\n            limits:\r\n              nvidia.com/gpu: 1\r\n            requests:\r\n              nvidia.com/gpu: 1\r\n          volumeMounts:\r\n            - mountPath: /data\r\n              name: data\r\n            - mountPath: /dev/shm\r\n              name: shm\r\n            - name: adapters\r\n              mountPath: \"/adapters\"\r\n      initContainers:\r\n        - name: adapter-loader\r\n          image: ghcr.io/tomatillo-and-multiverse/adapter-puller:demo\r\n          command: [\"python\"]\r\n          args:\r\n            - ./pull_adapters.py\r\n            - --adapter\r\n            - xtuner/Llama-2-7b-qlora-moss-003-sft\r\n            - --adapter\r\n            - yard1/llama-2-7b-sql-lora-test\r\n            - --adapter\r\n            - vineetsharma/qlora-adapter-Llama-2-7b-hf-TweetSumm\r\n            - --duplicate-count\r\n            - \"5\"\r\n          env:\r\n            - name: HF_TOKEN\r\n              valueFrom:\r\n                secretKeyRef:\r\n                  name: hf-token\r\n                  key: token\r\n            - name: HF_HOME\r\n              value: /adapters\r\n          volumeMounts:\r\n            - name: adapters\r\n              mountPath: \"/adapters\"\r\n      restartPolicy: Always\r\n      schedulerName: default-scheduler\r\n      terminationGracePeriodSeconds: 30\r\n      volumes:\r\n        - name: data\r\n          emptyDir: {}\r\n        - name: shm\r\n          emptyDir:\r\n            medium: Memory\r\n        - name: adapters\r\n          emptyDir: {}\r\n```\r\n\r\nThis deployment configuration sets up the vLLM server with LoRA adapters on GKE, with health probes, GPU limits, and a volume configuration for adapter management.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "performance",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-11-06T00:42:03+00:00",
    "closed_at": "2025-05-28T02:13:20+00:00",
    "comments": 16,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10062/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10062"
  }
}