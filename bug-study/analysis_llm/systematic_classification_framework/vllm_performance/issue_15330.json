{
  "issue_type": "performance",
  "extracted_from": "/root/yunwei37/vllm-exp/bug-study/analysis_llm/sample_results/vllm/label_based/performance/issues.json",
  "issue": {
    "number": 15330,
    "title": "[Performance]: poor performance in pipeline parallesm when batch-size is large",
    "body": "### Proposal to improve performance\n\nIn the case where the lengths of the sent requests are the same, pipeline parallelism should have fewer bubbles, which also means that pipeline parallelism should have a higher throughput than tensor parallelism. However, when I issue requests with a batch size of 400 and a sequence length of 2048, the throughput of the Decode stage in tensor parallelism is nearly three times higher than that in pipeline parallelism.\n\n![Image](https://github.com/user-attachments/assets/c40d96ec-3c99-41c6-85ad-4f7bca36fdae)\n\n### Report of performance regression\n\nYou can use the following script to reproduce the phenomenon that the performance of the Decode stage in pipeline parallelism is very poor as I mentioned. I sent 400 requests from the client to the started server, and the request configuration is that the input length is 2048 and the maximum output length is 1000.\n\n`nsys profile -o report.nsys-rep-pp-4-batch-micro-batch-100-python --trace-fork-before-exec=true --cuda-graph-trace=node --delay 120 --duration 120 --force-overwrite true --python-sampling=true vllm serve Qwen/Qwen2.5-14B-Instruct-1M --load-format dummy --pipeline-parallel-size 4 --no-enable-prefix-caching --port 30000 --max-model-len 3150 --disable-log-requests --max-num-seqs 100 --num-scheduler-steps 50`\n\nI analyzed the performance of the server side using nsys. I noticed that there are huge bubbles between the Decode processes. As can be seen from the analysis graph generated by nsys, there is a substantial CPU operation overhead, which even exceeds three times the execution time on the GPU. This leads to the performance of pipeline parallelism in the Decode stage being significantly inferior to that of tensor parallelism.\n\nI don't think the overhead here is caused by the issue of setting a relatively large --num-scheduler-steps parameter as mentioned in the previous issue, because I have already set this parameter to 50 to reduce the overhead. I believe that it is necessary to optimize the CPU overhead of pipeline parallelism. Especially in scenarios where the communication bandwidth between the GPUs is large, it is essential to use pipeline parallelism.\n\n\n### Misc discussion on performance\n\nIn the case where the lengths of the sent requests are the same, pipeline parallelism should have fewer bubbles, which also means that pipeline parallelism should have a higher throughput than tensor parallelism. However, when I issue requests with a batch size of 400 and a sequence length of 2048, the throughput of the Decode stage in tensor parallelism is nearly three times higher than that in pipeline parallelism.\n\n### Your current environment (if you think it is necessary)\n\nCollecting environment information...\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Alibaba Cloud Linux release 3 (Soaring Falcon)  (x86_64)\nGCC version: (GCC) 10.2.1 20200825 (Alibaba 10.2.1-3.8 2.32)\nClang version: Could not collect\nCMake version: version 3.26.5\nLibc version: glibc-2.32\n\nPython version: 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:56:27) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.10.134-16.101.al8.x86_64-x86_64-with-glibc2.32\nIs CUDA available: True\nCUDA runtime version: 12.4.99\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA L20Y\nGPU 1: NVIDIA L20Y\nGPU 2: NVIDIA L20Y\nGPU 3: NVIDIA L20Y\nGPU 4: NVIDIA L20Y\nGPU 5: NVIDIA L20Y\nGPU 6: NVIDIA L20Y\nGPU 7: NVIDIA L20Y\n\nNvidia driver version: 550.127.08\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:        x86_64\nCPU op-mode(s):      32-bit, 64-bit\nByte Order:          Little Endian\nCPU(s):              192\nOn-line CPU(s) list: 0-191\nThread(s) per core:  2\nCore(s) per socket:  48\nSocket(s):           2\nNUMA node(s):        2\nVendor ID:           GenuineIntel\nCPU family:          6\nModel:               143\nModel name:          Intel(R) Xeon(R) Platinum 8468V\nStepping:            8\nCPU MHz:             2871.487\nCPU max MHz:         3800.0000\nCPU min MHz:         800.0000\nBogoMIPS:            4800.00\nVirtualization:      VT-x\nL1d cache:           48K\nL1i cache:           32K\nL2 cache:            2048K\nL3 cache:            99840K\nNUMA node0 CPU(s):   0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126,128,130,132,134,136,138,140,142,144,146,148,150,152,154,156,158,160,162,164,166,168,170,172,174,176,178,180,182,184,186,188,190\nNUMA node1 CPU(s):   1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,121,123,125,127,129,131,133,135,137,139,141,143,145,147,149,151,153,155,157,159,161,163,165,167,169,171,173,175,177,179,181,183,185,187,189,191\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req hfi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm uintr md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.3.0\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.49.0\n[pip3] triton==3.2.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.6.2                    pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] pyzmq                     26.3.0                   pypi_0    pypi\n[conda] torch                     2.6.0                    pypi_0    pypi\n[conda] torchaudio                2.6.0                    pypi_0    pypi\n[conda] torchvision               0.21.0                   pypi_0    pypi\n[conda] transformers              4.49.0                   pypi_0    pypi\n[conda] triton                    3.2.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.4.dev473+g9ed6ee92\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6 NIC7     CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV8     NV8     NV8     NV8     NV8     NV8     NV8     PIX     NODE    NODE    NODE    SYS     SYS  SYS      SYS     0,2,4,6,8,10    0               N/A\nGPU1    NV8      X      NV8     NV8     NV8     NV8     NV8     NV8     NODE    PIX     NODE    NODE    SYS     SYS  SYS      SYS     0,2,4,6,8,10    0               N/A\nGPU2    NV8     NV8      X      NV8     NV8     NV8     NV8     NV8     NODE    NODE    PIX     NODE    SYS     SYS  SYS      SYS     0,2,4,6,8,10    0               N/A\nGPU3    NV8     NV8     NV8      X      NV8     NV8     NV8     NV8     NODE    NODE    NODE    PIX     SYS     SYS  SYS      SYS     0,2,4,6,8,10    0               N/A\nGPU4    NV8     NV8     NV8     NV8      X      NV8     NV8     NV8     SYS     SYS     SYS     SYS     PIX     NODE NODE     NODE    1,3,5,7,9,11    1               N/A\nGPU5    NV8     NV8     NV8     NV8     NV8      X      NV8     NV8     SYS     SYS     SYS     SYS     NODE    PIX  NODE     NODE    1,3,5,7,9,11    1               N/A\nGPU6    NV8     NV8     NV8     NV8     NV8     NV8      X      NV8     SYS     SYS     SYS     SYS     NODE    NODE PIX      NODE    1,3,5,7,9,11    1               N/A\nGPU7    NV8     NV8     NV8     NV8     NV8     NV8     NV8      X      SYS     SYS     SYS     SYS     NODE    NODE NODE     PIX     1,3,5,7,9,11    1               N/A\nNIC0    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    SYS     SYS  SYS      SYS\nNIC1    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    SYS     SYS  SYS      SYS\nNIC2    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    SYS     SYS  SYS      SYS\nNIC3    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      SYS     SYS  SYS      SYS\nNIC4    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE NODE     NODE\nNIC5    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE     X   NODE     NODE\nNIC6    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE  X       NODE\nNIC7    SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE NODE      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_bond_0\n  NIC1: mlx5_bond_1\n  NIC2: mlx5_bond_2\n  NIC3: mlx5_bond_3\n  NIC4: mlx5_bond_4\n  NIC5: mlx5_bond_5\n  NIC6: mlx5_bond_6\n  NIC7: mlx5_bond_7\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "performance",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-22T12:46:07+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15330/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15330"
  }
}