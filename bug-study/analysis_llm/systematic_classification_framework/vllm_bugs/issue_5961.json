{
  "issue_type": "bug",
  "extracted_from": "/root/yunwei37/vllm-exp/bug-study/analysis_llm/sample_results/vllm/label_based/bug/issues.json",
  "issue": {
    "number": 5961,
    "title": "[Bug]: vLLM crash when running Phi-3-small-8k-instruct with enable-chunked-prefill",
    "body": "### Your current environment\n\n\r\n```\r\nimage\": \"vllm/vllm-openai:latest\",\r\n--model=microsoft/Phi-3-small-8k-instruct \r\n--tensor-parallel-size=1\r\n--disable-log-requests\r\n--trust-remote-code\r\n--enable-chunked-prefill\r\n--max-num-batched-tokens=2048\r\n--max-model-len=4096\r\n--gpu-memory-utilization=0.9\",\r\n```\r\nAccelerator: 1x Nvidia L4\n\n### \ud83d\udc1b Describe the bug\n\n```\r\nERROR 06-28 13:26:18 async_llm_engine.py:52] Engine background task failed\r\nERROR 06-28 13:26:18 async_llm_engine.py:52] Traceback (most recent call last):\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 42, in _log_task_completion\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return_value = task.result()\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 532, in run_engine_loop\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     has_requests_in_progress = await asyncio.wait_for(\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return fut.result()\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 506, in engine_step\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     request_outputs = await self.engine.step_async()\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 235, in step_async\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     output = await self.model_executor.execute_model_async(\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py\", line 117, in execute_model_async\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     output = await make_async(self.driver_worker.execute_model\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     result = self.fn(*self.args, **self.kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return func(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 280, in execute_model\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     output = self.model_runner.execute_model(seq_group_metadata_list,\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return func(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 749, in execute_model\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     hidden_states = model_executable(\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return self._call_impl(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return forward_call(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/phi3_small.py\", line 416, in forward\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     output_hidden_states = self.model(\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return self._call_impl(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return forward_call(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/phi3_small.py\", line 338, in forward\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     hidden_states = layer(\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return self._call_impl(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return forward_call(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/phi3_small.py\", line 282, in forward\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     hidden_states = self.self_attn(\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return self._call_impl(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return forward_call(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/phi3_small.py\", line 244, in forward\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     attn_output = self.attn(q, k, v, kv_cache, attn_metadata=attn_metadata)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return self._call_impl(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return forward_call(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/attention/layer.py\", line 89, in forward\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return self.impl.forward(query, key, value, kv_cache, attn_metadata,\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/attention/backends/blocksparse_attn.py\", line 376, in forward\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     or prefill_meta.block_tables.numel() == 0, \\\r\nERROR 06-28 13:26:18 async_llm_engine.py:52] AssertionError: Does not support prefix-enabled attention.\r\n```",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-06-28T13:34:09+00:00",
    "closed_at": "2024-06-28T22:41:15+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5961/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5961"
  }
}