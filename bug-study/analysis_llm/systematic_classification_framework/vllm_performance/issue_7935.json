{
  "issue_type": "performance",
  "extracted_from": "/root/yunwei37/vllm-exp/bug-study/analysis_llm/sample_results/vllm/label_based/performance/issues.json",
  "issue": {
    "number": 7935,
    "title": "[Performance]: 5x slower throught with openAI client/server than native one",
    "body": "### Proposal to improve performance\r\n\r\nI've been trying to write a reliable benchmark to be used with vllm, and I discovered that when I use the openAI client it can't scale. If I try to use 50 concurrent clients the gpu load goes down to 5% and the throughput is extremely slow. The more clients I add the worst things get. With a single client there is no problem.\r\n\r\nI then used the same benchmark switching to the [vllm native client/server](https://docs.vllm.ai/en/latest/getting_started/examples/api_client.html) and I'm getting a 60-70% gpu util and 5x higher throughput.\r\n\r\nI checked that I had the same `SamplingParams` reported by the server in both cases.\r\n\r\nIn parallel with those I was using https://github.com/grafana/k6 against both uses cases - with openAI entrypoints and with the native entrypoint - I can confirm that the server isn't the problem - in both cases I get high gpu util with k6 client and high throughput.\r\n\r\nI thought that perhaps streaming was the cause but disabling it made a very small difference.\r\n\r\nSo everything points to the openAI client - I know that it's not your product but you recommend using it with [the openAI entrypoint](https://docs.vllm.ai/en/latest/getting_started/examples/api_client.html):\r\n\r\n> \"\"\"Example Python client for `vllm.entrypoints.api_server`\r\n> NOTE: The API server is used only for demonstration and simple performance\r\n> benchmarks. It is not intended for production use.\r\n> For production use, we recommend `vllm serve` and the OpenAI client API.\r\n\r\nSo perhaps you have some insights to what I'm missing? I'm just using your examples as is.\r\n\r\nvllm==0.5.5 here\r\n\r\nThank you!\r\n\r\n### Your current environment (if you think it is necessary)\r\n\r\n\r\n```text\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: 10.0.0-4ubuntu1 \r\nCMake version: version 3.30.2\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-1017-gcp-tcpx-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA H100 80GB HBM3\r\nGPU 1: NVIDIA H100 80GB HBM3\r\n\r\nNvidia driver version: 550.90.07\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-12.0/targets/x86_64-linux/lib/libcudnn.so.8.9.4\r\n/usr/local/cuda-12.0/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.9.4\r\n/usr/local/cuda-12.0/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.9.4\r\n/usr/local/cuda-12.0/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.9.4\r\n/usr/local/cuda-12.0/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.9.4\r\n/usr/local/cuda-12.0/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.9.4\r\n/usr/local/cuda-12.0/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.9.4\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nByte Order:                           Little Endian\r\nAddress sizes:                        52 bits physical, 57 bits virtual\r\nCPU(s):                               208\r\nOn-line CPU(s) list:                  0-207\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   52\r\nSocket(s):                            2\r\nNUMA node(s):                         2\r\nVendor ID:                            GenuineIntel\r\nCPU family:                           6\r\nModel:                                143\r\nModel name:                           Intel(R) Xeon(R) Platinum 8481C CPU @ 2.70GHz\r\nStepping:                             8\r\nCPU MHz:                              2699.998\r\nBogoMIPS:                             5399.99\r\nHypervisor vendor:                    KVM\r\nVirtualization type:                  full\r\nL1d cache:                            4.9 MiB\r\nL1i cache:                            3.3 MiB\r\nL2 cache:                             208 MiB\r\nL3 cache:                             210 MiB\r\nNUMA node0 CPU(s):                    0-51,104-155\r\nNUMA node1 CPU(s):                    52-103,156-207\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rtm avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx_vnni avx512_bf16 arat avx512vbmi umip avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid cldemote movdiri movdir64b fsrm md_clear serialize amx_bf16 avx512_fp16 amx_tile amx_int8 arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] flake8==7.1.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.3\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] onnxruntime==1.18.1\r\n[pip3] qtorch==0.3.0\r\n[pip3] sentence-transformers==3.0.1\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.44.0\r\n[pip3] triton==3.0.0\r\n[conda] numpy                     1.26.3                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] qtorch                    0.3.0                    pypi_0    pypi\r\n[conda] sentence-transformers     3.0.1                    pypi_0    pypi\r\n[conda] torch                     2.4.0                    pypi_0    pypi\r\n[conda] torchvision               0.19.0                   pypi_0    pypi\r\n[conda] transformers              4.44.0                   pypi_0    pypi\r\n[conda] triton                    3.0.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.5\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV18    57-63,161-166   1               N/A\r\nGPU1    NV18     X      57-63,161-166   1               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "performance"
    ],
    "state": "closed",
    "created_at": "2024-08-28T02:44:31+00:00",
    "closed_at": "2024-10-28T19:10:50+00:00",
    "comments": 21,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7935/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/7935"
  }
}