{
  "issue_type": "bug",
  "extracted_from": "/root/yunwei37/vllm-exp/bug-study/analysis_llm/sample_results/vllm/label_based/bug/issues.json",
  "issue": {
    "number": 5885,
    "title": "[Bug]: Concurrently captioning images with phi3 Vision can cause the backend to crash",
    "body": "### Your current environment\n\n```text\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.6\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-112-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA H100 PCIe\r\nNvidia driver version: 550.67\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      52 bits physical, 57 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             128\r\nOn-line CPU(s) list:                0-127\r\nVendor ID:                          AuthenticAMD\r\nModel name:                         AMD EPYC 9354 32-Core Processor\r\nCPU family:                         25\r\nModel:                              17\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 32\r\nSocket(s):                          2\r\nStepping:                           1\r\nFrequency boost:                    enabled\r\nCPU max MHz:                        3799.0720\r\nCPU min MHz:                        1500.0000\r\nBogoMIPS:                           6500.17\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid overflow_recov succor smca fsrm flush_l1d\r\nVirtualization:                     AMD-V\r\nL1d cache:                          2 MiB (64 instances)\r\nL1i cache:                          2 MiB (64 instances)\r\nL2 cache:                           64 MiB (64 instances)\r\nL3 cache:                           512 MiB (16 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0-31,64-95\r\nNUMA node1 CPU(s):                  32-63,96-127\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Mitigation; safe RET\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] onnxruntime-gpu==1.18.0\r\n[pip3] torch==2.3.0\r\n[pip3] torchaudio==2.1.1\r\n[pip3] torchvision==0.18.0\r\n[pip3] transformers==4.41.2\r\n[pip3] triton==2.3.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.0.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      32-63,96-127    1               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen running multiple requests to phi-3 vision, I sometimes get this error. Causing the app to refuse further requests.\r\n\r\nStacktrace on VLLM side:\r\n```\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 278, in app\r\n    raw_response = await run_endpoint_function(\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 191, in run_endpoint_function\r\n    return await dependant.call(**values)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 132, in create_chat_completion\r\n    generator = await openai_serving_chat.create_chat_completion(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 293, in create_chat_completion\r\n    return await self.chat_completion_full_generator(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 493, in chat_completion_full_generator\r\n    async for res in result_generator:\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 684, in generate\r\n    async for output in self._process_request(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 797, in _process_request\r\n    raise e\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 793, in _process_request\r\n    async for request_output in stream:\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 90, in __anext__\r\n    raise result\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 43, in _log_task_completion\r\n    return_value = task.result()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 552, in run_engine_loop\r\n    has_requests_in_progress = await self.engine_step()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 525, in engine_step\r\n    request_outputs = await self.engine.step_async()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 236, in step_async\r\n    output = await self.model_executor.execute_model_async(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py\", line 121, in execute_model_async\r\n    output = await make_async(self.driver_worker.execute_model\r\n  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 255, in execute_model\r\n    output = self.model_runner.execute_model(model_input, self.kv_cache)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 999, in execute_model\r\n    hidden_states = model_executable(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/phi3v.py\", line 376, in forward\r\n    inputs_embeds = self.vision_embed_tokens(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/phi3v.py\", line 220, in forward\r\n    sub_img = sub_img[:B_]\r\nTypeError: only integer tensors of a single element can be converted to an index\r\n```\r\n\r\nThe workaround I used was to send 1 request at a time with a multiprocessing lock from my client process.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-06-27T05:17:59+00:00",
    "closed_at": "2024-06-27T08:29:26+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5885/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5885"
  }
}