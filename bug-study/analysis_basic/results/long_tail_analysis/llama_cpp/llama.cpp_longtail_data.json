{
  "framework": "llama.cpp",
  "analysis_date": "2025-07-29T17:16:52.248559",
  "total_issues": 5470,
  "longtail_count": 603,
  "longtail_threshold": 12.0,
  "results": {
    "comment_stats": {
      "mean": 5.628884826325411,
      "median": 3.0,
      "std": 9.080616938602548,
      "p75": 6.0,
      "p90": 12.0,
      "p95": 18.0,
      "p99": 40.3100000000004,
      "max": "210"
    },
    "comparisons": {
      "long_tail_count": 603,
      "regular_count": 4867,
      "avg_body_length": {
        "long_tail": 5520.817578772802,
        "regular": 4570.010067803575
      },
      "resolution_rate": {
        "long_tail": 92.70315091210614,
        "regular": 95.1304705157181
      },
      "avg_resolution_days": {
        "long_tail": 93.57346344331812,
        "regular": 54.83574827763791
      },
      "avg_labels": {
        "long_tail": 1.6003316749585406,
        "regular": 1.436613930552702
      }
    },
    "label_distribution": {
      "total_labels": 47,
      "most_common": [
        [
          "stale",
          253
        ],
        [
          "bug-unconfirmed",
          192
        ],
        [
          "enhancement",
          134
        ],
        [
          "bug",
          54
        ],
        [
          "help wanted",
          35
        ],
        [
          "model",
          34
        ],
        [
          "good first issue",
          31
        ],
        [
          "medium severity",
          26
        ],
        [
          "performance",
          19
        ],
        [
          "high severity",
          16
        ]
      ],
      "label_count_distribution": {
        "0": 72,
        "1": 211,
        "2": 229,
        "3": 70,
        "4": 19,
        "5": 2
      }
    },
    "top_issues": [
      {
        "number": 1602,
        "title": "llama : add Falcon LLM support",
        "comments": 210,
        "state": "closed",
        "labels": "help wanted, model",
        "created_days_ago": 794,
        "body_length": 362,
        "url": "https://github.com/ggml-org/llama.cpp/issues/1602",
        "resolution_days": 89.1
      },
      {
        "number": 7062,
        "title": "Llama3 GGUF conversion with merged LORA Adapter seems to lose training data randomly",
        "comments": 147,
        "state": "closed",
        "labels": "bug-unconfirmed",
        "created_days_ago": 451,
        "body_length": 1436,
        "url": "https://github.com/ggml-org/llama.cpp/issues/7062",
        "resolution_days": 5.7
      },
      {
        "number": 8650,
        "title": "Feature Request: Proper Llama 3.1 Support in llama.cpp",
        "comments": 138,
        "state": "closed",
        "labels": "enhancement",
        "created_days_ago": 371,
        "body_length": 1356,
        "url": "https://github.com/ggml-org/llama.cpp/issues/8650",
        "resolution_days": 3.8
      },
      {
        "number": 9246,
        "title": "Feature Request: Support for Qwen2-VL",
        "comments": 131,
        "state": "closed",
        "labels": "enhancement, stale",
        "created_days_ago": 333,
        "body_length": 982,
        "url": "https://github.com/ggml-org/llama.cpp/issues/9246",
        "resolution_days": 268.1
      },
      {
        "number": 4216,
        "title": "server : improvements and maintenance",
        "comments": 120,
        "state": "open",
        "labels": "help wanted, refactoring, server/webui, roadmap",
        "created_days_ago": 612,
        "body_length": 3926,
        "url": "https://github.com/ggml-org/llama.cpp/issues/4216",
        "resolution_days": "N/A"
      },
      {
        "number": 722,
        "title": "Rockchip RK3588 perf",
        "comments": 103,
        "state": "closed",
        "labels": "",
        "created_days_ago": 848,
        "body_length": 4586,
        "url": "https://github.com/ggml-org/llama.cpp/issues/722",
        "resolution_days": 0.1
      },
      {
        "number": 2262,
        "title": "Add llama 2 model",
        "comments": 95,
        "state": "closed",
        "labels": "\ud83e\udd99., model",
        "created_days_ago": 742,
        "body_length": 232,
        "url": "https://github.com/ggml-org/llama.cpp/issues/2262",
        "resolution_days": 91.6
      },
      {
        "number": 2379,
        "title": "llama : add support for llama2.c models",
        "comments": 93,
        "state": "closed",
        "labels": "help wanted, good first issue, model",
        "created_days_ago": 735,
        "body_length": 437,
        "url": "https://github.com/ggml-org/llama.cpp/issues/2379",
        "resolution_days": 18.1
      },
      {
        "number": 1499,
        "title": "[Feature request] Any plans for AMD XDNA AI Engine support on Ryzen 7x40 processors?",
        "comments": 92,
        "state": "closed",
        "labels": "stale",
        "created_days_ago": 804,
        "body_length": 833,
        "url": "https://github.com/ggml-org/llama.cpp/issues/1499",
        "resolution_days": 690.6
      },
      {
        "number": 8240,
        "title": "Investigate gemma 2 generation quality",
        "comments": 90,
        "state": "closed",
        "labels": "enhancement, stale",
        "created_days_ago": 393,
        "body_length": 347,
        "url": "https://github.com/ggml-org/llama.cpp/issues/8240",
        "resolution_days": 106.3
      },
      {
        "number": 5761,
        "title": "Support BitNet b1.58 ternary models",
        "comments": 90,
        "state": "closed",
        "labels": "enhancement, stale, Tensor Encoding Scheme",
        "created_days_ago": 517,
        "body_length": 1003,
        "url": "https://github.com/ggml-org/llama.cpp/issues/5761",
        "resolution_days": 202.6
      },
      {
        "number": 6849,
        "title": "Support for Phi-3 models",
        "comments": 84,
        "state": "open",
        "labels": "good first issue, model",
        "created_days_ago": 462,
        "body_length": 129,
        "url": "https://github.com/ggml-org/llama.cpp/issues/6849",
        "resolution_days": "N/A"
      },
      {
        "number": 1291,
        "title": "Try whether OpenLLaMa works",
        "comments": 82,
        "state": "closed",
        "labels": "\ud83e\udd99., model, stale",
        "created_days_ago": 818,
        "body_length": 299,
        "url": "https://github.com/ggml-org/llama.cpp/issues/1291",
        "resolution_days": 342.1
      },
      {
        "number": 1735,
        "title": "with the newest builds i only get gibberish output",
        "comments": 81,
        "state": "closed",
        "labels": "bug, high priority",
        "created_days_ago": 783,
        "body_length": 663,
        "url": "https://github.com/ggml-org/llama.cpp/issues/1735",
        "resolution_days": 8.0
      },
      {
        "number": 6747,
        "title": "llama3 family support",
        "comments": 79,
        "state": "closed",
        "labels": "enhancement, model",
        "created_days_ago": 467,
        "body_length": 178,
        "url": "https://github.com/ggml-org/llama.cpp/issues/6747",
        "resolution_days": 2.8
      },
      {
        "number": 11483,
        "title": "Feature Request: Qwen 2.5 VL",
        "comments": 74,
        "state": "closed",
        "labels": "enhancement, stale",
        "created_days_ago": 181,
        "body_length": 771,
        "url": "https://github.com/ggml-org/llama.cpp/issues/11483",
        "resolution_days": 147.6
      },
      {
        "number": 7805,
        "title": "Bug: QWEN2 quantization GGML_ASSERT",
        "comments": 74,
        "state": "closed",
        "labels": "bug-unconfirmed, stale, high severity",
        "created_days_ago": 417,
        "body_length": 1697,
        "url": "https://github.com/ggml-org/llama.cpp/issues/7805",
        "resolution_days": 86.3
      },
      {
        "number": 7118,
        "title": "llama : add DeepSeek-v2-Chat support",
        "comments": 67,
        "state": "closed",
        "labels": "good first issue, model",
        "created_days_ago": 448,
        "body_length": 98,
        "url": "https://github.com/ggml-org/llama.cpp/issues/7118",
        "resolution_days": 21.4
      },
      {
        "number": 603,
        "title": "Performance Discrepancy: gpt4all Faster than Optimized llama.cpp",
        "comments": 67,
        "state": "closed",
        "labels": "performance",
        "created_days_ago": 852,
        "body_length": 2021,
        "url": "https://github.com/ggml-org/llama.cpp/issues/603",
        "resolution_days": 13.9
      },
      {
        "number": 10528,
        "title": "Misc. bug: Inconsistent Vulkan segfault",
        "comments": 65,
        "state": "open",
        "labels": "bug",
        "created_days_ago": 244,
        "body_length": 2728,
        "url": "https://github.com/ggml-org/llama.cpp/issues/10528",
        "resolution_days": "N/A"
      }
    ],
    "author_stats": {
      "unique_authors_regular": 2962,
      "unique_authors_longtail": 462,
      "avg_issues_per_author_regular": 1.6431465226198514,
      "avg_issues_per_author_longtail": 1.3051948051948052
    }
  },
  "longtail_issues": [
    {
      "number": 14727,
      "title": "Eval bug: Nondeterministic output with ROCm backend despite zero temperature",
      "state": "open",
      "comments": 14,
      "body_length": 19750,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2025-07-16 18:41:41+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 14642,
      "title": "Feature Request: Support Kimi K2",
      "state": "open",
      "comments": 44,
      "body_length": 1232,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2025-07-12 06:05:19+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 14542,
      "title": "Compile bug: cannot compile get_rows_iq1_m",
      "state": "closed",
      "comments": 30,
      "body_length": 5230,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2025-07-05 17:13:39+00:00",
      "closed_at": "2025-07-17 08:46:50+00:00",
      "resolution_days": 11.648043981481482
    },
    {
      "number": 14459,
      "title": "Misc. bug: convert_hf_to_gguf.py not working on qwen3-embedding and qwen3-embedding lora tuned models",
      "state": "open",
      "comments": 12,
      "body_length": 40643,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2025-06-30 09:31:04+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 14424,
      "title": "Eval bug: example/finetune.cpp crashing",
      "state": "closed",
      "comments": 29,
      "body_length": 19392,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2025-06-27 15:32:55+00:00",
      "closed_at": "2025-07-03 15:05:20+00:00",
      "resolution_days": 5.980844907407407
    },
    {
      "number": 14325,
      "title": "Eval bug: Program crashes during long input inference when batch size is set to 16384",
      "state": "open",
      "comments": 12,
      "body_length": 10372,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2025-06-22 11:00:19+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 14280,
      "title": "Eval bug: Inconsistent Embedding Similarity between llama-server and LlamaCppEmbeddings for BGE-M3 Model",
      "state": "open",
      "comments": 35,
      "body_length": 11858,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2025-06-19 10:42:10+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 14252,
      "title": "Feature Request: fix handling of Qwen3-Embedding-0.6B input to add EOS token",
      "state": "closed",
      "comments": 13,
      "body_length": 763,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2025-06-18 01:09:19+00:00",
      "closed_at": "2025-06-21 16:12:06+00:00",
      "resolution_days": 3.6269328703703705
    },
    {
      "number": 14237,
      "title": "Llama 4 mmproj fails `unable to find tensor mm.model.fc.weight`",
      "state": "closed",
      "comments": 12,
      "body_length": 941,
      "label_names": [],
      "created_at": "2025-06-17 11:06:03+00:00",
      "closed_at": "2025-06-21 04:32:03+00:00",
      "resolution_days": 3.7263888888888888
    },
    {
      "number": 14178,
      "title": "Misc. bug: Failure to allocate buffer with ROCm 6.4",
      "state": "open",
      "comments": 13,
      "body_length": 10365,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2025-06-13 21:04:19+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 14134,
      "title": "Misc. bug: Performance regression on aarch64 q4_0",
      "state": "closed",
      "comments": 14,
      "body_length": 1640,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2025-06-11 21:43:06+00:00",
      "closed_at": "2025-06-17 09:58:33+00:00",
      "resolution_days": 5.510729166666667
    },
    {
      "number": 14044,
      "title": "Feature Request: dots.llm1 model support",
      "state": "closed",
      "comments": 23,
      "body_length": 912,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2025-06-06 08:57:57+00:00",
      "closed_at": "2025-06-15 08:30:57+00:00",
      "resolution_days": 8.98125
    },
    {
      "number": 13990,
      "title": "Eval bug: Abort is called in a thread from a custom thread pool during a llama_decode call",
      "state": "closed",
      "comments": 12,
      "body_length": 10006,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2025-06-03 12:25:37+00:00",
      "closed_at": "2025-06-20 11:57:37+00:00",
      "resolution_days": 16.980555555555554
    },
    {
      "number": 13876,
      "title": "Eval bug: std::runtime_error Invalid diff:",
      "state": "closed",
      "comments": 19,
      "body_length": 1192,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2025-05-28 22:51:50+00:00",
      "closed_at": "2025-06-05 15:36:09+00:00",
      "resolution_days": 7.697442129629629
    },
    {
      "number": 13765,
      "title": "Misc. bug: vulkan prompt processing suddenly slows down once I reach a certain prompt size",
      "state": "closed",
      "comments": 15,
      "body_length": 3143,
      "label_names": [],
      "created_at": "2025-05-25 04:00:19+00:00",
      "closed_at": "2025-06-07 19:00:58+00:00",
      "resolution_days": 13.625451388888889
    },
    {
      "number": 13708,
      "title": "Eval bug: Segmentation fault when loading SmolVLM-500M-Instruct-Q8\\_0.gguf on Termux / Android ARM64, only in Termux, not in Prooted ones, other gguf work fine",
      "state": "closed",
      "comments": 12,
      "body_length": 21577,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2025-05-22 14:14:24+00:00",
      "closed_at": "2025-05-25 18:29:47+00:00",
      "resolution_days": 3.177349537037037
    },
    {
      "number": 13694,
      "title": "Eval bug: Qwen2.5-VL-7B-Instruct returns extremely inaccurate bbox coordinates",
      "state": "open",
      "comments": 17,
      "body_length": 12978,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2025-05-21 20:04:14+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 13566,
      "title": "Misc. bug: webui multimodal, image input is not supported by this server, server error 500",
      "state": "closed",
      "comments": 12,
      "body_length": 1029,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2025-05-15 14:23:12+00:00",
      "closed_at": "2025-05-15 16:34:30+00:00",
      "resolution_days": 0.09118055555555556
    },
    {
      "number": 13430,
      "title": "Eval bug: b5335 break flash attention on 4070",
      "state": "closed",
      "comments": 20,
      "body_length": 1202,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2025-05-10 15:47:40+00:00",
      "closed_at": "2025-05-10 20:22:49+00:00",
      "resolution_days": 0.1910763888888889
    },
    {
      "number": 13408,
      "title": "Differential mode for llama-bench + plotting code",
      "state": "closed",
      "comments": 13,
      "body_length": 2935,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2025-05-09 14:39:08+00:00",
      "closed_at": "2025-06-24 01:07:59+00:00",
      "resolution_days": 45.43670138888889
    },
    {
      "number": 13367,
      "title": "(Discussion) Improve usability of llama-server",
      "state": "closed",
      "comments": 21,
      "body_length": 1572,
      "label_names": [
        "stale"
      ],
      "created_at": "2025-05-07 21:55:37+00:00",
      "closed_at": "2025-06-28 01:07:49+00:00",
      "resolution_days": 51.133472222222224
    },
    {
      "number": 13314,
      "title": "Feature Request: tensor split needs control over where CPU layers go",
      "state": "closed",
      "comments": 13,
      "body_length": 10087,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2025-05-05 14:21:06+00:00",
      "closed_at": "2025-06-21 01:07:56+00:00",
      "resolution_days": 46.449189814814815
    },
    {
      "number": 13310,
      "title": "Qwen3-8B and other models generate garbage output / repeat tokens (GGGGGG...) in llama.cpp via LM Studio Vulkan backend",
      "state": "open",
      "comments": 27,
      "body_length": 10130,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2025-05-05 06:27:53+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 13305,
      "title": "Eval bug: DeepSeek-R1-UD-Q2_K_XL output broken",
      "state": "closed",
      "comments": 13,
      "body_length": 18214,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2025-05-04 16:17:10+00:00",
      "closed_at": "2025-05-05 20:32:15+00:00",
      "resolution_days": 1.1771412037037037
    },
    {
      "number": 13287,
      "title": "Eval bug: b5237 broke Llama Scout",
      "state": "closed",
      "comments": 19,
      "body_length": 12054,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-05-03 17:06:27+00:00",
      "closed_at": "2025-05-04 17:27:05+00:00",
      "resolution_days": 1.0143287037037036
    },
    {
      "number": 13280,
      "title": "Eval bug: Heavy nondeterminism in Qwen3 MoE (CUDA)",
      "state": "closed",
      "comments": 17,
      "body_length": 1914,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2025-05-03 08:28:37+00:00",
      "closed_at": "2025-05-04 13:48:15+00:00",
      "resolution_days": 1.2219675925925926
    },
    {
      "number": 13164,
      "title": "Misc. bug: Qwen3 30B A3B Q4_K_M loads on server but quickly dies after requesting inference through Llama.cpp web UI",
      "state": "closed",
      "comments": 37,
      "body_length": 987,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2025-04-29 03:15:18+00:00",
      "closed_at": "2025-05-10 19:31:33+00:00",
      "resolution_days": 11.67795138888889
    },
    {
      "number": 13157,
      "title": "bug: ValueError: Architecture qwen3 not supported",
      "state": "open",
      "comments": 25,
      "body_length": 3598,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2025-04-28 21:42:12+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 12976,
      "title": "Misc. bug: Vulkan performance depends on thread priority",
      "state": "open",
      "comments": 16,
      "body_length": 3399,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-16 14:13:31+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 12946,
      "title": "Eval bug: GLM-Z1-9B-0414",
      "state": "closed",
      "comments": 60,
      "body_length": 11763,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2025-04-14 18:28:14+00:00",
      "closed_at": "2025-05-25 16:12:57+00:00",
      "resolution_days": 40.90605324074074
    },
    {
      "number": 12817,
      "title": "Eval bug: ggml_vulkan: Device memory allocation of size N failed with ub > 4096 and c > 4096 and b > 4096",
      "state": "closed",
      "comments": 14,
      "body_length": 8511,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2025-04-08 08:09:05+00:00",
      "closed_at": "2025-05-28 01:07:54+00:00",
      "resolution_days": 49.707511574074076
    },
    {
      "number": 12816,
      "title": "Misc. bug: The model's reasoning performance has significantly decreased despite using different versions of the same model architecture, identical parameters, and the same set of questions.",
      "state": "closed",
      "comments": 19,
      "body_length": 1699,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2025-04-08 08:04:32+00:00",
      "closed_at": "2025-06-13 05:52:41+00:00",
      "resolution_days": 65.9084375
    },
    {
      "number": 12696,
      "title": "Cannot compile SYCL backend SYCL_LIBRARY=SYCL_LIBRARY - NOTFOUND as per documentation",
      "state": "closed",
      "comments": 12,
      "body_length": 8311,
      "label_names": [
        "stale"
      ],
      "created_at": "2025-04-01 18:41:13+00:00",
      "closed_at": "2025-05-19 13:39:22+00:00",
      "resolution_days": 47.79038194444445
    },
    {
      "number": 12690,
      "title": "When will llama.cpp's vulkan provide support for Intel Arc's matrix core?",
      "state": "closed",
      "comments": 13,
      "body_length": 73,
      "label_names": [
        "stale"
      ],
      "created_at": "2025-04-01 09:32:28+00:00",
      "closed_at": "2025-05-17 01:07:49+00:00",
      "resolution_days": 45.64954861111111
    },
    {
      "number": 12637,
      "title": "Feature Request: Interleaved sliding window attention support for gemma 2 and 3",
      "state": "closed",
      "comments": 18,
      "body_length": 1366,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2025-03-29 00:39:26+00:00",
      "closed_at": "2025-06-07 08:47:17+00:00",
      "resolution_days": 70.33878472222223
    },
    {
      "number": 12597,
      "title": "Misc. bug: \"Unexpected empty grammar stack after accepting piece\" tool crash",
      "state": "closed",
      "comments": 21,
      "body_length": 2426,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2025-03-26 17:30:06+00:00",
      "closed_at": "2025-04-16 14:32:28+00:00",
      "resolution_days": 20.87664351851852
    },
    {
      "number": 12435,
      "title": "Eval bug: b4882 broke t5",
      "state": "closed",
      "comments": 13,
      "body_length": 9129,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-03-17 20:23:04+00:00",
      "closed_at": "2025-03-18 23:26:58+00:00",
      "resolution_days": 1.1277083333333333
    },
    {
      "number": 12433,
      "title": "Eval bug: Gemma3 <unused32> spam",
      "state": "closed",
      "comments": 24,
      "body_length": 22159,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-03-17 19:27:24+00:00",
      "closed_at": "2025-03-29 23:07:38+00:00",
      "resolution_days": 12.152939814814815
    },
    {
      "number": 12392,
      "title": "csm : implement Sesame-based conversation example",
      "state": "closed",
      "comments": 23,
      "body_length": 747,
      "label_names": [
        "model",
        "research \ud83d\udd2c",
        "stale",
        "tts"
      ],
      "created_at": "2025-03-14 14:49:46+00:00",
      "closed_at": "2025-05-14 01:07:48+00:00",
      "resolution_days": 60.42918981481481
    },
    {
      "number": 12390,
      "title": "SYCL bug: DeepSeek-V2-Lite-Chat-Q4_K_M does not work as expected",
      "state": "closed",
      "comments": 18,
      "body_length": 13321,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-03-14 14:31:14+00:00",
      "closed_at": "2025-03-15 14:19:31+00:00",
      "resolution_days": 0.9918634259259259
    },
    {
      "number": 12380,
      "title": "Eval bug: Segmentation fault from latest git 84d547554123a62e9ac77107cb20e4f6cc503af4",
      "state": "closed",
      "comments": 22,
      "body_length": 5572,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2025-03-14 05:27:42+00:00",
      "closed_at": "2025-03-14 08:47:46+00:00",
      "resolution_days": 0.1389351851851852
    },
    {
      "number": 12367,
      "title": "Eval bug: Loading fail on Gemma 3:12b > llama_model_load: error loading model: error loading model hyperparameters: key not found in model: gemma3.attention.layer_norm_rms_epsilon",
      "state": "closed",
      "comments": 16,
      "body_length": 6576,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2025-03-13 12:52:40+00:00",
      "closed_at": "2025-06-09 01:08:07+00:00",
      "resolution_days": 87.51072916666666
    },
    {
      "number": 12352,
      "title": "Eval bug: Gemma 3 extremly slow prompt processing when using quantized kv cache.",
      "state": "closed",
      "comments": 21,
      "body_length": 25003,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2025-03-12 11:47:37+00:00",
      "closed_at": "2025-06-09 01:08:08+00:00",
      "resolution_days": 88.55591435185185
    },
    {
      "number": 12325,
      "title": "Compile bug: gcc-11: error: unrecognized command-line option '-compress-mode=size'",
      "state": "closed",
      "comments": 16,
      "body_length": 2004,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2025-03-11 03:14:48+00:00",
      "closed_at": "2025-05-31 01:07:55+00:00",
      "resolution_days": 80.91188657407407
    },
    {
      "number": 12279,
      "title": "Misc. bug: tool call issues with hf unsloth/Qwen2.5-Coder-7B-Instruct-128K-GGUF",
      "state": "closed",
      "comments": 13,
      "body_length": 8020,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-03-09 06:24:52+00:00",
      "closed_at": "2025-03-10 10:36:35+00:00",
      "resolution_days": 1.1748032407407407
    },
    {
      "number": 12253,
      "title": "Eval bug: garbage output right after kv-cache defragmentation for CPU backend",
      "state": "closed",
      "comments": 24,
      "body_length": 18125,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-03-07 14:14:43+00:00",
      "closed_at": "2025-03-11 07:43:33+00:00",
      "resolution_days": 3.7283564814814816
    },
    {
      "number": 12234,
      "title": "Eval bug: Excessive stack usage during tool calling",
      "state": "closed",
      "comments": 23,
      "body_length": 5787,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2025-03-06 21:08:46+00:00",
      "closed_at": "2025-05-02 01:07:57+00:00",
      "resolution_days": 56.166099537037034
    },
    {
      "number": 12232,
      "title": "Eval bug: Phi-4 mini in iOS with xcframework",
      "state": "closed",
      "comments": 12,
      "body_length": 22783,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2025-03-06 19:45:33+00:00",
      "closed_at": "2025-05-13 01:07:52+00:00",
      "resolution_days": 67.22383101851852
    },
    {
      "number": 12147,
      "title": "Misc. bug: vulkan on 6900xt",
      "state": "closed",
      "comments": 24,
      "body_length": 1049,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2025-03-02 16:35:57+00:00",
      "closed_at": "2025-04-11 15:24:47+00:00",
      "resolution_days": 39.950578703703705
    },
    {
      "number": 12106,
      "title": "Regarding llama-bench and llama-parallel commands",
      "state": "closed",
      "comments": 16,
      "body_length": 824,
      "label_names": [],
      "created_at": "2025-02-28 07:59:01+00:00",
      "closed_at": "2025-04-25 04:51:39+00:00",
      "resolution_days": 55.86988425925926
    },
    {
      "number": 12096,
      "title": "Eval bug: llama.cpp returns gibberish on Intel Core Ultra 7 (155H) with ARC iGPU",
      "state": "closed",
      "comments": 17,
      "body_length": 19316,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2025-02-27 15:16:18+00:00",
      "closed_at": "2025-03-06 22:32:04+00:00",
      "resolution_days": 7.302615740740741
    },
    {
      "number": 12091,
      "title": "Feature Request: Support for Phi-4-mini-instruct",
      "state": "closed",
      "comments": 32,
      "body_length": 2397,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2025-02-27 09:03:33+00:00",
      "closed_at": "2025-02-28 19:08:18+00:00",
      "resolution_days": 1.4199652777777778
    },
    {
      "number": 12080,
      "title": "Eval bug: getting assertion error when trying to use a gguf quantized model at inference \"GGML_ASSERT(n_outputs_enc > 0 && \"call llama_encode() first\") failed\"",
      "state": "closed",
      "comments": 28,
      "body_length": 1820,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2025-02-26 10:41:48+00:00",
      "closed_at": "2025-05-04 01:08:01+00:00",
      "resolution_days": 66.60153935185186
    },
    {
      "number": 12012,
      "title": "Eval bug: Several models producing gibberish",
      "state": "closed",
      "comments": 21,
      "body_length": 13471,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2025-02-21 20:52:18+00:00",
      "closed_at": "2025-02-26 00:59:59+00:00",
      "resolution_days": 4.172002314814815
    },
    {
      "number": 12003,
      "title": "Eval bug: does llama.cpp support Intel AMX instruction? how to enable it",
      "state": "closed",
      "comments": 18,
      "body_length": 289,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2025-02-21 13:00:24+00:00",
      "closed_at": "2025-05-04 01:08:02+00:00",
      "resolution_days": 71.50530092592592
    },
    {
      "number": 11970,
      "title": "Misc. bug: The KV cache is sometimes truncated incorrectly when making v1/chat/completions API calls",
      "state": "open",
      "comments": 45,
      "body_length": 5884,
      "label_names": [
        "bug",
        "high priority"
      ],
      "created_at": "2025-02-20 11:20:01+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 11931,
      "title": "Enhancement: Improve ROCm performance on various quants (benchmarks included)",
      "state": "open",
      "comments": 14,
      "body_length": 1713,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2025-02-17 20:03:13+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 11913,
      "title": "Misc. bug: ROCm images cannot be found",
      "state": "open",
      "comments": 17,
      "body_length": 1825,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-02-16 20:54:29+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 11866,
      "title": "Misc. bug: Problems with official jinja templates (Gemma 2, Llama 3.2, Qwen 2.5)",
      "state": "closed",
      "comments": 12,
      "body_length": 1513,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-02-14 09:57:12+00:00",
      "closed_at": "2025-03-10 10:40:33+00:00",
      "resolution_days": 24.030104166666668
    },
    {
      "number": 11861,
      "title": "Misc. bug: Missing <think> tag in response (DeepSeek R1)",
      "state": "closed",
      "comments": 12,
      "body_length": 1195,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2025-02-14 07:02:27+00:00",
      "closed_at": "2025-04-29 01:07:41+00:00",
      "resolution_days": 73.75363425925926
    },
    {
      "number": 11807,
      "title": "Eval bug: rpc backend surport cpu?",
      "state": "closed",
      "comments": 17,
      "body_length": 11070,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2025-02-11 11:23:07+00:00",
      "closed_at": "2025-04-12 01:07:42+00:00",
      "resolution_days": 59.57262731481482
    },
    {
      "number": 11804,
      "title": "Misc. bug: CUDA errors with multi-threaded use",
      "state": "closed",
      "comments": 21,
      "body_length": 5401,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2025-02-11 09:26:11+00:00",
      "closed_at": "2025-05-17 08:27:46+00:00",
      "resolution_days": 94.95943287037036
    },
    {
      "number": 11750,
      "title": "Compile bug: ptxas fatal   : Ptx assembly aborted due to errors",
      "state": "closed",
      "comments": 16,
      "body_length": 10859,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2025-02-08 07:26:56+00:00",
      "closed_at": "2025-02-11 05:08:42+00:00",
      "resolution_days": 2.90400462962963
    },
    {
      "number": 11742,
      "title": "Eval bug: llama-serve ignores SIGINT and SIGTERM when running within a container.",
      "state": "closed",
      "comments": 13,
      "body_length": 1344,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2025-02-07 21:26:15+00:00",
      "closed_at": "2025-02-11 14:51:14+00:00",
      "resolution_days": 3.7256828703703704
    },
    {
      "number": 11695,
      "title": "Compile bug: How to compile llama.cpp with Vulkan for android device",
      "state": "closed",
      "comments": 25,
      "body_length": 23683,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-02-06 04:02:27+00:00",
      "closed_at": "2025-04-03 12:59:23+00:00",
      "resolution_days": 56.37287037037037
    },
    {
      "number": 11598,
      "title": "Misc. bug: Vulcan premature out of memory exception on AMD Instinct MI60",
      "state": "closed",
      "comments": 13,
      "body_length": 2470,
      "label_names": [
        "bug-unconfirmed",
        "Vulkan",
        "stale"
      ],
      "created_at": "2025-02-02 17:21:05+00:00",
      "closed_at": "2025-04-26 01:07:46+00:00",
      "resolution_days": 82.32408564814814
    },
    {
      "number": 11532,
      "title": "Feature Request: MoE only load activated expert(s) to GPU while rest non-used experts are not loaded (to CPU/GPU) for DeekSeek-R1 Inference on consumer GPU",
      "state": "closed",
      "comments": 15,
      "body_length": 2199,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2025-01-30 23:48:43+00:00",
      "closed_at": "2025-04-08 01:07:32+00:00",
      "resolution_days": 67.05473379629629
    },
    {
      "number": 11509,
      "title": "Misc. bug: AMD Rcom command error only with cli tools",
      "state": "closed",
      "comments": 22,
      "body_length": 4718,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2025-01-30 08:39:37+00:00",
      "closed_at": "2025-03-21 01:07:40+00:00",
      "resolution_days": 49.686145833333335
    },
    {
      "number": 11483,
      "title": "Feature Request: Qwen 2.5 VL",
      "state": "closed",
      "comments": 74,
      "body_length": 771,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2025-01-29 11:36:22+00:00",
      "closed_at": "2025-06-26 01:08:02+00:00",
      "resolution_days": 147.56365740740742
    },
    {
      "number": 11474,
      "title": "Research: Benchmarking DeepSeek-R1 IQ1_S 1.58bit",
      "state": "closed",
      "comments": 45,
      "body_length": 5690,
      "label_names": [
        "research \ud83d\udd2c",
        "stale"
      ],
      "created_at": "2025-01-28 23:39:28+00:00",
      "closed_at": "2025-04-25 01:07:52+00:00",
      "resolution_days": 86.06138888888889
    },
    {
      "number": 11467,
      "title": "Feature Request: YuE (music gen)",
      "state": "closed",
      "comments": 14,
      "body_length": 2011,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2025-01-28 16:12:40+00:00",
      "closed_at": "2025-05-05 01:08:00+00:00",
      "resolution_days": 96.37175925925926
    },
    {
      "number": 11421,
      "title": "Eval bug: llama-server generating single letter in a loop and halting (ROCm/Windows)",
      "state": "closed",
      "comments": 12,
      "body_length": 5128,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2025-01-25 16:42:42+00:00",
      "closed_at": "2025-01-25 17:20:54+00:00",
      "resolution_days": 0.02652777777777778
    },
    {
      "number": 11405,
      "title": "Misc. bug: commit 5f0db95 breaks model loading on some AMD gpus",
      "state": "closed",
      "comments": 35,
      "body_length": 1563,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2025-01-25 02:43:07+00:00",
      "closed_at": "2025-01-25 20:28:46+00:00",
      "resolution_days": 0.7400347222222222
    },
    {
      "number": 11361,
      "title": "Eval bug: very slow inference on DeepSeek-R1-Distill-Qwen-32B",
      "state": "closed",
      "comments": 18,
      "body_length": 5247,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2025-01-22 21:38:51+00:00",
      "closed_at": "2025-04-02 01:07:52+00:00",
      "resolution_days": 69.14515046296296
    },
    {
      "number": 11337,
      "title": "[Regression][Bisected] Running out of memory under ROCM/HIP",
      "state": "closed",
      "comments": 20,
      "body_length": 1879,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2025-01-21 21:38:17+00:00",
      "closed_at": "2025-01-22 16:44:42+00:00",
      "resolution_days": 0.7961226851851851
    },
    {
      "number": 11327,
      "title": "Compile bug: Vulkan can not work on Android (cross-compilation from linux) - Aborted without explaination",
      "state": "closed",
      "comments": 34,
      "body_length": 21814,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2025-01-21 10:17:28+00:00",
      "closed_at": "2025-03-22 01:07:37+00:00",
      "resolution_days": 59.618159722222224
    },
    {
      "number": 11325,
      "title": "Eval bug: <think> tag with DeepSeek-R1-Distill-Qwen-1.5B-Q5_K_M.gguf",
      "state": "closed",
      "comments": 28,
      "body_length": 1254,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2025-01-21 06:00:38+00:00",
      "closed_at": "2025-04-30 01:07:54+00:00",
      "resolution_days": 98.79671296296296
    },
    {
      "number": 11290,
      "title": "Feature Request: MiniMax-Text-01 model",
      "state": "closed",
      "comments": 17,
      "body_length": 818,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2025-01-18 15:38:54+00:00",
      "closed_at": "2025-03-12 01:07:33+00:00",
      "resolution_days": 52.39489583333334
    },
    {
      "number": 11267,
      "title": "Misc. bug: llama-cli: error while loading shared libraries: libllama.so: cannot open shared object file: No such file or directory",
      "state": "closed",
      "comments": 12,
      "body_length": 998,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2025-01-16 18:02:42+00:00",
      "closed_at": "2025-04-09 01:07:46+00:00",
      "resolution_days": 82.29518518518519
    },
    {
      "number": 11153,
      "title": "Misc. bug: Very bad performance on Qwen 2 with HIP/ROCm",
      "state": "closed",
      "comments": 15,
      "body_length": 4327,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2025-01-09 03:48:24+00:00",
      "closed_at": "2025-01-31 10:08:08+00:00",
      "resolution_days": 22.263703703703705
    },
    {
      "number": 11123,
      "title": "llama-cli: error while loading shared libraries: libllama.so: cannot open shared object file: No such file or directory",
      "state": "closed",
      "comments": 12,
      "body_length": 887,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2025-01-07 14:37:09+00:00",
      "closed_at": "2025-04-01 01:08:06+00:00",
      "resolution_days": 83.43815972222222
    },
    {
      "number": 11091,
      "title": "Again, the releases don't have the libraries.",
      "state": "closed",
      "comments": 16,
      "body_length": 176,
      "label_names": [],
      "created_at": "2025-01-05 17:31:06+00:00",
      "closed_at": "2025-01-24 16:41:31+00:00",
      "resolution_days": 18.96556712962963
    },
    {
      "number": 11050,
      "title": "Feature Request: Add support for Kokoro TTS",
      "state": "open",
      "comments": 33,
      "body_length": 876,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2025-01-03 05:28:06+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 11044,
      "title": "Misc. bug: SYCL out of memory error",
      "state": "closed",
      "comments": 22,
      "body_length": 15076,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2025-01-02 14:34:17+00:00",
      "closed_at": "2025-03-06 01:07:33+00:00",
      "resolution_days": 62.43976851851852
    },
    {
      "number": 11004,
      "title": "Feature Request: Split model over multiple Vulkan GPUs",
      "state": "closed",
      "comments": 14,
      "body_length": 1712,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-12-28 15:33:41+00:00",
      "closed_at": "2025-03-04 01:07:41+00:00",
      "resolution_days": 65.39861111111111
    },
    {
      "number": 10984,
      "title": "Misc. bug: Buffer offset is not aligned on macOS / Intel / Vulkan",
      "state": "closed",
      "comments": 18,
      "body_length": 62485,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2024-12-26 12:33:44+00:00",
      "closed_at": "2025-05-04 09:00:02+00:00",
      "resolution_days": 128.8515972222222
    },
    {
      "number": 10982,
      "title": "Research: Performance differences between Metal (macOS) and Vulkan (Linux)",
      "state": "closed",
      "comments": 14,
      "body_length": 3624,
      "label_names": [
        "research \ud83d\udd2c",
        "stale"
      ],
      "created_at": "2024-12-26 11:12:21+00:00",
      "closed_at": "2025-05-04 01:08:09+00:00",
      "resolution_days": 128.58041666666668
    },
    {
      "number": 10981,
      "title": "Feature Request: add DeepSeek-v3 support",
      "state": "closed",
      "comments": 64,
      "body_length": 1155,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2024-12-26 11:08:12+00:00",
      "closed_at": "2025-01-04 20:06:12+00:00",
      "resolution_days": 9.373611111111112
    },
    {
      "number": 10976,
      "title": "Misc. bug: Unsupported op \"CPY\" / Segmentation fault on Metal",
      "state": "closed",
      "comments": 13,
      "body_length": 1482,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2024-12-25 18:06:13+00:00",
      "closed_at": "2025-02-25 09:31:16+00:00",
      "resolution_days": 61.64239583333333
    },
    {
      "number": 10929,
      "title": "Misc. bug: All llama executables exit immediately without console output",
      "state": "closed",
      "comments": 15,
      "body_length": 9636,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2024-12-21 08:24:04+00:00",
      "closed_at": "2025-03-26 01:07:39+00:00",
      "resolution_days": 94.69693287037038
    },
    {
      "number": 10914,
      "title": "[llama-server] Duplicated text being produced during streaming generation with stop strings",
      "state": "closed",
      "comments": 14,
      "body_length": 8335,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2024-12-20 07:33:43+00:00",
      "closed_at": "2024-12-28 15:08:55+00:00",
      "resolution_days": 8.31611111111111
    },
    {
      "number": 10860,
      "title": "Misc. bug: [SERVER] Multiple slots, generation speed is degraded after each generation/slot used",
      "state": "closed",
      "comments": 20,
      "body_length": 9086,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2024-12-17 07:59:39+00:00",
      "closed_at": "2025-07-16 13:49:55+00:00",
      "resolution_days": 211.24324074074073
    },
    {
      "number": 10843,
      "title": "Eval bug: Qwen2-VL Hallucinates image content on Vulkan backend",
      "state": "closed",
      "comments": 23,
      "body_length": 5445,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2024-12-15 17:36:55+00:00",
      "closed_at": "2025-02-17 22:43:03+00:00",
      "resolution_days": 64.21259259259259
    },
    {
      "number": 10816,
      "title": "Feature Request: Support for C4AI Command R7B / Cohere2ForCausalLM",
      "state": "closed",
      "comments": 12,
      "body_length": 1630,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2024-12-13 18:54:15+00:00",
      "closed_at": "2025-01-04 14:33:33+00:00",
      "resolution_days": 21.818958333333335
    },
    {
      "number": 10768,
      "title": "Misc. bug: Virus detected",
      "state": "closed",
      "comments": 36,
      "body_length": 522,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2024-12-10 18:44:44+00:00",
      "closed_at": "2025-03-28 01:07:37+00:00",
      "resolution_days": 107.2658912037037
    },
    {
      "number": 10758,
      "title": "Feature Request: Source code highlight and math formula rendering",
      "state": "closed",
      "comments": 17,
      "body_length": 3107,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-12-10 13:53:11+00:00",
      "closed_at": "2025-01-30 01:07:08+00:00",
      "resolution_days": 50.468020833333334
    },
    {
      "number": 10757,
      "title": "Misc. bug: Q4_0 with runtime repacking not working as expected (TYPE_Q4_0_4_4 REMOVED)",
      "state": "closed",
      "comments": 24,
      "body_length": 40360,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2024-12-10 13:47:39+00:00",
      "closed_at": "2024-12-18 22:21:43+00:00",
      "resolution_days": 8.356990740740741
    },
    {
      "number": 10747,
      "title": "Compile bug: ios swift xcode build error when upgrade to llama : use cmake for swift build ",
      "state": "open",
      "comments": 41,
      "body_length": 6110,
      "label_names": [
        "help wanted",
        "good first issue",
        "build"
      ],
      "created_at": "2024-12-10 05:12:25+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 10712,
      "title": "Compile bug: Can't build vulkan backend",
      "state": "closed",
      "comments": 25,
      "body_length": 3965,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2024-12-07 21:39:47+00:00",
      "closed_at": "2024-12-09 18:09:55+00:00",
      "resolution_days": 1.8542592592592593
    },
    {
      "number": 10710,
      "title": "Eval bug: ~~Q2_K and Q3_K~~ Q8_0 not working on Vulkan anymore on RX 5700XT",
      "state": "closed",
      "comments": 54,
      "body_length": 4867,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2024-12-07 17:08:41+00:00",
      "closed_at": "2025-05-17 01:07:56+00:00",
      "resolution_days": 160.3328125
    },
    {
      "number": 10696,
      "title": "Misc. bug: Unable to load model",
      "state": "closed",
      "comments": 13,
      "body_length": 10182,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2024-12-06 22:20:31+00:00",
      "closed_at": "2024-12-08 19:34:20+00:00",
      "resolution_days": 1.8845949074074073
    },
    {
      "number": 10664,
      "title": "Performance bug: Speculative Decoding Performance Degradation",
      "state": "closed",
      "comments": 20,
      "body_length": 32804,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2024-12-04 19:26:25+00:00",
      "closed_at": "2025-02-19 01:12:45+00:00",
      "resolution_days": 76.24050925925926
    },
    {
      "number": 10556,
      "title": "[CANN] Compile bug: no matching function for call to 'CastIntrinsicsImpl'",
      "state": "closed",
      "comments": 44,
      "body_length": 26578,
      "label_names": [
        "Ascend NPU"
      ],
      "created_at": "2024-11-28 03:48:05+00:00",
      "closed_at": "2024-11-29 06:46:03+00:00",
      "resolution_days": 1.123587962962963
    },
    {
      "number": 10555,
      "title": "Compile bug: ggml-impl.h(314): error: identifier \"__fp16\" is undefined on Jetson AGX Xavier",
      "state": "closed",
      "comments": 14,
      "body_length": 5395,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2024-11-28 03:33:52+00:00",
      "closed_at": "2024-12-04 00:41:38+00:00",
      "resolution_days": 5.8803935185185185
    },
    {
      "number": 10552,
      "title": "Misc. bug: [server] Using q8_0 for KV cache reduces performance when also using a draft model",
      "state": "closed",
      "comments": 12,
      "body_length": 4705,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2024-11-27 22:30:22+00:00",
      "closed_at": "2024-12-03 17:24:43+00:00",
      "resolution_days": 5.787743055555556
    },
    {
      "number": 10547,
      "title": "Eval bug: issues with draft model and Cline+VSCode",
      "state": "closed",
      "comments": 27,
      "body_length": 22514,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2024-11-27 14:21:13+00:00",
      "closed_at": "2024-12-04 20:38:21+00:00",
      "resolution_days": 7.261898148148148
    },
    {
      "number": 10528,
      "title": "Misc. bug: Inconsistent Vulkan segfault",
      "state": "open",
      "comments": 65,
      "body_length": 2728,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-11-26 19:54:03+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 10453,
      "title": "ggml : add ANE backend",
      "state": "open",
      "comments": 13,
      "body_length": 213,
      "label_names": [
        "help wanted",
        "research \ud83d\udd2c",
        "roadmap"
      ],
      "created_at": "2024-11-22 08:20:22+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 10439,
      "title": "Bug: Flash Attention performs worse under ROCM",
      "state": "closed",
      "comments": 49,
      "body_length": 856,
      "label_names": [
        "bug-unconfirmed",
        "medium severity"
      ],
      "created_at": "2024-11-20 19:57:45+00:00",
      "closed_at": "2025-03-03 21:30:41+00:00",
      "resolution_days": 103.06453703703704
    },
    {
      "number": 10435,
      "title": "Bug: Severe Performance Degradation on Q4_0 CPU-only with MacOS / Apple Silicon M2, after PR#9921 / Version 4081",
      "state": "open",
      "comments": 12,
      "body_length": 2897,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-11-20 17:06:21+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 10252,
      "title": "Bug: CANN: Inference result garbled",
      "state": "closed",
      "comments": 14,
      "body_length": 12546,
      "label_names": [
        "bug-unconfirmed",
        "stale",
        "high severity"
      ],
      "created_at": "2024-11-11 03:35:51+00:00",
      "closed_at": "2025-01-13 01:07:32+00:00",
      "resolution_days": 62.89700231481481
    },
    {
      "number": 10234,
      "title": "Bug: CUBLAS_STATUS_INTERNAL_ERROR when using --gpu-layers on ROCm 6.2",
      "state": "closed",
      "comments": 14,
      "body_length": 2269,
      "label_names": [
        "bug-unconfirmed",
        "high severity"
      ],
      "created_at": "2024-11-09 10:23:51+00:00",
      "closed_at": "2024-11-15 01:20:02+00:00",
      "resolution_days": 5.622349537037037
    },
    {
      "number": 10180,
      "title": "ggml : refactor ggml-cpu.c into multiple C++ source files",
      "state": "open",
      "comments": 17,
      "body_length": 293,
      "label_names": [
        "refactoring",
        "roadmap"
      ],
      "created_at": "2024-11-05 07:12:48+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 10176,
      "title": "Bug: Speculative Decoding \"Segmentation fault (core dumped)\"",
      "state": "closed",
      "comments": 13,
      "body_length": 20097,
      "label_names": [
        "bug",
        "low severity"
      ],
      "created_at": "2024-11-04 23:06:38+00:00",
      "closed_at": "2024-11-14 09:44:16+00:00",
      "resolution_days": 9.442800925925926
    },
    {
      "number": 10161,
      "title": "Bug: CANN  E89999",
      "state": "closed",
      "comments": 22,
      "body_length": 1406,
      "label_names": [
        "bug-unconfirmed",
        "stale",
        "low severity",
        "Ascend NPU"
      ],
      "created_at": "2024-11-04 09:49:12+00:00",
      "closed_at": "2025-01-27 01:07:16+00:00",
      "resolution_days": 83.63754629629629
    },
    {
      "number": 10095,
      "title": "Feature Request: RPC offloading using a local model copy",
      "state": "closed",
      "comments": 18,
      "body_length": 1047,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2024-10-30 13:04:52+00:00",
      "closed_at": "2025-03-28 07:44:57+00:00",
      "resolution_days": 148.77783564814814
    },
    {
      "number": 10028,
      "title": "Feature Request: Support for DeciLMForCausalLM",
      "state": "closed",
      "comments": 16,
      "body_length": 1108,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-10-24 07:05:35+00:00",
      "closed_at": "2025-01-11 01:07:33+00:00",
      "resolution_days": 78.75136574074074
    },
    {
      "number": 10011,
      "title": "Bug: K cache without FA goes Nan on Llama 3.1.",
      "state": "closed",
      "comments": 22,
      "body_length": 10870,
      "label_names": [
        "bug-unconfirmed",
        "high severity"
      ],
      "created_at": "2024-10-23 07:52:32+00:00",
      "closed_at": "2024-10-24 12:40:24+00:00",
      "resolution_days": 1.1999074074074074
    },
    {
      "number": 10005,
      "title": "llama : enable FA by default and disable it per-layer",
      "state": "open",
      "comments": 18,
      "body_length": 617,
      "label_names": [
        "enhancement",
        "roadmap"
      ],
      "created_at": "2024-10-22 14:07:59+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 9991,
      "title": "Bug: Unexpected output from Granite 3.0 MoE 1b when all layers on NVIDIA GPU",
      "state": "closed",
      "comments": 12,
      "body_length": 56080,
      "label_names": [
        "bug-unconfirmed",
        "medium severity"
      ],
      "created_at": "2024-10-21 22:53:15+00:00",
      "closed_at": "2024-10-27 18:59:59+00:00",
      "resolution_days": 5.838009259259259
    },
    {
      "number": 9988,
      "title": "Bug: Memory Leak in llama-server after exit",
      "state": "closed",
      "comments": 16,
      "body_length": 37167,
      "label_names": [
        "bug-unconfirmed",
        "stale",
        "medium severity"
      ],
      "created_at": "2024-10-21 18:43:26+00:00",
      "closed_at": "2024-11-25 01:59:58+00:00",
      "resolution_days": 34.303148148148146
    },
    {
      "number": 9978,
      "title": "Bug: llama-server crash with `--embeddings`",
      "state": "closed",
      "comments": 13,
      "body_length": 3514,
      "label_names": [
        "bug",
        "critical severity"
      ],
      "created_at": "2024-10-21 09:30:09+00:00",
      "closed_at": "2024-10-25 07:13:47+00:00",
      "resolution_days": 3.905300925925926
    },
    {
      "number": 9949,
      "title": "Bug: Segmentation fault when running speculative decoding",
      "state": "open",
      "comments": 16,
      "body_length": 19062,
      "label_names": [
        "bug",
        "critical severity"
      ],
      "created_at": "2024-10-19 04:03:33+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 9914,
      "title": "Feature Request: Support for Ministral-8B-Instruct-2410",
      "state": "closed",
      "comments": 14,
      "body_length": 2129,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-10-16 17:00:02+00:00",
      "closed_at": "2024-12-04 01:09:59+00:00",
      "resolution_days": 48.340243055555554
    },
    {
      "number": 9899,
      "title": "Bug: imatrix crash - nan detected in blk.1.attn_output.weight",
      "state": "closed",
      "comments": 15,
      "body_length": 11019,
      "label_names": [
        "bug-unconfirmed",
        "stale",
        "low severity"
      ],
      "created_at": "2024-10-15 11:58:27+00:00",
      "closed_at": "2024-12-04 01:10:00+00:00",
      "resolution_days": 49.5496875
    },
    {
      "number": 9848,
      "title": "Bug: Erroneous Output in llama-cli",
      "state": "closed",
      "comments": 15,
      "body_length": 2005,
      "label_names": [
        "bug-unconfirmed",
        "stale",
        "high severity"
      ],
      "created_at": "2024-10-11 17:10:51+00:00",
      "closed_at": "2024-12-01 01:08:07+00:00",
      "resolution_days": 50.331435185185185
    },
    {
      "number": 9811,
      "title": "server : remove system prompt support",
      "state": "closed",
      "comments": 13,
      "body_length": 548,
      "label_names": [
        "refactoring",
        "server"
      ],
      "created_at": "2024-10-09 19:10:10+00:00",
      "closed_at": "2024-10-12 11:51:55+00:00",
      "resolution_days": 2.695659722222222
    },
    {
      "number": 9761,
      "title": "Bug: Row Split Mode - Segmentation fault after model load on ROCm multi-gpu",
      "state": "closed",
      "comments": 15,
      "body_length": 9949,
      "label_names": [
        "bug-unconfirmed",
        "stale",
        "critical severity"
      ],
      "created_at": "2024-10-06 13:01:21+00:00",
      "closed_at": "2024-11-27 01:07:40+00:00",
      "resolution_days": 51.504386574074076
    },
    {
      "number": 9722,
      "title": "Feature Request: SYCL CI online",
      "state": "closed",
      "comments": 12,
      "body_length": 977,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2024-10-03 03:14:20+00:00",
      "closed_at": "2024-10-24 18:31:10+00:00",
      "resolution_days": 21.636689814814815
    },
    {
      "number": 9663,
      "title": "Feature Request: Add Support for MllamaForConditionalGeneration to Convert Llama 3.2 Vision Models to GGUF Format",
      "state": "closed",
      "comments": 14,
      "body_length": 913,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-09-27 00:05:12+00:00",
      "closed_at": "2025-01-19 01:07:46+00:00",
      "resolution_days": 114.04344907407408
    },
    {
      "number": 9645,
      "title": "Feature Request: Molmo 72B vision support",
      "state": "closed",
      "comments": 20,
      "body_length": 829,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-09-25 21:17:10+00:00",
      "closed_at": "2025-02-26 01:07:27+00:00",
      "resolution_days": 153.15991898148147
    },
    {
      "number": 9643,
      "title": "Llama-3.2 11B Vision Support",
      "state": "closed",
      "comments": 47,
      "body_length": 35,
      "label_names": [],
      "created_at": "2024-09-25 20:00:17+00:00",
      "closed_at": "2025-03-16 22:51:45+00:00",
      "resolution_days": 172.11907407407406
    },
    {
      "number": 9612,
      "title": "Bug: [SYCL] crash since b-3805",
      "state": "closed",
      "comments": 43,
      "body_length": 1787,
      "label_names": [
        "bug-unconfirmed",
        "critical severity"
      ],
      "created_at": "2024-09-23 19:07:11+00:00",
      "closed_at": "2024-10-21 09:26:22+00:00",
      "resolution_days": 27.59665509259259
    },
    {
      "number": 9585,
      "title": "Feature Request: Support Jina V3 arch",
      "state": "open",
      "comments": 13,
      "body_length": 2034,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2024-09-21 22:00:06+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 9530,
      "title": "Bug:  Lower performance in pre-built binary llama-server,  Since llama-b3681-bin-win-cuda-cu12.2.0-x64",
      "state": "closed",
      "comments": 13,
      "body_length": 37438,
      "label_names": [
        "bug-unconfirmed",
        "medium severity"
      ],
      "created_at": "2024-09-18 03:17:16+00:00",
      "closed_at": "2024-09-24 06:03:18+00:00",
      "resolution_days": 6.115300925925926
    },
    {
      "number": 9505,
      "title": "Bug: Lower performance in SYCL vs IPEX LLM. ",
      "state": "closed",
      "comments": 16,
      "body_length": 25352,
      "label_names": [
        "bug-unconfirmed",
        "stale",
        "medium severity"
      ],
      "created_at": "2024-09-16 09:42:37+00:00",
      "closed_at": "2024-11-02 01:07:13+00:00",
      "resolution_days": 46.64208333333333
    },
    {
      "number": 9492,
      "title": "Bug: llama-server api first query very slow",
      "state": "open",
      "comments": 13,
      "body_length": 16756,
      "label_names": [
        "bug",
        "medium severity"
      ],
      "created_at": "2024-09-15 06:37:09+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 9473,
      "title": "Bug: Build failure in master on Ubuntu 24.04 with CUDA enabled",
      "state": "closed",
      "comments": 23,
      "body_length": 2041,
      "label_names": [
        "bug-unconfirmed",
        "high severity"
      ],
      "created_at": "2024-09-13 15:35:09+00:00",
      "closed_at": "2024-09-16 14:22:09+00:00",
      "resolution_days": 2.9493055555555556
    },
    {
      "number": 9440,
      "title": "Feature Request: Pixtral by Mistral support (pixtral-12b-240910)",
      "state": "closed",
      "comments": 14,
      "body_length": 1392,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-09-11 18:03:29+00:00",
      "closed_at": "2025-02-08 01:07:14+00:00",
      "resolution_days": 149.29427083333334
    },
    {
      "number": 9416,
      "title": "Bug: llama-server cannot open shared object file libllama.so after building from SYCL-Dockerfile",
      "state": "closed",
      "comments": 15,
      "body_length": 1225,
      "label_names": [
        "bug-unconfirmed",
        "stale",
        "high severity"
      ],
      "created_at": "2024-09-10 17:18:10+00:00",
      "closed_at": "2024-10-29 01:07:25+00:00",
      "resolution_days": 48.32586805555555
    },
    {
      "number": 9390,
      "title": "server : ability to disable context shift",
      "state": "closed",
      "comments": 14,
      "body_length": 607,
      "label_names": [
        "enhancement",
        "server"
      ],
      "created_at": "2024-09-09 14:52:29+00:00",
      "closed_at": "2024-09-23 20:23:55+00:00",
      "resolution_days": 14.230162037037037
    },
    {
      "number": 9315,
      "title": "Bug: RWKV 6 Finch 3B+ models crash llama.cpp with CPU backend",
      "state": "closed",
      "comments": 26,
      "body_length": 7084,
      "label_names": [
        "bug-unconfirmed",
        "critical severity"
      ],
      "created_at": "2024-09-04 17:10:11+00:00",
      "closed_at": "2024-09-12 11:25:17+00:00",
      "resolution_days": 7.760486111111111
    },
    {
      "number": 9291,
      "title": "changelog : `llama-server` REST API",
      "state": "open",
      "comments": 16,
      "body_length": 2193,
      "label_names": [
        "documentation",
        "roadmap"
      ],
      "created_at": "2024-09-03 06:56:11+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 9246,
      "title": "Feature Request: Support for Qwen2-VL",
      "state": "closed",
      "comments": 131,
      "body_length": 982,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-08-29 22:34:11+00:00",
      "closed_at": "2025-05-25 01:08:24+00:00",
      "resolution_days": 268.10709490740743
    },
    {
      "number": 9195,
      "title": "Bug: Regular crashes due to invalid UTF-8 in output.",
      "state": "closed",
      "comments": 13,
      "body_length": 605,
      "label_names": [
        "bug-unconfirmed",
        "medium severity"
      ],
      "created_at": "2024-08-27 00:51:23+00:00",
      "closed_at": "2024-08-29 23:17:14+00:00",
      "resolution_days": 2.9346180555555557
    },
    {
      "number": 9181,
      "title": "Feature Request: NPU Support",
      "state": "closed",
      "comments": 12,
      "body_length": 972,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-08-26 04:56:54+00:00",
      "closed_at": "2024-11-04 01:07:36+00:00",
      "resolution_days": 69.84076388888889
    },
    {
      "number": 9168,
      "title": " Request to use Phi-3.5-MoE-instruct",
      "state": "closed",
      "comments": 15,
      "body_length": 1015,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-08-25 14:55:32+00:00",
      "closed_at": "2024-10-25 01:28:12+00:00",
      "resolution_days": 60.43935185185185
    },
    {
      "number": 9135,
      "title": "Bug: Difficulties Using LLaMa.cpp Server and --prompt-cache [FNAME] (not supported?)",
      "state": "closed",
      "comments": 16,
      "body_length": 1534,
      "label_names": [
        "bug-unconfirmed",
        "stale",
        "high severity"
      ],
      "created_at": "2024-08-22 19:19:23+00:00",
      "closed_at": "2024-12-01 01:08:11+00:00",
      "resolution_days": 100.24222222222222
    },
    {
      "number": 9119,
      "title": "Feature Request: Add support for Phi-3.5 MoE and Vision Instruct",
      "state": "closed",
      "comments": 24,
      "body_length": 927,
      "label_names": [
        "enhancement",
        "model",
        "stale"
      ],
      "created_at": "2024-08-21 14:32:40+00:00",
      "closed_at": "2025-02-12 01:07:21+00:00",
      "resolution_days": 174.4407523148148
    },
    {
      "number": 9106,
      "title": "Bug: Intel Arc - not working at all",
      "state": "closed",
      "comments": 28,
      "body_length": 9905,
      "label_names": [
        "bug-unconfirmed",
        "stale",
        "SYCL",
        "critical severity"
      ],
      "created_at": "2024-08-20 19:45:26+00:00",
      "closed_at": "2024-12-17 01:07:43+00:00",
      "resolution_days": 118.22380787037036
    },
    {
      "number": 9066,
      "title": "Bug: MiniCPM-V-2.6 commit d565bb2fd5a2a58b9924a7a34e77a87c78c52137 causing crash in moondream",
      "state": "closed",
      "comments": 15,
      "body_length": 21381,
      "label_names": [
        "bug-unconfirmed",
        "critical severity"
      ],
      "created_at": "2024-08-17 17:14:50+00:00",
      "closed_at": "2024-08-21 07:45:50+00:00",
      "resolution_days": 3.604861111111111
    },
    {
      "number": 9061,
      "title": "Bug: failed to load model with docker image",
      "state": "closed",
      "comments": 13,
      "body_length": 951,
      "label_names": [
        "bug-unconfirmed",
        "high severity"
      ],
      "created_at": "2024-08-17 05:03:50+00:00",
      "closed_at": "2024-08-17 10:22:16+00:00",
      "resolution_days": 0.22113425925925925
    },
    {
      "number": 8886,
      "title": "Mac build with metal flag --main-gpu",
      "state": "closed",
      "comments": 14,
      "body_length": 64792,
      "label_names": [
        "bug-unconfirmed",
        "stale",
        "low severity"
      ],
      "created_at": "2024-08-06 14:54:17+00:00",
      "closed_at": "2024-09-27 01:07:25+00:00",
      "resolution_days": 51.42578703703704
    },
    {
      "number": 8859,
      "title": "Bug: SYCL release not working on intel i7 8665U iGPU UHD Graphics 620",
      "state": "closed",
      "comments": 14,
      "body_length": 688,
      "label_names": [
        "bug-unconfirmed",
        "stale",
        "medium severity"
      ],
      "created_at": "2024-08-04 19:28:40+00:00",
      "closed_at": "2024-10-22 01:07:26+00:00",
      "resolution_days": 78.23525462962964
    },
    {
      "number": 8828,
      "title": "Bug: Vulkan backend crash on model loading",
      "state": "closed",
      "comments": 13,
      "body_length": 13588,
      "label_names": [
        "bug-unconfirmed",
        "critical severity"
      ],
      "created_at": "2024-08-02 13:32:45+00:00",
      "closed_at": "2024-08-18 13:58:08+00:00",
      "resolution_days": 16.017627314814813
    },
    {
      "number": 8817,
      "title": "Bug:  n_ctx will reuse n_ctx_train when --ctx_size not set and make deepseek-v2 models meet out of memory crash even on a small output length.",
      "state": "closed",
      "comments": 12,
      "body_length": 575,
      "label_names": [
        "bug",
        "bug-unconfirmed",
        "medium severity"
      ],
      "created_at": "2024-08-02 05:54:10+00:00",
      "closed_at": "2024-11-02 13:18:57+00:00",
      "resolution_days": 92.30887731481481
    },
    {
      "number": 8808,
      "title": "Feature Request: Instructions how to correctly use/convert original llama3.1 instruct .pth model",
      "state": "closed",
      "comments": 13,
      "body_length": 1714,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2024-08-01 09:53:20+00:00",
      "closed_at": "2024-08-20 09:39:48+00:00",
      "resolution_days": 18.990601851851853
    },
    {
      "number": 8760,
      "title": "Bug: llamacpp for CPU/GPU (avx avx2) quants IQ1xx, IQ2xx, IQ3xx are overheating (CPU 90C) CPU ryzen 9 7950x3d but IQ4xx and other quants not (CPU 65C) ",
      "state": "closed",
      "comments": 31,
      "body_length": 23079,
      "label_names": [
        "stale"
      ],
      "created_at": "2024-07-29 22:08:21+00:00",
      "closed_at": "2024-10-12 01:13:14+00:00",
      "resolution_days": 74.1283912037037
    },
    {
      "number": 8736,
      "title": "Refactor: decide the future of llama_tensor_get_type()",
      "state": "closed",
      "comments": 14,
      "body_length": 2981,
      "label_names": [
        "stale"
      ],
      "created_at": "2024-07-28 12:38:18+00:00",
      "closed_at": "2024-11-04 01:07:40+00:00",
      "resolution_days": 98.52039351851852
    },
    {
      "number": 8730,
      "title": "Bug: Llama 3.1 might not be fully supported yet",
      "state": "closed",
      "comments": 17,
      "body_length": 1947,
      "label_names": [
        "bug-unconfirmed",
        "medium severity"
      ],
      "created_at": "2024-07-28 01:42:02+00:00",
      "closed_at": "2024-08-04 22:28:58+00:00",
      "resolution_days": 7.865925925925926
    },
    {
      "number": 8724,
      "title": "Improve `cvector-generator`",
      "state": "open",
      "comments": 33,
      "body_length": 566,
      "label_names": [
        "enhancement",
        "help wanted",
        "good first issue"
      ],
      "created_at": "2024-07-27 13:05:20+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 8705,
      "title": "How to utilize GPU on Android to accelerate inference?",
      "state": "closed",
      "comments": 18,
      "body_length": 493,
      "label_names": [
        "stale"
      ],
      "created_at": "2024-07-26 07:30:12+00:00",
      "closed_at": "2024-12-31 01:07:28+00:00",
      "resolution_days": 157.73421296296297
    },
    {
      "number": 8685,
      "title": "Bug: (CUDA) Corrupted output when offloading to multiple GPUs",
      "state": "closed",
      "comments": 22,
      "body_length": 3844,
      "label_names": [
        "bug-unconfirmed",
        "medium severity"
      ],
      "created_at": "2024-07-25 10:02:08+00:00",
      "closed_at": "2024-08-07 11:29:04+00:00",
      "resolution_days": 13.06037037037037
    },
    {
      "number": 8684,
      "title": "Feature Request: Multi core support for full GPU offload",
      "state": "closed",
      "comments": 25,
      "body_length": 3285,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2024-07-25 08:18:13+00:00",
      "closed_at": "2024-07-26 23:20:20+00:00",
      "resolution_days": 1.6264699074074074
    },
    {
      "number": 8650,
      "title": "Feature Request: Proper Llama 3.1 Support in llama.cpp",
      "state": "closed",
      "comments": 138,
      "body_length": 1356,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2024-07-23 16:19:17+00:00",
      "closed_at": "2024-07-27 12:03:46+00:00",
      "resolution_days": 3.82255787037037
    },
    {
      "number": 8593,
      "title": "Random seed possible problems.",
      "state": "closed",
      "comments": 20,
      "body_length": 371,
      "label_names": [
        "stale"
      ],
      "created_at": "2024-07-19 19:01:20+00:00",
      "closed_at": "2024-10-19 01:07:19+00:00",
      "resolution_days": 91.2541550925926
    },
    {
      "number": 8577,
      "title": "Support Mistral-Nemo-Instruct-2407 128K",
      "state": "closed",
      "comments": 61,
      "body_length": 691,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-07-18 19:59:40+00:00",
      "closed_at": "2024-12-16 01:07:48+00:00",
      "resolution_days": 150.21398148148148
    },
    {
      "number": 8569,
      "title": "Bug: [SYCL] Backend seems not stable",
      "state": "closed",
      "comments": 13,
      "body_length": 1067,
      "label_names": [
        "bug-unconfirmed",
        "medium severity"
      ],
      "created_at": "2024-07-18 13:01:16+00:00",
      "closed_at": "2024-07-23 08:04:41+00:00",
      "resolution_days": 4.794039351851852
    },
    {
      "number": 8566,
      "title": "llama : reimplement logging",
      "state": "closed",
      "comments": 13,
      "body_length": 181,
      "label_names": [
        "enhancement",
        "refactoring"
      ],
      "created_at": "2024-07-18 11:16:04+00:00",
      "closed_at": "2024-09-15 17:49:48+00:00",
      "resolution_days": 59.27342592592593
    },
    {
      "number": 8555,
      "title": "llama : support reranking API endpoint and models",
      "state": "closed",
      "comments": 22,
      "body_length": 1289,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2024-07-18 07:44:30+00:00",
      "closed_at": "2024-09-29 12:03:42+00:00",
      "resolution_days": 73.18
    },
    {
      "number": 8551,
      "title": "Bug: InvalidModule: Invalid SPIR-V module: input SPIR-V module uses extension 'SPV_INTEL_memory_access_aliasing' which were disabled by --spirv-ext option",
      "state": "closed",
      "comments": 17,
      "body_length": 10893,
      "label_names": [
        "bug-unconfirmed",
        "stale",
        "SYCL",
        "medium severity"
      ],
      "created_at": "2024-07-18 04:49:20+00:00",
      "closed_at": "2024-09-06 01:07:04+00:00",
      "resolution_days": 49.84564814814815
    },
    {
      "number": 8519,
      "title": "Feature Request: Support Codestral Mamba",
      "state": "closed",
      "comments": 16,
      "body_length": 972,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2024-07-16 16:22:42+00:00",
      "closed_at": "2025-07-02 17:10:26+00:00",
      "resolution_days": 351.03314814814814
    },
    {
      "number": 8499,
      "title": "Bug: Weird output from llama-speculative",
      "state": "closed",
      "comments": 15,
      "body_length": 3235,
      "label_names": [
        "bug-unconfirmed",
        "stale",
        "medium severity"
      ],
      "created_at": "2024-07-16 04:56:46+00:00",
      "closed_at": "2024-09-05 01:07:05+00:00",
      "resolution_days": 50.840497685185184
    },
    {
      "number": 8485,
      "title": "Feature Request: T-MAC: CPU Renaissance via Table Lookup for Low-Bit LLM Deployment on Edge",
      "state": "closed",
      "comments": 12,
      "body_length": 2784,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-07-15 04:55:03+00:00",
      "closed_at": "2024-09-30 01:13:35+00:00",
      "resolution_days": 76.84620370370371
    },
    {
      "number": 8455,
      "title": "Bug: llama.cpp with Vulkan not running on Snapdragon X + Windows (Copilot+PCs)",
      "state": "closed",
      "comments": 25,
      "body_length": 1499,
      "label_names": [
        "bug-unconfirmed",
        "stale",
        "low severity"
      ],
      "created_at": "2024-07-12 10:25:53+00:00",
      "closed_at": "2025-01-31 01:07:14+00:00",
      "resolution_days": 202.6120486111111
    },
    {
      "number": 8446,
      "title": "Bug: ggml-aarch64.c does not compile on Windows ARM64 with MSVC",
      "state": "closed",
      "comments": 12,
      "body_length": 963,
      "label_names": [
        "bug-unconfirmed",
        "medium severity"
      ],
      "created_at": "2024-07-12 08:21:28+00:00",
      "closed_at": "2024-07-25 16:01:01+00:00",
      "resolution_days": 13.319131944444445
    },
    {
      "number": 8445,
      "title": "Bug:  After converting the InternLM2 7b from LLamaFactory and importing it into ollama, i get an error: tensor 'token_embd.weight' has wrong shape.",
      "state": "closed",
      "comments": 20,
      "body_length": 5351,
      "label_names": [
        "bug-unconfirmed",
        "stale",
        "low severity"
      ],
      "created_at": "2024-07-12 05:30:48+00:00",
      "closed_at": "2024-10-10 01:07:27+00:00",
      "resolution_days": 89.81711805555555
    },
    {
      "number": 8405,
      "title": "Bug: InternLM 2.5 Chat Tool Calls: Incorrect and Inconsistent Formatting",
      "state": "closed",
      "comments": 14,
      "body_length": 6817,
      "label_names": [
        "bug-unconfirmed",
        "stale",
        "low severity"
      ],
      "created_at": "2024-07-10 05:30:12+00:00",
      "closed_at": "2024-09-01 01:07:42+00:00",
      "resolution_days": 52.817708333333336
    },
    {
      "number": 8328,
      "title": "Huge performance degradation using latest branch on Intel Core Ultra 7 155H",
      "state": "closed",
      "comments": 15,
      "body_length": 5234,
      "label_names": [
        "bug-unconfirmed",
        "stale",
        "medium severity"
      ],
      "created_at": "2024-07-05 14:39:30+00:00",
      "closed_at": "2024-09-19 01:08:13+00:00",
      "resolution_days": 75.4366087962963
    },
    {
      "number": 8285,
      "title": "Add support for InternLM 2.5 1M context.  Should be as good as command r+",
      "state": "closed",
      "comments": 14,
      "body_length": 722,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-07-04 01:34:58+00:00",
      "closed_at": "2024-08-28 01:06:59+00:00",
      "resolution_days": 54.98056712962963
    },
    {
      "number": 8276,
      "title": "Why is the single input used incorrect, or no output?",
      "state": "closed",
      "comments": 13,
      "body_length": 845,
      "label_names": [
        "bug-unconfirmed",
        "stale",
        "medium severity"
      ],
      "created_at": "2024-07-03 11:22:58+00:00",
      "closed_at": "2024-08-27 01:06:59+00:00",
      "resolution_days": 54.572233796296295
    },
    {
      "number": 8254,
      "title": "Bug: Failed to load quantizied DeepSeek-V2-Lite-Chat model",
      "state": "closed",
      "comments": 14,
      "body_length": 16051,
      "label_names": [
        "bug",
        "high severity"
      ],
      "created_at": "2024-07-02 10:39:45+00:00",
      "closed_at": "2024-07-05 07:05:35+00:00",
      "resolution_days": 2.8512731481481484
    },
    {
      "number": 8253,
      "title": "Using the latest llama-cli will result in an output that keeps blanking",
      "state": "closed",
      "comments": 27,
      "body_length": 557,
      "label_names": [
        "bug-unconfirmed",
        "medium severity"
      ],
      "created_at": "2024-07-02 09:21:50+00:00",
      "closed_at": "2024-07-03 16:23:40+00:00",
      "resolution_days": 1.2929398148148148
    },
    {
      "number": 8240,
      "title": "Investigate gemma 2 generation quality",
      "state": "closed",
      "comments": 90,
      "body_length": 347,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-07-01 16:52:28+00:00",
      "closed_at": "2024-10-16 01:11:07+00:00",
      "resolution_days": 106.34628472222222
    },
    {
      "number": 8211,
      "title": "Bug: ld: symbol(s) not found for architecture arm64 ",
      "state": "closed",
      "comments": 27,
      "body_length": 1997,
      "label_names": [
        "bug",
        "Apple Metal",
        "high severity"
      ],
      "created_at": "2024-06-29 15:26:26+00:00",
      "closed_at": "2024-08-07 16:24:06+00:00",
      "resolution_days": 39.040046296296296
    },
    {
      "number": 8195,
      "title": "Bug: Gemma-2 not supported on b3262",
      "state": "closed",
      "comments": 15,
      "body_length": 4923,
      "label_names": [
        "bug-unconfirmed",
        "medium severity"
      ],
      "created_at": "2024-06-28 15:34:02+00:00",
      "closed_at": "2024-06-28 18:00:39+00:00",
      "resolution_days": 0.10181712962962963
    },
    {
      "number": 8188,
      "title": "Feature Request: Installable package via winget",
      "state": "open",
      "comments": 19,
      "body_length": 1021,
      "label_names": [
        "enhancement",
        "help wanted"
      ],
      "created_at": "2024-06-28 13:27:20+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 8183,
      "title": "Bug: quantized gemma 27b output still wrong after tokenizer fix and soft capping",
      "state": "closed",
      "comments": 14,
      "body_length": 3673,
      "label_names": [
        "bug-unconfirmed",
        "medium severity"
      ],
      "created_at": "2024-06-28 08:57:08+00:00",
      "closed_at": "2024-07-01 17:02:14+00:00",
      "resolution_days": 3.336875
    },
    {
      "number": 8174,
      "title": "Bug: Cannot load DeepSeek-Coder-V2-Instruct",
      "state": "closed",
      "comments": 12,
      "body_length": 9564,
      "label_names": [
        "bug-unconfirmed",
        "critical severity"
      ],
      "created_at": "2024-06-27 16:58:50+00:00",
      "closed_at": "2024-06-27 18:28:43+00:00",
      "resolution_days": 0.06241898148148148
    },
    {
      "number": 8164,
      "title": "Bug: on AMD gpu, it offloads all the work to the CPU unless you specify --n-gpu-layers on the llama-cli command line",
      "state": "closed",
      "comments": 24,
      "body_length": 4071,
      "label_names": [
        "bug-unconfirmed",
        "medium severity"
      ],
      "created_at": "2024-06-27 12:19:01+00:00",
      "closed_at": "2024-06-28 04:22:31+00:00",
      "resolution_days": 0.6690972222222222
    },
    {
      "number": 8096,
      "title": "Bug: Crashes at the end of startup during first prompt processing",
      "state": "closed",
      "comments": 24,
      "body_length": 8629,
      "label_names": [
        "bug-unconfirmed",
        "stale",
        "critical severity"
      ],
      "created_at": "2024-06-24 13:11:16+00:00",
      "closed_at": "2024-08-08 01:06:48+00:00",
      "resolution_days": 44.49689814814815
    },
    {
      "number": 8092,
      "title": "Vulkan backend regression: gibberish output when layers offloaded to GPU",
      "state": "closed",
      "comments": 13,
      "body_length": 813,
      "label_names": [
        "bug-unconfirmed",
        "high severity"
      ],
      "created_at": "2024-06-24 10:54:18+00:00",
      "closed_at": "2024-07-15 07:38:53+00:00",
      "resolution_days": 20.86429398148148
    },
    {
      "number": 8076,
      "title": "Bug: llama-server crashes when started with --embeddings",
      "state": "closed",
      "comments": 13,
      "body_length": 3092,
      "label_names": [
        "bug-unconfirmed",
        "high severity"
      ],
      "created_at": "2024-06-23 11:03:07+00:00",
      "closed_at": "2024-07-12 08:14:13+00:00",
      "resolution_days": 18.882708333333333
    },
    {
      "number": 8025,
      "title": "Bug: Qwen2-72B-Instruct (and finetunes) Q4_K_M, Q5_K_M generates random output with CuBLAS prompt processing ",
      "state": "closed",
      "comments": 28,
      "body_length": 957,
      "label_names": [
        "bug-unconfirmed",
        "stale",
        "high severity"
      ],
      "created_at": "2024-06-20 03:39:38+00:00",
      "closed_at": "2024-09-07 01:07:11+00:00",
      "resolution_days": 78.89413194444444
    },
    {
      "number": 8019,
      "title": "Feature Request: Hardware support check",
      "state": "closed",
      "comments": 12,
      "body_length": 1960,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2024-06-19 21:52:03+00:00",
      "closed_at": "2024-06-19 23:34:00+00:00",
      "resolution_days": 0.07079861111111112
    },
    {
      "number": 8010,
      "title": "server: Bring back multimodal support",
      "state": "closed",
      "comments": 51,
      "body_length": 2259,
      "label_names": [
        "enhancement",
        "llava",
        "server"
      ],
      "created_at": "2024-06-19 12:03:45+00:00",
      "closed_at": "2025-05-09 21:20:01+00:00",
      "resolution_days": 324.3862962962963
    },
    {
      "number": 7995,
      "title": "Feature Request: Support for Meta Chameleon 7B and 34B",
      "state": "closed",
      "comments": 12,
      "body_length": 1525,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-06-18 20:04:05+00:00",
      "closed_at": "2024-09-01 01:07:45+00:00",
      "resolution_days": 74.21087962962963
    },
    {
      "number": 7969,
      "title": "Bug: b3028 breaks mixtral 8x22b",
      "state": "closed",
      "comments": 20,
      "body_length": 757,
      "label_names": [
        "bug-unconfirmed",
        "stale",
        "high severity"
      ],
      "created_at": "2024-06-17 05:04:18+00:00",
      "closed_at": "2024-08-20 01:06:49+00:00",
      "resolution_days": 63.835081018518515
    },
    {
      "number": 7966,
      "title": "Feature Request: Nemotron-4-340B-Instruct Support",
      "state": "closed",
      "comments": 13,
      "body_length": 1164,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-06-17 01:48:57+00:00",
      "closed_at": "2024-09-02 01:07:16+00:00",
      "resolution_days": 76.97105324074074
    },
    {
      "number": 7924,
      "title": "Bug: Error while converting BERT to GGUF: Can not map tensor 'bert.embeddings.LayerNorm.beta'",
      "state": "closed",
      "comments": 14,
      "body_length": 4694,
      "label_names": [
        "bug-unconfirmed",
        "stale",
        "medium severity"
      ],
      "created_at": "2024-06-13 19:36:58+00:00",
      "closed_at": "2024-07-28 01:07:05+00:00",
      "resolution_days": 44.229247685185186
    },
    {
      "number": 7922,
      "title": "converting phi-3-small error.",
      "state": "closed",
      "comments": 13,
      "body_length": 275,
      "label_names": [
        "stale"
      ],
      "created_at": "2024-06-13 17:17:16+00:00",
      "closed_at": "2024-08-14 01:06:57+00:00",
      "resolution_days": 61.32616898148148
    },
    {
      "number": 7885,
      "title": "Bug: CUDA error: out of memory - Phi-3 Mini 128k prompted with 20k+ tokens on 4GB GPU",
      "state": "closed",
      "comments": 28,
      "body_length": 1508,
      "label_names": [
        "bug-unconfirmed",
        "stale",
        "3rd party"
      ],
      "created_at": "2024-06-11 19:50:26+00:00",
      "closed_at": "2024-08-01 01:07:07+00:00",
      "resolution_days": 50.219918981481484
    },
    {
      "number": 7836,
      "title": "Refactor: Formalise Keys.General GGUF KV Store",
      "state": "closed",
      "comments": 12,
      "body_length": 6674,
      "label_names": [],
      "created_at": "2024-06-08 15:13:18+00:00",
      "closed_at": "2024-07-21 07:47:17+00:00",
      "resolution_days": 42.6902662037037
    },
    {
      "number": 7813,
      "title": "Qwen2-57B-A14B-Instruct not supported",
      "state": "closed",
      "comments": 13,
      "body_length": 608,
      "label_names": [
        "bug-unconfirmed",
        "medium severity"
      ],
      "created_at": "2024-06-07 08:47:04+00:00",
      "closed_at": "2024-06-07 10:00:28+00:00",
      "resolution_days": 0.050972222222222224
    },
    {
      "number": 7805,
      "title": "Bug: QWEN2 quantization GGML_ASSERT",
      "state": "closed",
      "comments": 74,
      "body_length": 1697,
      "label_names": [
        "bug-unconfirmed",
        "stale",
        "high severity"
      ],
      "created_at": "2024-06-06 17:32:36+00:00",
      "closed_at": "2024-09-01 01:07:49+00:00",
      "resolution_days": 86.31612268518519
    },
    {
      "number": 7773,
      "title": "ggml : add WebGPU backend",
      "state": "open",
      "comments": 21,
      "body_length": 286,
      "label_names": [
        "help wanted",
        "research \ud83d\udd2c",
        "roadmap"
      ],
      "created_at": "2024-06-05 14:24:37+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 7768,
      "title": "Please compile also clblast version!",
      "state": "closed",
      "comments": 46,
      "body_length": 216,
      "label_names": [
        "stale"
      ],
      "created_at": "2024-06-05 10:43:05+00:00",
      "closed_at": "2024-08-02 01:20:46+00:00",
      "resolution_days": 57.60950231481481
    },
    {
      "number": 7709,
      "title": "Bug: Phi-3 4K output broken after 2000~ tokens (Reproducible)",
      "state": "closed",
      "comments": 13,
      "body_length": 1111,
      "label_names": [
        "bug",
        "model",
        "medium severity"
      ],
      "created_at": "2024-06-03 07:25:37+00:00",
      "closed_at": "2024-12-27 12:33:27+00:00",
      "resolution_days": 207.21377314814814
    },
    {
      "number": 7666,
      "title": "Bug: The output of llama.cpp with Phi-3 contains Non-sense/meaningless words, Does anyone encounter the similar problem?",
      "state": "closed",
      "comments": 16,
      "body_length": 1596,
      "label_names": [
        "bug-unconfirmed",
        "stale",
        "low severity"
      ],
      "created_at": "2024-05-31 09:50:47+00:00",
      "closed_at": "2024-07-19 01:06:55+00:00",
      "resolution_days": 48.63620370370371
    },
    {
      "number": 7661,
      "title": "When using GPU (OpenCL), the reply speed is slower and all replies are incorrect\uff1f\uff1f",
      "state": "closed",
      "comments": 23,
      "body_length": 693,
      "label_names": [
        "bug-unconfirmed",
        "stale",
        "medium severity"
      ],
      "created_at": "2024-05-31 07:45:45+00:00",
      "closed_at": "2024-09-16 01:07:32+00:00",
      "resolution_days": 107.72346064814815
    },
    {
      "number": 7658,
      "title": "Why is convert.py missing?",
      "state": "closed",
      "comments": 15,
      "body_length": 1120,
      "label_names": [
        "documentation",
        "script",
        "python",
        "high severity"
      ],
      "created_at": "2024-05-31 05:46:36+00:00",
      "closed_at": "2024-06-10 19:58:23+00:00",
      "resolution_days": 10.591516203703703
    },
    {
      "number": 7643,
      "title": "Bug: cant finetune",
      "state": "closed",
      "comments": 17,
      "body_length": 8419,
      "label_names": [
        "bug-unconfirmed",
        "stale",
        "critical severity"
      ],
      "created_at": "2024-05-30 14:40:56+00:00",
      "closed_at": "2024-09-02 01:07:20+00:00",
      "resolution_days": 94.435
    },
    {
      "number": 7629,
      "title": "Bug: SPM tokenization breaks in at least one specific case.",
      "state": "closed",
      "comments": 16,
      "body_length": 1536,
      "label_names": [
        "bug-unconfirmed",
        "stale",
        "medium severity"
      ],
      "created_at": "2024-05-29 22:07:01+00:00",
      "closed_at": "2024-08-22 01:07:12+00:00",
      "resolution_days": 84.12512731481482
    },
    {
      "number": 7507,
      "title": "CI Docker Build Issue (intel public key needs updating?)",
      "state": "closed",
      "comments": 16,
      "body_length": 1904,
      "label_names": [
        "help wanted",
        "devops"
      ],
      "created_at": "2024-05-24 02:39:04+00:00",
      "closed_at": "2024-06-03 17:53:32+00:00",
      "resolution_days": 10.635046296296297
    },
    {
      "number": 7444,
      "title": "FR: Phi-3-vision-128k-instruct implementation",
      "state": "closed",
      "comments": 23,
      "body_length": 101,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-05-21 20:05:30+00:00",
      "closed_at": "2024-10-17 01:21:30+00:00",
      "resolution_days": 148.21944444444443
    },
    {
      "number": 7439,
      "title": "Phi 3 medium/small support",
      "state": "closed",
      "comments": 38,
      "body_length": 510,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-05-21 15:47:54+00:00",
      "closed_at": "2024-10-06 01:07:41+00:00",
      "resolution_days": 137.38873842592594
    },
    {
      "number": 7399,
      "title": "speedup ROCm AMD Unified Memory Architecture",
      "state": "closed",
      "comments": 30,
      "body_length": 3551,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-05-19 23:01:17+00:00",
      "closed_at": "2024-07-11 01:06:46+00:00",
      "resolution_days": 52.0871412037037
    },
    {
      "number": 7343,
      "title": "Flash attention implementations do not handle case where value vectors have different dimension from query vectors",
      "state": "closed",
      "comments": 13,
      "body_length": 1809,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-05-17 15:23:12+00:00",
      "closed_at": "2025-03-17 01:07:56+00:00",
      "resolution_days": 303.40606481481484
    },
    {
      "number": 7211,
      "title": "ggml-cuda.cu:1278: to_fp32_cuda != nullptr",
      "state": "closed",
      "comments": 12,
      "body_length": 8074,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2024-05-10 21:52:08+00:00",
      "closed_at": "2024-08-10 01:07:09+00:00",
      "resolution_days": 91.13542824074074
    },
    {
      "number": 7207,
      "title": "convert-hf-to-gguf-update.py breaks",
      "state": "closed",
      "comments": 16,
      "body_length": 567,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2024-05-10 19:30:59+00:00",
      "closed_at": "2024-07-09 01:06:57+00:00",
      "resolution_days": 59.23331018518518
    },
    {
      "number": 7190,
      "title": "Native Intel IPEX-LLM Support",
      "state": "closed",
      "comments": 13,
      "body_length": 3954,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-05-10 02:02:37+00:00",
      "closed_at": "2024-07-09 01:06:58+00:00",
      "resolution_days": 59.961354166666666
    },
    {
      "number": 7156,
      "title": "ggml-cuda.so is 90mb with -arch=all",
      "state": "closed",
      "comments": 20,
      "body_length": 2522,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2024-05-09 03:26:09+00:00",
      "closed_at": "2024-08-31 01:07:10+00:00",
      "resolution_days": 113.9034837962963
    },
    {
      "number": 7130,
      "title": "Embedding fails to run on vulkan backend",
      "state": "closed",
      "comments": 14,
      "body_length": 7533,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2024-05-07 19:24:20+00:00",
      "closed_at": "2024-05-19 15:19:54+00:00",
      "resolution_days": 11.83025462962963
    },
    {
      "number": 7118,
      "title": "llama : add DeepSeek-v2-Chat support",
      "state": "closed",
      "comments": 67,
      "body_length": 98,
      "label_names": [
        "good first issue",
        "model"
      ],
      "created_at": "2024-05-07 06:22:43+00:00",
      "closed_at": "2024-05-28 15:07:06+00:00",
      "resolution_days": 21.364155092592593
    },
    {
      "number": 7116,
      "title": "Add Support for IBM Granite",
      "state": "closed",
      "comments": 39,
      "body_length": 1186,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2024-05-07 05:51:11+00:00",
      "closed_at": "2024-05-28 18:49:50+00:00",
      "resolution_days": 21.540729166666665
    },
    {
      "number": 7111,
      "title": "Making perplexity \"comparable\" between models with different vocab: per-byte perplexity",
      "state": "closed",
      "comments": 30,
      "body_length": 3382,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-05-07 01:10:21+00:00",
      "closed_at": "2024-07-07 01:07:02+00:00",
      "resolution_days": 60.997696759259256
    },
    {
      "number": 7088,
      "title": "The convert-hf-to-gguf-update.py seems doesn't work.",
      "state": "closed",
      "comments": 15,
      "body_length": 625,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2024-05-05 13:42:39+00:00",
      "closed_at": "2024-06-26 02:25:34+00:00",
      "resolution_days": 51.52980324074074
    },
    {
      "number": 7062,
      "title": "Llama3 GGUF conversion with merged LORA Adapter seems to lose training data randomly",
      "state": "closed",
      "comments": 147,
      "body_length": 1436,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2024-05-03 19:48:32+00:00",
      "closed_at": "2024-05-09 12:30:49+00:00",
      "resolution_days": 5.696030092592593
    },
    {
      "number": 7060,
      "title": "llava 1.5 invalid output after first inference (llamacpp server)",
      "state": "closed",
      "comments": 13,
      "body_length": 14156,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2024-05-03 14:47:40+00:00",
      "closed_at": "2024-05-10 06:41:11+00:00",
      "resolution_days": 6.662164351851852
    },
    {
      "number": 7056,
      "title": "Gibberish from longer context",
      "state": "closed",
      "comments": 15,
      "body_length": 1200,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2024-05-03 07:49:21+00:00",
      "closed_at": "2024-05-07 09:27:04+00:00",
      "resolution_days": 4.067858796296297
    },
    {
      "number": 7048,
      "title": "Significantly different results (and WRONG) inference when GPU is enabled.",
      "state": "closed",
      "comments": 40,
      "body_length": 804,
      "label_names": [
        "bug",
        "Nvidia GPU"
      ],
      "created_at": "2024-05-02 18:51:50+00:00",
      "closed_at": "2024-05-17 18:49:39+00:00",
      "resolution_days": 14.998483796296297
    },
    {
      "number": 7042,
      "title": "Llama.cpp not working with intel ARC 770?",
      "state": "closed",
      "comments": 17,
      "body_length": 3313,
      "label_names": [
        "Intel GPU"
      ],
      "created_at": "2024-05-02 12:37:39+00:00",
      "closed_at": "2024-05-08 19:23:42+00:00",
      "resolution_days": 6.281979166666667
    },
    {
      "number": 7024,
      "title": "\"\u00ef\u00b8\u0131\" is causing chktok to mismatch when using chkhsh",
      "state": "closed",
      "comments": 14,
      "body_length": 372,
      "label_names": [
        "need more info",
        "bug-unconfirmed"
      ],
      "created_at": "2024-05-01 14:28:37+00:00",
      "closed_at": "2024-05-02 08:22:12+00:00",
      "resolution_days": 0.7455439814814815
    },
    {
      "number": 7021,
      "title": "Cannot convert llama3 8b model to gguf",
      "state": "closed",
      "comments": 19,
      "body_length": 767,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2024-05-01 10:45:52+00:00",
      "closed_at": "2024-05-01 10:55:27+00:00",
      "resolution_days": 0.006655092592592593
    },
    {
      "number": 6980,
      "title": "Tokenizers questions and ... proposals?",
      "state": "closed",
      "comments": 36,
      "body_length": 1527,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-04-29 13:16:40+00:00",
      "closed_at": "2024-06-23 01:12:25+00:00",
      "resolution_days": 54.49704861111111
    },
    {
      "number": 6955,
      "title": "I finally found prove that server output can be different (and vs groq now) - model name : llama3 8b Instruct",
      "state": "closed",
      "comments": 13,
      "body_length": 4667,
      "label_names": [],
      "created_at": "2024-04-28 01:26:07+00:00",
      "closed_at": "2024-04-29 21:04:43+00:00",
      "resolution_days": 1.8184722222222223
    },
    {
      "number": 6927,
      "title": "Help test CPUSet patch for Windows and Linux",
      "state": "closed",
      "comments": 16,
      "body_length": 1684,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-04-26 13:28:52+00:00",
      "closed_at": "2024-06-13 02:55:45+00:00",
      "resolution_days": 47.56033564814815
    },
    {
      "number": 6914,
      "title": "Something might be wrong with either llama.cpp or the Llama 3 GGUFs",
      "state": "closed",
      "comments": 14,
      "body_length": 1872,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2024-04-25 22:09:50+00:00",
      "closed_at": "2024-05-11 10:51:02+00:00",
      "resolution_days": 15.528611111111111
    },
    {
      "number": 6903,
      "title": "server: phi-3 end token not handled?",
      "state": "closed",
      "comments": 13,
      "body_length": 219,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2024-04-25 10:58:12+00:00",
      "closed_at": "2024-06-25 02:41:36+00:00",
      "resolution_days": 60.65513888888889
    },
    {
      "number": 6898,
      "title": "I use python's llama-cpp package to run the code. There is a cuda environment and the contents of llama.cpp (compiled), but I still cannot use the GPU.",
      "state": "closed",
      "comments": 13,
      "body_length": 640,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2024-04-25 07:12:22+00:00",
      "closed_at": "2024-06-29 01:06:59+00:00",
      "resolution_days": 64.74626157407407
    },
    {
      "number": 6886,
      "title": "Error Building llama.cpp on Intel MacBook Pro with Metal",
      "state": "closed",
      "comments": 12,
      "body_length": 24344,
      "label_names": [
        "stale"
      ],
      "created_at": "2024-04-24 20:58:05+00:00",
      "closed_at": "2024-06-17 01:07:05+00:00",
      "resolution_days": 53.172916666666666
    },
    {
      "number": 6877,
      "title": "Add support to ArcticForCausalLM",
      "state": "closed",
      "comments": 36,
      "body_length": 798,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2024-04-24 14:45:07+00:00",
      "closed_at": "2024-05-24 12:31:15+00:00",
      "resolution_days": 29.907037037037036
    },
    {
      "number": 6849,
      "title": "Support for Phi-3 models",
      "state": "open",
      "comments": 84,
      "body_length": 129,
      "label_names": [
        "good first issue",
        "model"
      ],
      "created_at": "2024-04-23 15:22:53+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 6841,
      "title": "Is it normal that ROCm+HIPBLAS produces different results than on CPU or breaks completely?",
      "state": "closed",
      "comments": 33,
      "body_length": 1421,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2024-04-23 11:12:58+00:00",
      "closed_at": "2024-04-26 16:39:59+00:00",
      "resolution_days": 3.2270949074074076
    },
    {
      "number": 6819,
      "title": "Refactor convert.py and add support for Metas official Llama 3 model",
      "state": "closed",
      "comments": 20,
      "body_length": 3492,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2024-04-22 04:11:07+00:00",
      "closed_at": "2024-05-02 01:45:34+00:00",
      "resolution_days": 9.898923611111112
    },
    {
      "number": 6803,
      "title": "Support for InternVL",
      "state": "closed",
      "comments": 34,
      "body_length": 352,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-04-21 05:58:47+00:00",
      "closed_at": "2024-11-16 01:59:12+00:00",
      "resolution_days": 208.8336226851852
    },
    {
      "number": 6763,
      "title": "New optimization from NVIDIA to use CUDA Graphs in llama.cpp",
      "state": "closed",
      "comments": 12,
      "body_length": 2504,
      "label_names": [
        "stale"
      ],
      "created_at": "2024-04-19 09:01:05+00:00",
      "closed_at": "2024-07-06 01:06:38+00:00",
      "resolution_days": 77.67052083333333
    },
    {
      "number": 6758,
      "title": "ggml : add GPU support for Mamba models",
      "state": "open",
      "comments": 32,
      "body_length": 564,
      "label_names": [
        "enhancement",
        "help wanted",
        "Nvidia GPU",
        "roadmap"
      ],
      "created_at": "2024-04-19 06:47:35+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 6747,
      "title": "llama3 family support",
      "state": "closed",
      "comments": 79,
      "body_length": 178,
      "label_names": [
        "enhancement",
        "model"
      ],
      "created_at": "2024-04-18 16:57:57+00:00",
      "closed_at": "2024-04-21 12:19:26+00:00",
      "resolution_days": 2.806585648148148
    },
    {
      "number": 6743,
      "title": "Llama only uses dedicated memory when both shared and dedicated are available.",
      "state": "closed",
      "comments": 18,
      "body_length": 645,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-04-18 14:04:33+00:00",
      "closed_at": "2024-08-26 01:07:12+00:00",
      "resolution_days": 129.46017361111112
    },
    {
      "number": 6715,
      "title": "Feature request: Graphical GGUF viewer",
      "state": "open",
      "comments": 18,
      "body_length": 1376,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-04-17 04:30:46+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 6690,
      "title": "can llama.cpp/convert.py support tokenizer rather than 'spm', 'bpe', 'hfft'",
      "state": "closed",
      "comments": 13,
      "body_length": 1417,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2024-04-15 16:59:48+00:00",
      "closed_at": "2024-07-13 01:07:00+00:00",
      "resolution_days": 88.33833333333334
    },
    {
      "number": 6685,
      "title": "parallel/server crashes with: ggml.c:16521: i != GGML_HASHTABLE_FULL when defragmentation is enabled",
      "state": "open",
      "comments": 28,
      "body_length": 648,
      "label_names": [
        "bug",
        "server/webui"
      ],
      "created_at": "2024-04-15 11:39:29+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 6681,
      "title": "Has the args \"-- parallel N\" not been developed yet? ./server -m xx -ng 10",
      "state": "closed",
      "comments": 14,
      "body_length": 533,
      "label_names": [
        "server/webui",
        "bug-unconfirmed"
      ],
      "created_at": "2024-04-15 04:23:42+00:00",
      "closed_at": "2024-04-16 02:01:35+00:00",
      "resolution_days": 0.9013078703703704
    },
    {
      "number": 6617,
      "title": "Please upgrade the KV cache size yes using `--ctx-size`",
      "state": "closed",
      "comments": 37,
      "body_length": 939,
      "label_names": [],
      "created_at": "2024-04-11 19:29:35+00:00",
      "closed_at": "2024-04-12 06:07:35+00:00",
      "resolution_days": 0.44305555555555554
    },
    {
      "number": 6580,
      "title": "Request Support for Mistral-8x22B",
      "state": "closed",
      "comments": 16,
      "body_length": 678,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-04-10 05:04:18+00:00",
      "closed_at": "2024-06-07 01:06:57+00:00",
      "resolution_days": 57.83517361111111
    },
    {
      "number": 6571,
      "title": "b2447 (c47cf41) decreased output quality",
      "state": "closed",
      "comments": 17,
      "body_length": 299,
      "label_names": [
        "need more info",
        "bug-unconfirmed"
      ],
      "created_at": "2024-04-09 18:50:49+00:00",
      "closed_at": "2024-05-24 13:29:29+00:00",
      "resolution_days": 44.77685185185185
    },
    {
      "number": 6564,
      "title": "Support for RecurrentGemma (Gemma with Griffin Architecture)",
      "state": "closed",
      "comments": 14,
      "body_length": 1690,
      "label_names": [
        "enhancement",
        "model",
        "stale"
      ],
      "created_at": "2024-04-09 14:11:30+00:00",
      "closed_at": "2024-11-17 01:07:50+00:00",
      "resolution_days": 221.45578703703703
    },
    {
      "number": 6546,
      "title": "kubernetes example",
      "state": "open",
      "comments": 18,
      "body_length": 454,
      "label_names": [
        "enhancement",
        "help wanted",
        "server/webui",
        "kubernetes"
      ],
      "created_at": "2024-04-08 16:31:37+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 6528,
      "title": "Fails to run in SYCL mode",
      "state": "closed",
      "comments": 27,
      "body_length": 10425,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2024-04-07 22:21:24+00:00",
      "closed_at": "2024-07-14 01:07:19+00:00",
      "resolution_days": 97.11521990740741
    },
    {
      "number": 6516,
      "title": "CPU only, Vs GPU/CPU split VS GPU only",
      "state": "closed",
      "comments": 16,
      "body_length": 2287,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2024-04-07 00:01:57+00:00",
      "closed_at": "2024-04-16 05:01:15+00:00",
      "resolution_days": 9.207847222222222
    },
    {
      "number": 6490,
      "title": "wrong number of tensors for AdaptLLM/medicine-chat",
      "state": "closed",
      "comments": 13,
      "body_length": 10054,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2024-04-04 17:03:26+00:00",
      "closed_at": "2024-06-23 01:12:31+00:00",
      "resolution_days": 79.3396412037037
    },
    {
      "number": 6476,
      "title": "Multi GPU `--split-mode row` speed regression ",
      "state": "closed",
      "comments": 12,
      "body_length": 4444,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2024-04-04 09:56:33+00:00",
      "closed_at": "2024-04-10 19:17:22+00:00",
      "resolution_days": 6.389456018518518
    },
    {
      "number": 6444,
      "title": "Support QuaRot quantization scheme",
      "state": "closed",
      "comments": 15,
      "body_length": 1271,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-04-02 22:19:11+00:00",
      "closed_at": "2024-12-14 01:07:47+00:00",
      "resolution_days": 255.11708333333334
    },
    {
      "number": 6442,
      "title": "Mixtral on Windows big performance drop compared to Linux",
      "state": "closed",
      "comments": 16,
      "body_length": 878,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2024-04-02 19:14:50+00:00",
      "closed_at": "2024-05-20 01:09:01+00:00",
      "resolution_days": 47.24596064814815
    },
    {
      "number": 6434,
      "title": "AMD EPYC 9654 is not optimized for max speed",
      "state": "closed",
      "comments": 12,
      "body_length": 361,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2024-04-02 06:28:28+00:00",
      "closed_at": "2024-06-02 01:16:19+00:00",
      "resolution_days": 60.783229166666665
    },
    {
      "number": 6417,
      "title": "Performance decreated between tag b1500 and b2581 on Windows ARM64 PC",
      "state": "closed",
      "comments": 54,
      "body_length": 2028,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-04-01 03:20:36+00:00",
      "closed_at": "2024-07-08 01:06:56+00:00",
      "resolution_days": 97.90717592592593
    },
    {
      "number": 6391,
      "title": "Implement (properly) different chat templates in main.cpp",
      "state": "closed",
      "comments": 36,
      "body_length": 3392,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-03-29 23:07:35+00:00",
      "closed_at": "2024-06-08 01:07:04+00:00",
      "resolution_days": 70.08297453703703
    },
    {
      "number": 6372,
      "title": "Suport for Jamba JambaForCausalLM",
      "state": "closed",
      "comments": 37,
      "body_length": 1423,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2024-03-28 16:39:22+00:00",
      "closed_at": "2025-07-09 18:59:59+00:00",
      "resolution_days": 468.097650462963
    },
    {
      "number": 6360,
      "title": "llama.cpp Python bindings not working for multiple GPUs",
      "state": "closed",
      "comments": 12,
      "body_length": 19360,
      "label_names": [
        "stale"
      ],
      "created_at": "2024-03-28 06:24:28+00:00",
      "closed_at": "2024-05-17 01:06:31+00:00",
      "resolution_days": 49.779201388888886
    },
    {
      "number": 6344,
      "title": "Add support for DBRX models: dbrx-base and dbrx-instruct",
      "state": "closed",
      "comments": 36,
      "body_length": 3961,
      "label_names": [
        "enhancement",
        "model"
      ],
      "created_at": "2024-03-27 12:34:45+00:00",
      "closed_at": "2024-04-13 09:33:53+00:00",
      "resolution_days": 16.87439814814815
    },
    {
      "number": 6294,
      "title": "On Windows (but not on UNIX) redirecting the stdin of main to a pipe or a file results in wrong decoding of non-ASCII characters",
      "state": "closed",
      "comments": 13,
      "body_length": 2089,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-03-25 09:40:26+00:00",
      "closed_at": "2024-09-30 08:23:43+00:00",
      "resolution_days": 188.94672453703703
    },
    {
      "number": 6233,
      "title": "server: bench: continuous performance testing",
      "state": "closed",
      "comments": 19,
      "body_length": 10032,
      "label_names": [
        "enhancement",
        "performance",
        "server/webui",
        "need feedback",
        "stale"
      ],
      "created_at": "2024-03-22 11:36:09+00:00",
      "closed_at": "2024-07-03 01:06:46+00:00",
      "resolution_days": 102.56292824074075
    },
    {
      "number": 6224,
      "title": "Correlation between cpu threads and n-gpu-layers",
      "state": "closed",
      "comments": 12,
      "body_length": 1073,
      "label_names": [
        "performance",
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2024-03-22 07:18:01+00:00",
      "closed_at": "2024-05-07 01:06:31+00:00",
      "resolution_days": 45.74201388888889
    },
    {
      "number": 6148,
      "title": "7B model running too slow on android",
      "state": "closed",
      "comments": 14,
      "body_length": 533,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2024-03-18 22:13:25+00:00",
      "closed_at": "2024-03-23 02:18:31+00:00",
      "resolution_days": 4.170208333333333
    },
    {
      "number": 6120,
      "title": "llama: add Grok support",
      "state": "closed",
      "comments": 21,
      "body_length": 277,
      "label_names": [
        "enhancement",
        "good first issue",
        "model"
      ],
      "created_at": "2024-03-17 20:31:28+00:00",
      "closed_at": "2024-05-08 17:05:36+00:00",
      "resolution_days": 51.85703703703704
    },
    {
      "number": 6031,
      "title": "Segmentation fault during inference on AMD gfx900 with codebooga-34b-v0.1.Q5_K_M.gguf",
      "state": "closed",
      "comments": 15,
      "body_length": 13664,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2024-03-13 01:47:52+00:00",
      "closed_at": "2024-03-14 18:46:32+00:00",
      "resolution_days": 1.7074074074074075
    },
    {
      "number": 6018,
      "title": "GGML_ASSERT: ../llama.cpp/ggml-quants.c:10340: grid_index >= 0",
      "state": "closed",
      "comments": 18,
      "body_length": 852,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2024-03-12 14:38:22+00:00",
      "closed_at": "2024-06-08 01:07:09+00:00",
      "resolution_days": 87.43665509259259
    },
    {
      "number": 6007,
      "title": "Model Request for BAAI/bge-m3 (XLMRoberta-based Multilingual Embedding Model)",
      "state": "closed",
      "comments": 12,
      "body_length": 1042,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-03-12 06:25:08+00:00",
      "closed_at": "2024-07-07 01:07:07+00:00",
      "resolution_days": 116.77915509259259
    },
    {
      "number": 5996,
      "title": "New IQ1_S somehow much worse than previous version",
      "state": "closed",
      "comments": 27,
      "body_length": 1524,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2024-03-11 11:40:53+00:00",
      "closed_at": "2024-05-09 01:06:24+00:00",
      "resolution_days": 58.559386574074075
    },
    {
      "number": 5993,
      "title": "Memory allocation increases until OOM - llama.cpp server",
      "state": "closed",
      "comments": 19,
      "body_length": 12406,
      "label_names": [],
      "created_at": "2024-03-11 09:50:08+00:00",
      "closed_at": "2024-03-15 07:54:56+00:00",
      "resolution_days": 3.92
    },
    {
      "number": 5976,
      "title": "For CUDA versions < 11.7 a target CUDA architecture must be explicitly provided via CUDA_DOCKER_ARCH",
      "state": "closed",
      "comments": 14,
      "body_length": 3450,
      "label_names": [
        "stale"
      ],
      "created_at": "2024-03-10 15:55:26+00:00",
      "closed_at": "2024-06-09 01:07:11+00:00",
      "resolution_days": 90.38315972222222
    },
    {
      "number": 5922,
      "title": "Server: possibility of customizable chat template?",
      "state": "closed",
      "comments": 17,
      "body_length": 2389,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-03-07 13:07:50+00:00",
      "closed_at": "2024-06-26 02:25:49+00:00",
      "resolution_days": 110.5541550925926
    },
    {
      "number": 5877,
      "title": "Support speculative decoding in `server` example",
      "state": "closed",
      "comments": 21,
      "body_length": 1395,
      "label_names": [
        "enhancement",
        "good first issue",
        "server/webui"
      ],
      "created_at": "2024-03-05 02:39:35+00:00",
      "closed_at": "2025-01-12 08:12:38+00:00",
      "resolution_days": 313.2312847222222
    },
    {
      "number": 5873,
      "title": "GPU Memory Leak",
      "state": "closed",
      "comments": 12,
      "body_length": 496,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2024-03-04 20:52:20+00:00",
      "closed_at": "2024-03-14 13:22:52+00:00",
      "resolution_days": 9.687870370370371
    },
    {
      "number": 5856,
      "title": "Regressions on IQ3_XXS over time",
      "state": "open",
      "comments": 16,
      "body_length": 1104,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-03-03 17:11:32+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 5848,
      "title": "Multi GPU with Vulkan out of memory issue.",
      "state": "closed",
      "comments": 20,
      "body_length": 1307,
      "label_names": [
        "bug-unconfirmed",
        "Vulkan",
        "stale"
      ],
      "created_at": "2024-03-03 03:08:43+00:00",
      "closed_at": "2024-05-15 09:34:18+00:00",
      "resolution_days": 73.2677662037037
    },
    {
      "number": 5827,
      "title": "Continuous batching load test stuck",
      "state": "closed",
      "comments": 12,
      "body_length": 1006,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2024-03-02 03:09:35+00:00",
      "closed_at": "2024-03-03 07:48:37+00:00",
      "resolution_days": 1.1937731481481482
    },
    {
      "number": 5763,
      "title": "llama : add T5 (encoder-decoder) support",
      "state": "closed",
      "comments": 52,
      "body_length": 260,
      "label_names": [
        "model"
      ],
      "created_at": "2024-02-28 11:24:59+00:00",
      "closed_at": "2024-07-04 13:46:12+00:00",
      "resolution_days": 127.09806712962963
    },
    {
      "number": 5761,
      "title": "Support BitNet b1.58 ternary models",
      "state": "closed",
      "comments": 90,
      "body_length": 1003,
      "label_names": [
        "enhancement",
        "stale",
        "Tensor Encoding Scheme"
      ],
      "created_at": "2024-02-28 09:41:38+00:00",
      "closed_at": "2024-09-18 01:07:17+00:00",
      "resolution_days": 202.6428125
    },
    {
      "number": 5674,
      "title": "[SYCL] Support newer non linear quantization",
      "state": "closed",
      "comments": 20,
      "body_length": 122,
      "label_names": [
        "help wanted",
        "bug-unconfirmed"
      ],
      "created_at": "2024-02-23 05:40:16+00:00",
      "closed_at": "2024-04-09 05:38:28+00:00",
      "resolution_days": 45.99875
    },
    {
      "number": 5655,
      "title": "Segmentation fault",
      "state": "closed",
      "comments": 14,
      "body_length": 514,
      "label_names": [
        "bug",
        "server/webui"
      ],
      "created_at": "2024-02-22 01:47:05+00:00",
      "closed_at": "2024-02-29 07:53:19+00:00",
      "resolution_days": 7.254328703703703
    },
    {
      "number": 5635,
      "title": "Need support for GemmaForCausalLM",
      "state": "closed",
      "comments": 27,
      "body_length": 1626,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2024-02-21 14:00:33+00:00",
      "closed_at": "2024-02-22 22:09:17+00:00",
      "resolution_days": 1.339398148148148
    },
    {
      "number": 5600,
      "title": "Qwen1.5-1.8B-Chat conversion to gguf causes an error",
      "state": "closed",
      "comments": 12,
      "body_length": 2563,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2024-02-20 00:26:04+00:00",
      "closed_at": "2024-04-19 01:06:46+00:00",
      "resolution_days": 59.02826388888889
    },
    {
      "number": 5545,
      "title": "Llava generates gibberish on the Vulkan backend",
      "state": "closed",
      "comments": 21,
      "body_length": 3586,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2024-02-17 09:38:24+00:00",
      "closed_at": "2024-05-18 17:29:07+00:00",
      "resolution_days": 91.32688657407408
    },
    {
      "number": 5533,
      "title": "Build Error",
      "state": "closed",
      "comments": 19,
      "body_length": 258,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2024-02-16 13:18:42+00:00",
      "closed_at": "2024-02-19 12:07:23+00:00",
      "resolution_days": 2.950474537037037
    },
    {
      "number": 5513,
      "title": "[SYCL] GGML_ASSERT issue when running llama.cpp with SYCL on A770",
      "state": "closed",
      "comments": 34,
      "body_length": 7192,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2024-02-15 17:52:38+00:00",
      "closed_at": "2024-06-17 01:07:18+00:00",
      "resolution_days": 122.30185185185185
    },
    {
      "number": 5469,
      "title": "[SYCL] Segmentation fault after #5411",
      "state": "closed",
      "comments": 30,
      "body_length": 7188,
      "label_names": [
        "bug-unconfirmed",
        "Intel GPU"
      ],
      "created_at": "2024-02-13 02:11:58+00:00",
      "closed_at": "2024-02-21 09:52:08+00:00",
      "resolution_days": 8.319560185185185
    },
    {
      "number": 5467,
      "title": " SYCL backend error PI_ERROR_INVALID_WORK_GROUP_SIZE on iGPU UHD 770",
      "state": "closed",
      "comments": 16,
      "body_length": 620,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2024-02-12 18:33:25+00:00",
      "closed_at": "2024-04-01 19:11:02+00:00",
      "resolution_days": 49.026122685185186
    },
    {
      "number": 5447,
      "title": "Get chat_template from a server endpoint.",
      "state": "closed",
      "comments": 12,
      "body_length": 377,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-02-10 23:27:50+00:00",
      "closed_at": "2024-04-21 01:06:43+00:00",
      "resolution_days": 70.06866898148148
    },
    {
      "number": 5441,
      "title": "Vulkan Device memory allocation failed (ErrorOutOfDeviceMemory )",
      "state": "closed",
      "comments": 30,
      "body_length": 2515,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2024-02-10 17:34:34+00:00",
      "closed_at": "2024-04-10 01:06:17+00:00",
      "resolution_days": 59.31369212962963
    },
    {
      "number": 5410,
      "title": "GPU Performance Data Point via Vulkan ",
      "state": "closed",
      "comments": 15,
      "body_length": 295,
      "label_names": [
        "enhancement",
        "Vulkan",
        "stale"
      ],
      "created_at": "2024-02-08 10:58:07+00:00",
      "closed_at": "2024-04-02 01:06:54+00:00",
      "resolution_days": 53.58943287037037
    },
    {
      "number": 5390,
      "title": "[Request/Enhancment]1-bit quants",
      "state": "closed",
      "comments": 13,
      "body_length": 1083,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2024-02-07 13:21:01+00:00",
      "closed_at": "2024-04-02 01:06:58+00:00",
      "resolution_days": 54.49024305555555
    },
    {
      "number": 5356,
      "title": "Vulkan generated targets and shader organization",
      "state": "closed",
      "comments": 16,
      "body_length": 1060,
      "label_names": [
        "enhancement",
        "Vulkan",
        "stale"
      ],
      "created_at": "2024-02-06 02:30:51+00:00",
      "closed_at": "2024-07-30 08:22:32+00:00",
      "resolution_days": 175.24422453703704
    },
    {
      "number": 5319,
      "title": "Fails with  ggml_vulkan: No suitable memory type found: ErrorOutOfDeviceMemory",
      "state": "closed",
      "comments": 21,
      "body_length": 768,
      "label_names": [
        "bug-unconfirmed",
        "Vulkan"
      ],
      "created_at": "2024-02-04 08:37:09+00:00",
      "closed_at": "2024-02-15 06:11:16+00:00",
      "resolution_days": 10.89869212962963
    },
    {
      "number": 5294,
      "title": "nvcc fatal : Value 'native' is not defined for option 'gpu-architecture' when compiling cuBLAS",
      "state": "closed",
      "comments": 17,
      "body_length": 2925,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2024-02-02 22:54:16+00:00",
      "closed_at": "2024-02-13 11:38:38+00:00",
      "resolution_days": 10.530810185185185
    },
    {
      "number": 5280,
      "title": "AMD ROCm problem: GPU is constantly running at 100%",
      "state": "closed",
      "comments": 21,
      "body_length": 9813,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2024-02-02 09:31:30+00:00",
      "closed_at": "2024-07-02 01:06:49+00:00",
      "resolution_days": 150.64952546296297
    },
    {
      "number": 5276,
      "title": "MiniCPM 2b model support?",
      "state": "open",
      "comments": 26,
      "body_length": 319,
      "label_names": [
        "enhancement",
        "good first issue"
      ],
      "created_at": "2024-02-02 08:06:39+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 5247,
      "title": "Can't get server to run with more than 6 slots",
      "state": "closed",
      "comments": 24,
      "body_length": 598,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-01-31 22:10:12+00:00",
      "closed_at": "2024-04-20 01:07:11+00:00",
      "resolution_days": 79.12290509259259
    },
    {
      "number": 5237,
      "title": "Issues with running Llama.cpp on Raspberry Pi 5 with Vulkan.",
      "state": "closed",
      "comments": 16,
      "body_length": 622,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2024-01-31 11:04:06+00:00",
      "closed_at": "2024-04-02 01:07:44+00:00",
      "resolution_days": 61.58585648148148
    },
    {
      "number": 5234,
      "title": "Custom fine-tuned DeepSeek coder model unable to be quantized to Fp16 ",
      "state": "closed",
      "comments": 12,
      "body_length": 2733,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2024-01-31 07:54:11+00:00",
      "closed_at": "2024-04-30 01:15:56+00:00",
      "resolution_days": 89.7234375
    },
    {
      "number": 5215,
      "title": "llama : create llamax library",
      "state": "open",
      "comments": 18,
      "body_length": 808,
      "label_names": [
        "refactoring",
        "roadmap"
      ],
      "created_at": "2024-01-30 13:01:06+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 5186,
      "title": "Subtle Vulkan shader compilation bug when running on Adreno GPUs (Samsung Galaxy S23 Ultra)",
      "state": "closed",
      "comments": 33,
      "body_length": 7241,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2024-01-29 07:08:45+00:00",
      "closed_at": "2024-09-01 01:07:51+00:00",
      "resolution_days": 215.749375
    },
    {
      "number": 5156,
      "title": "convert-hf-to-gguf.py Qwen-72B-Chat model get Killed result",
      "state": "closed",
      "comments": 12,
      "body_length": 1186,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-01-27 03:02:05+00:00",
      "closed_at": "2024-04-02 01:08:10+00:00",
      "resolution_days": 65.9208912037037
    },
    {
      "number": 5142,
      "title": "Architecture \"LlamaForCausalLM\" not supported",
      "state": "closed",
      "comments": 18,
      "body_length": 779,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2024-01-26 12:45:54+00:00",
      "closed_at": "2024-04-02 01:08:16+00:00",
      "resolution_days": 66.5155324074074
    },
    {
      "number": 5140,
      "title": "CUDA: assert when using batch size less than 129",
      "state": "closed",
      "comments": 13,
      "body_length": 6789,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-01-26 11:24:14+00:00",
      "closed_at": "2024-03-20 10:32:17+00:00",
      "resolution_days": 53.96392361111111
    },
    {
      "number": 5130,
      "title": "Add `--grammar-file` argument to `server` (similar to how `main` does it)",
      "state": "closed",
      "comments": 17,
      "body_length": 600,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2024-01-26 01:00:52+00:00",
      "closed_at": "2024-01-27 22:34:11+00:00",
      "resolution_days": 1.898136574074074
    },
    {
      "number": 5079,
      "title": " Intel\u00ae Core\u2122 Ultra processors NPU  Support ",
      "state": "open",
      "comments": 15,
      "body_length": 1152,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2024-01-22 14:15:28+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 5040,
      "title": "Include `eos_token` and `bos_token` from `tokenizer_config.json` for chat templating",
      "state": "open",
      "comments": 12,
      "body_length": 1224,
      "label_names": [
        "enhancement",
        "help wanted"
      ],
      "created_at": "2024-01-19 20:31:05+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 5031,
      "title": "convert.py couldn't convert internlm2",
      "state": "closed",
      "comments": 14,
      "body_length": 579,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2024-01-19 00:52:07+00:00",
      "closed_at": "2024-05-18 01:58:30+00:00",
      "resolution_days": 120.04609953703704
    },
    {
      "number": 4998,
      "title": "unsupported op 'MUL_MAT'",
      "state": "closed",
      "comments": 12,
      "body_length": 193,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2024-01-17 12:12:18+00:00",
      "closed_at": "2024-04-03 01:13:43+00:00",
      "resolution_days": 76.54265046296297
    },
    {
      "number": 4922,
      "title": "aarch64 CUDA build: ggml.h(309): error: identifier \"half\" is undefined",
      "state": "closed",
      "comments": 17,
      "body_length": 1956,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2024-01-13 22:57:25+00:00",
      "closed_at": "2024-01-22 16:32:04+00:00",
      "resolution_days": 8.732395833333333
    },
    {
      "number": 4886,
      "title": "Support Self extend for server (main is already supported)",
      "state": "closed",
      "comments": 22,
      "body_length": 240,
      "label_names": [],
      "created_at": "2024-01-12 04:09:43+00:00",
      "closed_at": "2024-01-29 12:29:07+00:00",
      "resolution_days": 17.346805555555555
    },
    {
      "number": 4771,
      "title": "OpenAI incompatible image handling in server multimodal",
      "state": "closed",
      "comments": 14,
      "body_length": 1448,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2024-01-04 11:24:44+00:00",
      "closed_at": "2024-04-02 01:08:54+00:00",
      "resolution_days": 88.57233796296296
    },
    {
      "number": 4755,
      "title": "Quantization of transformer state for matrix-vector products potentially causes numerical accuracy issues",
      "state": "closed",
      "comments": 17,
      "body_length": 1995,
      "label_names": [
        "stale"
      ],
      "created_at": "2024-01-03 12:30:13+00:00",
      "closed_at": "2024-04-02 01:09:01+00:00",
      "resolution_days": 89.52694444444444
    },
    {
      "number": 4747,
      "title": "The inference performance of 8xH100+nvlink is worse than that of 4xA100 pcie",
      "state": "closed",
      "comments": 18,
      "body_length": 586,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2024-01-03 03:52:08+00:00",
      "closed_at": "2024-01-03 21:51:07+00:00",
      "resolution_days": 0.7492939814814815
    },
    {
      "number": 4672,
      "title": "Unite test \"test-backend-ops\" crashed on MacOS",
      "state": "closed",
      "comments": 13,
      "body_length": 191,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2023-12-28 17:16:42+00:00",
      "closed_at": "2024-01-13 16:03:46+00:00",
      "resolution_days": 15.949351851851851
    },
    {
      "number": 4667,
      "title": "[FEATURE REQUEST] - Make MoE offloading offload real layers rather than groups of layers",
      "state": "closed",
      "comments": 12,
      "body_length": 2518,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2023-12-28 15:02:16+00:00",
      "closed_at": "2024-04-02 01:09:41+00:00",
      "resolution_days": 95.42181712962963
    },
    {
      "number": 4634,
      "title": "Model does not appear to have a file named config.json",
      "state": "closed",
      "comments": 16,
      "body_length": 524,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2023-12-25 22:45:36+00:00",
      "closed_at": "2024-04-02 01:09:55+00:00",
      "resolution_days": 98.1002199074074
    },
    {
      "number": 4542,
      "title": "PowerInfer faster Inference than llama cpp.",
      "state": "closed",
      "comments": 15,
      "body_length": 1047,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2023-12-20 03:54:15+00:00",
      "closed_at": "2024-04-02 01:10:29+00:00",
      "resolution_days": 103.88627314814815
    },
    {
      "number": 4530,
      "title": "KeyError: 'transformer.wte.weight'",
      "state": "closed",
      "comments": 15,
      "body_length": 1630,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-12-19 08:07:12+00:00",
      "closed_at": "2024-04-06 01:06:20+00:00",
      "resolution_days": 108.70773148148149
    },
    {
      "number": 4502,
      "title": "issue running mixtrall",
      "state": "closed",
      "comments": 18,
      "body_length": 5132,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2023-12-16 18:57:37+00:00",
      "closed_at": "2024-04-02 01:10:48+00:00",
      "resolution_days": 107.2591550925926
    },
    {
      "number": 4493,
      "title": "Failed to convert Llama-v2 models ",
      "state": "closed",
      "comments": 28,
      "body_length": 3471,
      "label_names": [
        "bug",
        "high priority",
        "stale"
      ],
      "created_at": "2023-12-16 08:47:53+00:00",
      "closed_at": "2024-04-02 01:10:54+00:00",
      "resolution_days": 107.68265046296297
    },
    {
      "number": 4463,
      "title": "Adding MistralForCausalLM architecture to convert-hf-to-gguf.py",
      "state": "closed",
      "comments": 12,
      "body_length": 549,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2023-12-14 10:00:26+00:00",
      "closed_at": "2024-05-10 01:28:38+00:00",
      "resolution_days": 147.64458333333334
    },
    {
      "number": 4459,
      "title": "Running Mixtral-8x7B-Instruct-v0.1 gets aborted or my Mac freezes and need a force restart",
      "state": "closed",
      "comments": 13,
      "body_length": 3112,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2023-12-14 05:53:43+00:00",
      "closed_at": "2023-12-14 21:12:18+00:00",
      "resolution_days": 0.6379050925925925
    },
    {
      "number": 4451,
      "title": "CLBlast support for Mixtral",
      "state": "closed",
      "comments": 13,
      "body_length": 789,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2023-12-13 20:45:30+00:00",
      "closed_at": "2024-04-04 01:07:15+00:00",
      "resolution_days": 112.18177083333333
    },
    {
      "number": 4445,
      "title": "QMoE support for mixtral",
      "state": "closed",
      "comments": 18,
      "body_length": 936,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2023-12-13 16:22:10+00:00",
      "closed_at": "2023-12-29 17:52:13+00:00",
      "resolution_days": 16.06253472222222
    },
    {
      "number": 4439,
      "title": "Run Mixtral-8x7b-instruct with Llama.cpp: Could not load Llama model from path",
      "state": "closed",
      "comments": 13,
      "body_length": 2386,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-12-13 13:26:16+00:00",
      "closed_at": "2024-04-03 01:14:14+00:00",
      "resolution_days": 111.49164351851852
    },
    {
      "number": 4437,
      "title": "Will llama.cpp be able to use Phi-2 ?",
      "state": "closed",
      "comments": 27,
      "body_length": 94,
      "label_names": [
        "enhancement",
        "good first issue",
        "model"
      ],
      "created_at": "2023-12-13 12:02:56+00:00",
      "closed_at": "2023-12-18 17:27:49+00:00",
      "resolution_days": 5.225613425925926
    },
    {
      "number": 4425,
      "title": "Quantizing V cache not working yet",
      "state": "closed",
      "comments": 14,
      "body_length": 808,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2023-12-12 09:40:08+00:00",
      "closed_at": "2024-05-18 01:58:31+00:00",
      "resolution_days": 157.67943287037036
    },
    {
      "number": 4409,
      "title": "Cannot compile with openblas on Windows",
      "state": "closed",
      "comments": 15,
      "body_length": 328,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-12-11 14:53:22+00:00",
      "closed_at": "2024-04-03 01:14:23+00:00",
      "resolution_days": 113.43126157407407
    },
    {
      "number": 4395,
      "title": "cuBLAS error 15 at ggml-cuda.cu:7956: the requested functionality is not supported",
      "state": "closed",
      "comments": 20,
      "body_length": 48134,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2023-12-09 18:26:37+00:00",
      "closed_at": "2024-04-03 01:14:25+00:00",
      "resolution_days": 115.28319444444445
    },
    {
      "number": 4389,
      "title": "finetune: terminate called after throwing an instance of 'std::bad_alloc'",
      "state": "closed",
      "comments": 14,
      "body_length": 45510,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2023-12-09 05:54:01+00:00",
      "closed_at": "2023-12-17 15:05:57+00:00",
      "resolution_days": 8.383287037037038
    },
    {
      "number": 4387,
      "title": "Enhancement: We need CogVLM support - extremely good image and text analysis, feels like a multi generational step forward.",
      "state": "closed",
      "comments": 50,
      "body_length": 2924,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-12-09 01:16:22+00:00",
      "closed_at": "2024-07-19 01:07:03+00:00",
      "resolution_days": 222.99353009259258
    },
    {
      "number": 4386,
      "title": "Add QuIP Sharp Support - New Quantization method with amazing 2-bit performance ( close to 16 bit )",
      "state": "closed",
      "comments": 12,
      "body_length": 1451,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2023-12-09 01:01:51+00:00",
      "closed_at": "2024-04-03 01:14:27+00:00",
      "resolution_days": 116.00875
    },
    {
      "number": 4381,
      "title": "llama : add Mixtral support",
      "state": "closed",
      "comments": 62,
      "body_length": 116,
      "label_names": [
        "enhancement",
        "high priority",
        "model"
      ],
      "created_at": "2023-12-08 18:20:09+00:00",
      "closed_at": "2023-12-13 12:04:31+00:00",
      "resolution_days": 4.739143518518518
    },
    {
      "number": 4365,
      "title": "Running convert fails with BadZipFile (Bad CRC-32)",
      "state": "closed",
      "comments": 14,
      "body_length": 1882,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2023-12-07 16:55:30+00:00",
      "closed_at": "2024-07-06 01:06:42+00:00",
      "resolution_days": 211.3411111111111
    },
    {
      "number": 4353,
      "title": "Possible of implementing mamba ssm",
      "state": "closed",
      "comments": 17,
      "body_length": 954,
      "label_names": [
        "enhancement",
        "help wanted"
      ],
      "created_at": "2023-12-07 04:36:19+00:00",
      "closed_at": "2024-03-08 22:31:01+00:00",
      "resolution_days": 92.74631944444444
    },
    {
      "number": 4331,
      "title": "Qwen-72B-Chat conversion script does not treat <|im_start|> and <|im_end|> correctly.",
      "state": "closed",
      "comments": 13,
      "body_length": 2524,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2023-12-04 23:16:13+00:00",
      "closed_at": "2024-04-08 01:06:38+00:00",
      "resolution_days": 125.07667824074075
    },
    {
      "number": 4326,
      "title": "\"main : failed to eval\" when LLM produces a long output",
      "state": "closed",
      "comments": 13,
      "body_length": 44802,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2023-12-04 16:01:57+00:00",
      "closed_at": "2024-04-03 01:14:42+00:00",
      "resolution_days": 120.38385416666667
    },
    {
      "number": 4296,
      "title": "CLBlast fails on context lengths above 2048 after merging #4256",
      "state": "closed",
      "comments": 15,
      "body_length": 812,
      "label_names": [
        "bug"
      ],
      "created_at": "2023-12-02 16:40:30+00:00",
      "closed_at": "2023-12-03 13:56:23+00:00",
      "resolution_days": 0.8860300925925926
    },
    {
      "number": 4218,
      "title": "llama : speed-up grammar sampling",
      "state": "open",
      "comments": 40,
      "body_length": 324,
      "label_names": [
        "performance",
        "refactoring",
        "roadmap"
      ],
      "created_at": "2023-11-25 17:04:06+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 4216,
      "title": "server : improvements and maintenance",
      "state": "open",
      "comments": 120,
      "body_length": 3926,
      "label_names": [
        "help wanted",
        "refactoring",
        "server/webui",
        "roadmap"
      ],
      "created_at": "2023-11-25 09:57:53+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 4196,
      "title": "ShareGPT4V-7B-- a multimodel model that surpasses Llava",
      "state": "closed",
      "comments": 19,
      "body_length": 202,
      "label_names": [
        "enhancement",
        "model"
      ],
      "created_at": "2023-11-24 07:54:38+00:00",
      "closed_at": "2023-12-08 12:17:44+00:00",
      "resolution_days": 14.182708333333334
    },
    {
      "number": 4177,
      "title": "Can't run 3B models",
      "state": "closed",
      "comments": 12,
      "body_length": 380,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2023-11-22 22:24:10+00:00",
      "closed_at": "2023-11-24 17:04:33+00:00",
      "resolution_days": 1.7780439814814815
    },
    {
      "number": 4123,
      "title": "does not compile on CUDA 10 anymore",
      "state": "closed",
      "comments": 26,
      "body_length": 86,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-11-18 10:38:49+00:00",
      "closed_at": "2024-05-07 01:06:47+00:00",
      "resolution_days": 170.60275462962963
    },
    {
      "number": 4113,
      "title": "KV cache cell index changes predicted completion",
      "state": "closed",
      "comments": 14,
      "body_length": 49485,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2023-11-17 13:25:02+00:00",
      "closed_at": "2023-11-23 19:50:49+00:00",
      "resolution_days": 6.267905092592593
    },
    {
      "number": 4099,
      "title": "Compilation error on Nvidia Jetson Nano",
      "state": "closed",
      "comments": 23,
      "body_length": 49010,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2023-11-16 11:56:41+00:00",
      "closed_at": "2024-03-24 11:35:49+00:00",
      "resolution_days": 128.98550925925926
    },
    {
      "number": 4086,
      "title": "parallel.cpp exits when encountering a long prompt.",
      "state": "closed",
      "comments": 24,
      "body_length": 43178,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2023-11-15 11:42:43+00:00",
      "closed_at": "2023-11-24 11:34:32+00:00",
      "resolution_days": 8.99431712962963
    },
    {
      "number": 4077,
      "title": "Convert consolidated pth files to gguf / missing script?",
      "state": "closed",
      "comments": 15,
      "body_length": 1608,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2023-11-14 15:53:37+00:00",
      "closed_at": "2023-11-14 21:40:07+00:00",
      "resolution_days": 0.240625
    },
    {
      "number": 4075,
      "title": "CUDA error 716 at ggml-cuda.cu:7546: misaligned address",
      "state": "closed",
      "comments": 14,
      "body_length": 90799,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2023-11-14 15:10:05+00:00",
      "closed_at": "2023-11-15 12:58:14+00:00",
      "resolution_days": 0.9084375
    },
    {
      "number": 4061,
      "title": "clip : offload to GPU",
      "state": "closed",
      "comments": 12,
      "body_length": 474,
      "label_names": [
        "good first issue",
        "performance"
      ],
      "created_at": "2023-11-13 15:02:49+00:00",
      "closed_at": "2023-12-30 21:25:46+00:00",
      "resolution_days": 47.2659375
    },
    {
      "number": 4055,
      "title": "Multi GPU CUDA - 8x performance degradation when splitting tensors -> let's split by layer as an option",
      "state": "closed",
      "comments": 17,
      "body_length": 1314,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2023-11-13 04:57:22+00:00",
      "closed_at": "2024-01-12 19:07:39+00:00",
      "resolution_days": 60.59047453703704
    },
    {
      "number": 4038,
      "title": "Adept Persimmon Models not working with CUDA Acceleration",
      "state": "closed",
      "comments": 16,
      "body_length": 5143,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2023-11-11 14:46:23+00:00",
      "closed_at": "2024-04-02 01:11:35+00:00",
      "resolution_days": 142.43416666666667
    },
    {
      "number": 4030,
      "title": "Segmentation fault after model load on ROCm multi-gpu, multi-gfx",
      "state": "closed",
      "comments": 23,
      "body_length": 58740,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2023-11-11 01:01:05+00:00",
      "closed_at": "2024-04-02 01:11:41+00:00",
      "resolution_days": 143.00736111111112
    },
    {
      "number": 3991,
      "title": "Stuck loading VRAM ROCm multi gpu",
      "state": "closed",
      "comments": 29,
      "body_length": 6493,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2023-11-08 10:22:50+00:00",
      "closed_at": "2024-04-02 01:11:58+00:00",
      "resolution_days": 145.61745370370372
    },
    {
      "number": 3969,
      "title": "Infinite loop of \"context shift\"",
      "state": "closed",
      "comments": 23,
      "body_length": 15392,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2023-11-06 16:02:22+00:00",
      "closed_at": "2024-02-23 18:41:04+00:00",
      "resolution_days": 109.11020833333333
    },
    {
      "number": 3960,
      "title": "ggml : become thread-safe",
      "state": "closed",
      "comments": 14,
      "body_length": 288,
      "label_names": [
        "refactoring"
      ],
      "created_at": "2023-11-05 15:56:19+00:00",
      "closed_at": "2024-05-05 01:06:52+00:00",
      "resolution_days": 181.38232638888888
    },
    {
      "number": 3957,
      "title": "GGUF endianness cannot be determined from GGUF itself",
      "state": "open",
      "comments": 17,
      "body_length": 997,
      "label_names": [
        "enhancement",
        "good first issue",
        "breaking change"
      ],
      "created_at": "2023-11-05 14:00:47+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 3930,
      "title": "Multi-GPU has been broken for me recently. ggml-cuda.cu:7068: invalid argument",
      "state": "closed",
      "comments": 18,
      "body_length": 3824,
      "label_names": [
        "bug-unconfirmed"
      ],
      "created_at": "2023-11-03 11:40:43+00:00",
      "closed_at": "2023-11-05 12:24:10+00:00",
      "resolution_days": 2.030173611111111
    },
    {
      "number": 3880,
      "title": "Can't compile \"llama.cpp/ggml-quants.c\"",
      "state": "closed",
      "comments": 20,
      "body_length": 12762,
      "label_names": [
        "bug-unconfirmed",
        "stale"
      ],
      "created_at": "2023-11-01 03:34:20+00:00",
      "closed_at": "2024-04-02 01:12:34+00:00",
      "resolution_days": 152.90155092592593
    },
    {
      "number": 3869,
      "title": "CTX Processing regression for Pascal - Commit 2b4ea35",
      "state": "closed",
      "comments": 18,
      "body_length": 3582,
      "label_names": [
        "performance"
      ],
      "created_at": "2023-10-31 11:01:49+00:00",
      "closed_at": "2023-11-02 06:35:12+00:00",
      "resolution_days": 1.814849537037037
    },
    {
      "number": 3867,
      "title": "Mistral & Sliding Window Attention - GGUF ROPE accomodation.",
      "state": "closed",
      "comments": 15,
      "body_length": 416,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2023-10-31 08:25:25+00:00",
      "closed_at": "2024-04-02 01:12:36+00:00",
      "resolution_days": 153.69943287037037
    },
    {
      "number": 3815,
      "title": "Server allow /completion and /embedding",
      "state": "closed",
      "comments": 13,
      "body_length": 1779,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2023-10-27 14:28:21+00:00",
      "closed_at": "2023-11-01 09:28:29+00:00",
      "resolution_days": 4.791759259259259
    },
    {
      "number": 3796,
      "title": "How to stock prompt's result into txt file ?",
      "state": "closed",
      "comments": 14,
      "body_length": 1850,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2023-10-26 13:30:50+00:00",
      "closed_at": "2024-04-02 01:13:00+00:00",
      "resolution_days": 158.48761574074075
    },
    {
      "number": 3772,
      "title": "multi-gpu inference produces broken output",
      "state": "closed",
      "comments": 21,
      "body_length": 9561,
      "label_names": [
        "bug"
      ],
      "created_at": "2023-10-25 06:35:00+00:00",
      "closed_at": "2025-03-11 19:25:38+00:00",
      "resolution_days": 503.535162037037
    },
    {
      "number": 3752,
      "title": "Trouble running llama.cpp compiled for OpenMPI",
      "state": "closed",
      "comments": 16,
      "body_length": 11096,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2023-10-23 22:14:38+00:00",
      "closed_at": "2024-04-02 01:13:19+00:00",
      "resolution_days": 161.12408564814814
    },
    {
      "number": 3732,
      "title": "CausalLM: Llama + vocab.json BPE tokenizer = `error loading model: cannot find tokenizer merges in model file`",
      "state": "closed",
      "comments": 16,
      "body_length": 35766,
      "label_names": [
        "model",
        "stale"
      ],
      "created_at": "2023-10-22 21:30:13+00:00",
      "closed_at": "2024-04-04 01:07:28+00:00",
      "resolution_days": 164.15086805555555
    },
    {
      "number": 3723,
      "title": "server: Default web UI erroneously inteprets markdown special characters inside code blocks",
      "state": "closed",
      "comments": 13,
      "body_length": 47890,
      "label_names": [
        "bug",
        "server/webui"
      ],
      "created_at": "2023-10-22 06:40:53+00:00",
      "closed_at": "2024-12-31 10:02:52+00:00",
      "resolution_days": 436.14026620370373
    },
    {
      "number": 3708,
      "title": "Sorry, your GGJTv1 file of type MOSTLY_Q4_1_SOME_F16 is not eligible for conversion",
      "state": "closed",
      "comments": 22,
      "body_length": 1361,
      "label_names": [],
      "created_at": "2023-10-21 05:46:24+00:00",
      "closed_at": "2023-10-24 17:54:07+00:00",
      "resolution_days": 3.505358796296296
    },
    {
      "number": 3664,
      "title": "[feature] Make space insertion optional in SentencePiece tokenizer",
      "state": "closed",
      "comments": 29,
      "body_length": 2310,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2023-10-18 07:40:44+00:00",
      "closed_at": "2024-09-02 01:07:23+00:00",
      "resolution_days": 319.7268402777778
    },
    {
      "number": 3649,
      "title": "Speculative Decoding is slower than expected on A100",
      "state": "closed",
      "comments": 17,
      "body_length": 3571,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-10-17 04:22:20+00:00",
      "closed_at": "2024-04-04 01:08:06+00:00",
      "resolution_days": 169.86511574074075
    },
    {
      "number": 3644,
      "title": "[User] Unable to Finetune Llama 2 70B",
      "state": "closed",
      "comments": 14,
      "body_length": 7489,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-10-16 14:29:16+00:00",
      "closed_at": "2024-04-04 01:08:09+00:00",
      "resolution_days": 170.4436689814815
    },
    {
      "number": 3637,
      "title": "Segmentation fault when running llava",
      "state": "closed",
      "comments": 20,
      "body_length": 7261,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-10-15 15:04:24+00:00",
      "closed_at": "2024-04-04 01:08:16+00:00",
      "resolution_days": 171.41935185185184
    },
    {
      "number": 3628,
      "title": "ci : fix Docker workflow",
      "state": "open",
      "comments": 16,
      "body_length": 176,
      "label_names": [
        "help wanted",
        "build"
      ],
      "created_at": "2023-10-15 06:37:47+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 3625,
      "title": "Bug: Invalid Embeddings if GPU offloaded (CUDA)",
      "state": "closed",
      "comments": 13,
      "body_length": 2030,
      "label_names": [],
      "created_at": "2023-10-14 16:07:14+00:00",
      "closed_at": "2023-10-17 20:24:52+00:00",
      "resolution_days": 3.178912037037037
    },
    {
      "number": 3622,
      "title": "Illegal instruction on Android (Honor Magic 5)",
      "state": "closed",
      "comments": 18,
      "body_length": 50796,
      "label_names": [
        "build",
        "android"
      ],
      "created_at": "2023-10-14 01:54:42+00:00",
      "closed_at": "2023-10-19 22:06:01+00:00",
      "resolution_days": 5.841192129629629
    },
    {
      "number": 3604,
      "title": "MPT: tokenization crashes",
      "state": "closed",
      "comments": 31,
      "body_length": 3855,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-10-12 21:07:35+00:00",
      "closed_at": "2024-04-04 01:08:25+00:00",
      "resolution_days": 174.16724537037038
    },
    {
      "number": 3602,
      "title": "multimodal - Improve LLaVA model accuracy and performance",
      "state": "closed",
      "comments": 24,
      "body_length": 351,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-10-12 19:34:59+00:00",
      "closed_at": "2024-04-04 01:08:27+00:00",
      "resolution_days": 174.23157407407408
    },
    {
      "number": 3593,
      "title": "Running Lllava in interactive mode just Quits after generating response without waiting for next prompt.",
      "state": "closed",
      "comments": 15,
      "body_length": 424,
      "label_names": [
        "stale",
        "llava"
      ],
      "created_at": "2023-10-12 04:23:58+00:00",
      "closed_at": "2024-10-02 01:11:36+00:00",
      "resolution_days": 355.86641203703704
    },
    {
      "number": 3525,
      "title": "[User] Android build fails with \"ld.lld: error: undefined symbol: clGetPlatformIDs\"",
      "state": "closed",
      "comments": 17,
      "body_length": 16883,
      "label_names": [
        "build",
        "android"
      ],
      "created_at": "2023-10-07 12:03:41+00:00",
      "closed_at": "2023-10-07 15:12:36+00:00",
      "resolution_days": 0.13119212962962962
    },
    {
      "number": 3503,
      "title": "Infill Incorrect Tokenization",
      "state": "closed",
      "comments": 15,
      "body_length": 1252,
      "label_names": [
        "bug",
        "high priority"
      ],
      "created_at": "2023-10-06 13:45:19+00:00",
      "closed_at": "2023-10-10 07:31:22+00:00",
      "resolution_days": 3.7403125
    },
    {
      "number": 3492,
      "title": "unable to load model",
      "state": "closed",
      "comments": 12,
      "body_length": 725,
      "label_names": [],
      "created_at": "2023-10-05 21:49:03+00:00",
      "closed_at": "2023-10-06 19:28:24+00:00",
      "resolution_days": 0.9023263888888889
    },
    {
      "number": 3484,
      "title": "[Falcon] Attempting to run Falcon-180B Q5/6 give \"illegal character\"",
      "state": "closed",
      "comments": 15,
      "body_length": 5153,
      "label_names": [],
      "created_at": "2023-10-05 03:21:50+00:00",
      "closed_at": "2023-10-18 07:32:24+00:00",
      "resolution_days": 13.17400462962963
    },
    {
      "number": 3483,
      "title": "[Feature Request] Dynamic temperature sampling for better coherence / creativity",
      "state": "closed",
      "comments": 47,
      "body_length": 3913,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-10-05 02:23:01+00:00",
      "closed_at": "2024-06-12 01:06:49+00:00",
      "resolution_days": 250.94708333333332
    },
    {
      "number": 3479,
      "title": "llama : improve batched decoding performance",
      "state": "closed",
      "comments": 12,
      "body_length": 17950,
      "label_names": [
        "performance",
        "Nvidia GPU"
      ],
      "created_at": "2023-10-04 20:20:55+00:00",
      "closed_at": "2023-10-24 13:48:38+00:00",
      "resolution_days": 19.727581018518517
    },
    {
      "number": 3478,
      "title": "llama : add batched inference endpoint to server",
      "state": "closed",
      "comments": 15,
      "body_length": 547,
      "label_names": [
        "enhancement",
        "help wanted",
        "server/webui"
      ],
      "created_at": "2023-10-04 19:10:07+00:00",
      "closed_at": "2023-10-24 16:38:46+00:00",
      "resolution_days": 19.894895833333333
    },
    {
      "number": 3475,
      "title": "Tokenizer not picking the right tokens ( mistral openorca )",
      "state": "closed",
      "comments": 40,
      "body_length": 3844,
      "label_names": [],
      "created_at": "2023-10-04 16:27:33+00:00",
      "closed_at": "2023-10-18 06:48:32+00:00",
      "resolution_days": 13.597905092592592
    },
    {
      "number": 3458,
      "title": "finetune: -ngl option to offload to gpu?",
      "state": "closed",
      "comments": 15,
      "body_length": 78,
      "label_names": [],
      "created_at": "2023-10-03 18:03:26+00:00",
      "closed_at": "2023-11-01 11:49:05+00:00",
      "resolution_days": 28.740034722222223
    },
    {
      "number": 3456,
      "title": "[User] How to convert Stability 3B model to ggml/ggul",
      "state": "closed",
      "comments": 13,
      "body_length": 740,
      "label_names": [],
      "created_at": "2023-10-03 17:33:07+00:00",
      "closed_at": "2023-11-14 10:17:14+00:00",
      "resolution_days": 41.69730324074074
    },
    {
      "number": 3440,
      "title": "[User] Implement Streaming LLM - Make the inference more efficient",
      "state": "closed",
      "comments": 19,
      "body_length": 675,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-10-02 17:38:36+00:00",
      "closed_at": "2024-04-03 01:15:25+00:00",
      "resolution_days": 183.3172337962963
    },
    {
      "number": 3433,
      "title": "simple convert scripts run out of tmpfs space on Linux",
      "state": "closed",
      "comments": 18,
      "body_length": 1820,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-10-02 01:28:24+00:00",
      "closed_at": "2024-04-03 01:15:28+00:00",
      "resolution_days": 183.99101851851853
    },
    {
      "number": 3422,
      "title": "[User] AMD GPU slower than CPU",
      "state": "closed",
      "comments": 40,
      "body_length": 19764,
      "label_names": [
        "performance",
        "AMD GPU",
        "stale"
      ],
      "created_at": "2023-10-01 05:57:21+00:00",
      "closed_at": "2024-05-12 01:35:23+00:00",
      "resolution_days": 223.8180787037037
    },
    {
      "number": 3398,
      "title": "Win11-WSL2/Ubuntu22.04/Cuda12/RTX4070 not compilng due ggml-cuda.cu(6125): function \"log2\" ",
      "state": "closed",
      "comments": 13,
      "body_length": 10673,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-09-29 11:58:39+00:00",
      "closed_at": "2024-04-03 01:15:30+00:00",
      "resolution_days": 186.55336805555555
    },
    {
      "number": 3377,
      "title": "llama : support sliding window attention",
      "state": "closed",
      "comments": 21,
      "body_length": 193,
      "label_names": [
        "performance",
        "stale"
      ],
      "created_at": "2023-09-28 12:12:40+00:00",
      "closed_at": "2024-11-01 01:21:36+00:00",
      "resolution_days": 399.5478703703704
    },
    {
      "number": 3365,
      "title": "llama : revisit using flash attention for prompt processing (a.k.a. prefil) + GPU implementation",
      "state": "closed",
      "comments": 12,
      "body_length": 319,
      "label_names": [
        "performance",
        "research \ud83d\udd2c"
      ],
      "created_at": "2023-09-27 15:21:17+00:00",
      "closed_at": "2024-04-30 09:34:02+00:00",
      "resolution_days": 215.75885416666668
    },
    {
      "number": 3320,
      "title": "ROCm error: ggml-cuda.cu:6246: invalid device function",
      "state": "closed",
      "comments": 14,
      "body_length": 814,
      "label_names": [],
      "created_at": "2023-09-23 18:10:24+00:00",
      "closed_at": "2023-10-31 17:24:05+00:00",
      "resolution_days": 37.967835648148146
    },
    {
      "number": 3306,
      "title": "zsh: abort      ./main -m ./models/vicuna/vicuna_v1.5.gguf",
      "state": "closed",
      "comments": 30,
      "body_length": 52220,
      "label_names": [],
      "created_at": "2023-09-22 10:30:49+00:00",
      "closed_at": "2023-12-13 09:38:30+00:00",
      "resolution_days": 81.96366898148148
    },
    {
      "number": 3303,
      "title": "Grammars from other projects",
      "state": "closed",
      "comments": 21,
      "body_length": 107,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-09-21 21:43:21+00:00",
      "closed_at": "2024-04-03 01:15:49+00:00",
      "resolution_days": 194.1475462962963
    },
    {
      "number": 3298,
      "title": "[User]Failed to execute any models on s390x",
      "state": "closed",
      "comments": 18,
      "body_length": 3345,
      "label_names": [],
      "created_at": "2023-09-21 15:27:45+00:00",
      "closed_at": "2023-10-20 11:19:41+00:00",
      "resolution_days": 28.827731481481482
    },
    {
      "number": 3293,
      "title": "GPT-NeoX has only minimal inference support",
      "state": "closed",
      "comments": 12,
      "body_length": 602,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2023-09-21 04:50:58+00:00",
      "closed_at": "2024-05-23 09:49:55+00:00",
      "resolution_days": 245.20760416666667
    },
    {
      "number": 3284,
      "title": "examples for iOS (objc / swift ui)",
      "state": "closed",
      "comments": 22,
      "body_length": 556,
      "label_names": [],
      "created_at": "2023-09-20 20:48:39+00:00",
      "closed_at": "2023-11-28 16:21:00+00:00",
      "resolution_days": 68.81413194444444
    },
    {
      "number": 3270,
      "title": "when will baichuan2 be supported?",
      "state": "closed",
      "comments": 30,
      "body_length": 89,
      "label_names": [],
      "created_at": "2023-09-19 20:24:26+00:00",
      "closed_at": "2023-10-04 14:20:29+00:00",
      "resolution_days": 14.747256944444445
    },
    {
      "number": 3263,
      "title": "[Issue] Prompt Size Issue when using CLBLAST | GGML_ASSERT: ggml.c:11270: ne02 == ne12 Aborted (core dumped)",
      "state": "closed",
      "comments": 12,
      "body_length": 18568,
      "label_names": [],
      "created_at": "2023-09-19 14:44:10+00:00",
      "closed_at": "2023-10-02 19:26:17+00:00",
      "resolution_days": 13.195914351851853
    },
    {
      "number": 3256,
      "title": "Could not find tokenizer.model in llama2",
      "state": "closed",
      "comments": 17,
      "body_length": 1920,
      "label_names": [],
      "created_at": "2023-09-19 06:39:40+00:00",
      "closed_at": "2023-09-22 02:30:20+00:00",
      "resolution_days": 2.8268518518518517
    },
    {
      "number": 3230,
      "title": "CUDA error 217 at ggml-cuda.cu:6292: peer access is not supported between these two devices",
      "state": "closed",
      "comments": 12,
      "body_length": 9822,
      "label_names": [],
      "created_at": "2023-09-17 17:39:50+00:00",
      "closed_at": "2023-09-17 21:35:22+00:00",
      "resolution_days": 0.1635648148148148
    },
    {
      "number": 3149,
      "title": "Is anyone working on 'conditional' logit biasing? (How to potentially improve repetition penalty?)",
      "state": "closed",
      "comments": 12,
      "body_length": 2791,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-09-12 21:12:14+00:00",
      "closed_at": "2024-04-03 01:16:12+00:00",
      "resolution_days": 203.1694212962963
    },
    {
      "number": 3148,
      "title": "Certain 70B Q4_0 quants outputting gibberish (other quant formats unaffected)",
      "state": "closed",
      "comments": 19,
      "body_length": 12551,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-09-12 19:51:19+00:00",
      "closed_at": "2024-04-03 01:16:13+00:00",
      "resolution_days": 203.225625
    },
    {
      "number": 3146,
      "title": "Requesting Support for phi-1_5 by Microsoft",
      "state": "closed",
      "comments": 43,
      "body_length": 180,
      "label_names": [],
      "created_at": "2023-09-12 19:43:33+00:00",
      "closed_at": "2024-02-28 13:03:54+00:00",
      "resolution_days": 168.7224652777778
    },
    {
      "number": 3135,
      "title": "Freeze after offloading layers to GPU",
      "state": "closed",
      "comments": 27,
      "body_length": 3468,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-09-12 03:22:42+00:00",
      "closed_at": "2024-04-03 01:16:16+00:00",
      "resolution_days": 203.91219907407407
    },
    {
      "number": 3129,
      "title": "Intel CPU and Graphics card Macbook pro: failed to create context with model './models/model.q4_k_s.gguf'",
      "state": "closed",
      "comments": 18,
      "body_length": 1483,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-09-11 21:04:11+00:00",
      "closed_at": "2024-04-03 01:16:18+00:00",
      "resolution_days": 204.1750810185185
    },
    {
      "number": 3125,
      "title": "Win 11 ROCm dual GPU models loads then server just shuts down",
      "state": "closed",
      "comments": 22,
      "body_length": 2466,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-09-11 15:38:55+00:00",
      "closed_at": "2024-04-03 01:16:21+00:00",
      "resolution_days": 204.40099537037037
    },
    {
      "number": 3109,
      "title": "How can I use huggingface model?",
      "state": "closed",
      "comments": 12,
      "body_length": 272,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-09-10 11:17:12+00:00",
      "closed_at": "2024-04-03 01:16:25+00:00",
      "resolution_days": 205.58278935185186
    },
    {
      "number": 3092,
      "title": "After b1177 it failed load gguf model on Intel macOS 13.5.1 #macos",
      "state": "closed",
      "comments": 12,
      "body_length": 1622,
      "label_names": [],
      "created_at": "2023-09-09 03:11:51+00:00",
      "closed_at": "2023-09-09 15:00:09+00:00",
      "resolution_days": 0.491875
    },
    {
      "number": 3077,
      "title": "\u91cf\u5316\u4e0e\u90e8\u7f72\u7684\u95ee\u9898",
      "state": "closed",
      "comments": 14,
      "body_length": 212,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-09-08 03:52:21+00:00",
      "closed_at": "2024-04-03 01:16:30+00:00",
      "resolution_days": 207.89177083333334
    },
    {
      "number": 3054,
      "title": "Segfault when compiling with make LLAMA_CUBLAS=1",
      "state": "closed",
      "comments": 12,
      "body_length": 5617,
      "label_names": [],
      "created_at": "2023-09-07 09:30:22+00:00",
      "closed_at": "2023-09-07 18:20:06+00:00",
      "resolution_days": 0.36787037037037035
    },
    {
      "number": 3051,
      "title": "Multi-GPU support for AMD?",
      "state": "closed",
      "comments": 28,
      "body_length": 102,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-09-07 00:15:22+00:00",
      "closed_at": "2024-06-12 01:06:50+00:00",
      "resolution_days": 279.0357407407407
    },
    {
      "number": 3032,
      "title": "Problem with  compilation with gpu support ",
      "state": "closed",
      "comments": 19,
      "body_length": 1802,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-09-05 18:51:51+00:00",
      "closed_at": "2024-04-05 01:06:19+00:00",
      "resolution_days": 212.2600462962963
    },
    {
      "number": 3030,
      "title": "llama.cpp output vs huggingface output",
      "state": "closed",
      "comments": 12,
      "body_length": 1116,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-09-05 18:17:34+00:00",
      "closed_at": "2024-04-05 01:06:20+00:00",
      "resolution_days": 212.28386574074074
    },
    {
      "number": 3002,
      "title": "[User] GGML_ASSERT failure for opencl",
      "state": "closed",
      "comments": 22,
      "body_length": 62419,
      "label_names": [],
      "created_at": "2023-09-04 04:36:18+00:00",
      "closed_at": "2023-10-02 19:26:17+00:00",
      "resolution_days": 28.61804398148148
    },
    {
      "number": 2990,
      "title": "Converting GGML->GGUF: ValueError: Only GGJTv3 supported",
      "state": "closed",
      "comments": 25,
      "body_length": 5890,
      "label_names": [],
      "created_at": "2023-09-03 11:07:07+00:00",
      "closed_at": "2023-09-06 08:49:12+00:00",
      "resolution_days": 2.904224537037037
    },
    {
      "number": 2981,
      "title": "pad_token_id of -1 is not understood by convert.py",
      "state": "closed",
      "comments": 14,
      "body_length": 2011,
      "label_names": [],
      "created_at": "2023-09-03 05:12:51+00:00",
      "closed_at": "2023-12-17 15:45:47+00:00",
      "resolution_days": 105.43953703703704
    },
    {
      "number": 2970,
      "title": "[Bug] Suggested Fixes for mathematical inaccuracy in llama_sample_repetition_penalty function",
      "state": "closed",
      "comments": 12,
      "body_length": 3714,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-09-02 17:08:10+00:00",
      "closed_at": "2024-04-05 01:06:27+00:00",
      "resolution_days": 215.3321412037037
    },
    {
      "number": 2923,
      "title": "llama : combined beam search + grammar sampling strategy",
      "state": "open",
      "comments": 13,
      "body_length": 995,
      "label_names": [
        "good first issue",
        "generation quality",
        "research \ud83d\udd2c",
        "roadmap"
      ],
      "created_at": "2023-08-31 06:29:29+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 2898,
      "title": "common/log.h:290:61: error: expected primary-expression before ',' token",
      "state": "closed",
      "comments": 13,
      "body_length": 309,
      "label_names": [],
      "created_at": "2023-08-30 09:02:10+00:00",
      "closed_at": "2023-09-01 09:07:07+00:00",
      "resolution_days": 2.0034375
    },
    {
      "number": 2894,
      "title": "[User] \u201c'token_embd.weight' has wrong shape\u201d when loading WizardLM-Uncensored-Falcon-40b",
      "state": "closed",
      "comments": 18,
      "body_length": 62557,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-08-30 05:48:10+00:00",
      "closed_at": "2024-04-05 01:06:40+00:00",
      "resolution_days": 218.8045138888889
    },
    {
      "number": 2872,
      "title": "llama : add BERT support",
      "state": "closed",
      "comments": 42,
      "body_length": 571,
      "label_names": [
        "model"
      ],
      "created_at": "2023-08-29 09:57:28+00:00",
      "closed_at": "2024-02-16 14:06:26+00:00",
      "resolution_days": 171.17289351851852
    },
    {
      "number": 2867,
      "title": "failed to load model",
      "state": "closed",
      "comments": 13,
      "body_length": 1425,
      "label_names": [],
      "created_at": "2023-08-29 07:38:48+00:00",
      "closed_at": "2023-10-03 17:31:56+00:00",
      "resolution_days": 35.41189814814815
    },
    {
      "number": 2865,
      "title": "Converting kfkas Llama-2-ko-7b-Chat to GGUF fails",
      "state": "closed",
      "comments": 34,
      "body_length": 53337,
      "label_names": [],
      "created_at": "2023-08-29 06:17:47+00:00",
      "closed_at": "2023-11-28 11:52:00+00:00",
      "resolution_days": 91.2320949074074
    },
    {
      "number": 2844,
      "title": "Performance issues on high performance computer",
      "state": "closed",
      "comments": 14,
      "body_length": 46531,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-08-28 00:36:13+00:00",
      "closed_at": "2024-04-09 01:06:37+00:00",
      "resolution_days": 225.0211111111111
    },
    {
      "number": 2843,
      "title": "Windows ROCm Build.",
      "state": "closed",
      "comments": 25,
      "body_length": 461,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-08-27 19:15:53+00:00",
      "closed_at": "2024-08-07 02:03:37+00:00",
      "resolution_days": 345.28314814814814
    },
    {
      "number": 2838,
      "title": "CUDA non-determinism on identical requests",
      "state": "open",
      "comments": 13,
      "body_length": 1852,
      "label_names": [
        "bug",
        "good first issue"
      ],
      "created_at": "2023-08-27 17:38:30+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 2829,
      "title": "Can't run codellama",
      "state": "closed",
      "comments": 15,
      "body_length": 36010,
      "label_names": [],
      "created_at": "2023-08-27 11:07:38+00:00",
      "closed_at": "2023-08-30 14:30:05+00:00",
      "resolution_days": 3.1405902777777777
    },
    {
      "number": 2828,
      "title": "Codellama on gpu M1",
      "state": "closed",
      "comments": 16,
      "body_length": 1380,
      "label_names": [],
      "created_at": "2023-08-27 09:32:01+00:00",
      "closed_at": "2023-08-28 12:41:23+00:00",
      "resolution_days": 1.1315046296296296
    },
    {
      "number": 2820,
      "title": "convert.py : handle special tokens",
      "state": "closed",
      "comments": 44,
      "body_length": 366,
      "label_names": [
        "enhancement",
        "good first issue"
      ],
      "created_at": "2023-08-26 18:31:52+00:00",
      "closed_at": "2023-10-18 13:47:59+00:00",
      "resolution_days": 52.8028587962963
    },
    {
      "number": 2813,
      "title": "llama : add support for batched inference",
      "state": "closed",
      "comments": 19,
      "body_length": 1954,
      "label_names": [
        "performance"
      ],
      "created_at": "2023-08-26 13:20:56+00:00",
      "closed_at": "2023-09-28 16:05:57+00:00",
      "resolution_days": 33.11459490740741
    },
    {
      "number": 2812,
      "title": "Unable to convert GGML to GGUF",
      "state": "closed",
      "comments": 13,
      "body_length": 1270,
      "label_names": [],
      "created_at": "2023-08-26 13:13:53+00:00",
      "closed_at": "2023-09-05 10:23:36+00:00",
      "resolution_days": 9.881747685185186
    },
    {
      "number": 2766,
      "title": "[Question] Will \"Code Llama\" be supported out of the box? ",
      "state": "closed",
      "comments": 16,
      "body_length": 600,
      "label_names": [],
      "created_at": "2023-08-24 13:44:23+00:00",
      "closed_at": "2023-08-28 17:11:57+00:00",
      "resolution_days": 4.1441435185185185
    },
    {
      "number": 2754,
      "title": "[User] -n -2 generates nothing",
      "state": "closed",
      "comments": 21,
      "body_length": 89,
      "label_names": [],
      "created_at": "2023-08-23 23:20:31+00:00",
      "closed_at": "2023-08-24 15:48:35+00:00",
      "resolution_days": 0.6861574074074074
    },
    {
      "number": 2715,
      "title": "Error converting GGML to GGUF",
      "state": "closed",
      "comments": 16,
      "body_length": 1376,
      "label_names": [],
      "created_at": "2023-08-22 15:53:23+00:00",
      "closed_at": "2023-08-23 09:32:10+00:00",
      "resolution_days": 0.7352662037037037
    },
    {
      "number": 2711,
      "title": "[User] GGUF conversion, stop sequence Problem",
      "state": "closed",
      "comments": 19,
      "body_length": 7881,
      "label_names": [],
      "created_at": "2023-08-22 12:27:28+00:00",
      "closed_at": "2023-08-23 00:29:28+00:00",
      "resolution_days": 0.5013888888888889
    },
    {
      "number": 2697,
      "title": "Silently failing ggml to gguf conversion",
      "state": "closed",
      "comments": 13,
      "body_length": 1742,
      "label_names": [],
      "created_at": "2023-08-21 23:26:06+00:00",
      "closed_at": "2023-08-22 00:01:35+00:00",
      "resolution_days": 0.024641203703703703
    },
    {
      "number": 2694,
      "title": "main : add detailed trace to a log file",
      "state": "closed",
      "comments": 12,
      "body_length": 667,
      "label_names": [
        "enhancement",
        "help wanted",
        "good first issue",
        "high priority"
      ],
      "created_at": "2023-08-21 18:21:14+00:00",
      "closed_at": "2023-08-30 06:30:00+00:00",
      "resolution_days": 8.506087962962964
    },
    {
      "number": 2687,
      "title": "will this project support on device npu like qualcomm hexagon\uff1f",
      "state": "closed",
      "comments": 40,
      "body_length": 163,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-08-21 06:29:48+00:00",
      "closed_at": "2024-06-06 01:07:06+00:00",
      "resolution_days": 289.7759027777778
    },
    {
      "number": 2572,
      "title": "POST to server takes forever",
      "state": "closed",
      "comments": 18,
      "body_length": 5593,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-08-10 00:28:51+00:00",
      "closed_at": "2024-05-12 01:35:24+00:00",
      "resolution_days": 276.0462152777778
    },
    {
      "number": 2555,
      "title": "Add support for AMX instructions (bf16 and/or int8)",
      "state": "closed",
      "comments": 20,
      "body_length": 2958,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-08-08 16:20:40+00:00",
      "closed_at": "2024-05-18 01:58:36+00:00",
      "resolution_days": 283.4013425925926
    },
    {
      "number": 2540,
      "title": "Is there any support for AMD GPU (ROCM)",
      "state": "closed",
      "comments": 28,
      "body_length": 108,
      "label_names": [],
      "created_at": "2023-08-07 12:00:22+00:00",
      "closed_at": "2023-08-25 09:09:44+00:00",
      "resolution_days": 17.881504629629628
    },
    {
      "number": 2530,
      "title": "Request support for LLaMA-2-7B-32K",
      "state": "closed",
      "comments": 13,
      "body_length": 542,
      "label_names": [],
      "created_at": "2023-08-06 10:09:30+00:00",
      "closed_at": "2023-08-07 15:33:49+00:00",
      "resolution_days": 1.2252199074074075
    },
    {
      "number": 2516,
      "title": "Convert.py issue",
      "state": "closed",
      "comments": 16,
      "body_length": 1134,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-08-04 18:03:29+00:00",
      "closed_at": "2024-04-09 01:07:06+00:00",
      "resolution_days": 248.29417824074073
    },
    {
      "number": 2507,
      "title": "Regression in interactive mode",
      "state": "closed",
      "comments": 12,
      "body_length": 963,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-08-03 21:29:35+00:00",
      "closed_at": "2024-04-09 01:07:09+00:00",
      "resolution_days": 249.15108796296298
    },
    {
      "number": 2428,
      "title": "[Prompt Processing] Is there a way to speed up prompt processing for Metal? (M1/M2)",
      "state": "closed",
      "comments": 16,
      "body_length": 944,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-07-27 22:00:17+00:00",
      "closed_at": "2024-04-09 01:07:22+00:00",
      "resolution_days": 256.12991898148147
    },
    {
      "number": 2417,
      "title": "Prompt structure after the --in-prefix-bos commit",
      "state": "closed",
      "comments": 12,
      "body_length": 1663,
      "label_names": [],
      "created_at": "2023-07-27 07:29:43+00:00",
      "closed_at": "2023-08-05 18:24:35+00:00",
      "resolution_days": 9.454768518518518
    },
    {
      "number": 2379,
      "title": "llama : add support for llama2.c models",
      "state": "closed",
      "comments": 93,
      "body_length": 437,
      "label_names": [
        "help wanted",
        "good first issue",
        "model"
      ],
      "created_at": "2023-07-24 20:15:39+00:00",
      "closed_at": "2023-08-11 23:17:27+00:00",
      "resolution_days": 18.12625
    },
    {
      "number": 2376,
      "title": "Llama 2 70B: Update needed to convert.py to support 70B HF format model files",
      "state": "closed",
      "comments": 19,
      "body_length": 1447,
      "label_names": [
        "help wanted",
        "high priority"
      ],
      "created_at": "2023-07-24 16:24:41+00:00",
      "closed_at": "2023-07-27 20:39:19+00:00",
      "resolution_days": 3.1768287037037037
    },
    {
      "number": 2361,
      "title": "quantize exiting prematurely without error msg",
      "state": "closed",
      "comments": 27,
      "body_length": 6152,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-07-24 07:24:50+00:00",
      "closed_at": "2024-04-09 01:07:31+00:00",
      "resolution_days": 259.737974537037
    },
    {
      "number": 2310,
      "title": "Tokenization is not equal to Meta's tokenization.",
      "state": "closed",
      "comments": 24,
      "body_length": 2131,
      "label_names": [
        "help wanted",
        "high priority"
      ],
      "created_at": "2023-07-21 18:07:02+00:00",
      "closed_at": "2023-09-20 07:03:24+00:00",
      "resolution_days": 60.53914351851852
    },
    {
      "number": 2302,
      "title": "GPU Inference error -  Cannot find lcublasLt",
      "state": "closed",
      "comments": 17,
      "body_length": 1705,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-07-21 01:16:06+00:00",
      "closed_at": "2024-04-09 01:07:43+00:00",
      "resolution_days": 262.9941782407407
    },
    {
      "number": 2262,
      "title": "Add llama 2 model",
      "state": "closed",
      "comments": 95,
      "body_length": 232,
      "label_names": [
        "\ud83e\udd99.",
        "model"
      ],
      "created_at": "2023-07-18 16:35:53+00:00",
      "closed_at": "2023-10-18 07:31:45+00:00",
      "resolution_days": 91.62212962962963
    },
    {
      "number": 2259,
      "title": "[User] interactive mode with --multiline-input cannot input  more than around 13000 byte  words by one time.",
      "state": "closed",
      "comments": 18,
      "body_length": 180,
      "label_names": [],
      "created_at": "2023-07-18 14:54:55+00:00",
      "closed_at": "2023-07-18 15:32:46+00:00",
      "resolution_days": 0.026284722222222223
    },
    {
      "number": 2164,
      "title": "mpi : attempt inference of 65B LLaMA on a cluster of Raspberry Pis",
      "state": "open",
      "comments": 54,
      "body_length": 1101,
      "label_names": [
        "help wanted",
        "\ud83e\udd99.",
        "hardware",
        "research \ud83d\udd2c"
      ],
      "created_at": "2023-07-10 16:12:22+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 2134,
      "title": "Large sched_yield() and threading overhead (+25-40% perf boost)",
      "state": "closed",
      "comments": 33,
      "body_length": 5898,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-07-07 14:31:45+00:00",
      "closed_at": "2024-05-19 01:07:17+00:00",
      "resolution_days": 316.4413425925926
    },
    {
      "number": 2090,
      "title": "illegal instrution",
      "state": "closed",
      "comments": 27,
      "body_length": 1162,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-07-03 17:15:27+00:00",
      "closed_at": "2024-04-09 01:08:32+00:00",
      "resolution_days": 280.3285300925926
    },
    {
      "number": 2083,
      "title": "llama : add support for Classifier-Free Guidance (CFG) sampling to stay on topic better",
      "state": "closed",
      "comments": 19,
      "body_length": 703,
      "label_names": [
        "enhancement",
        "good first issue",
        "generation quality",
        "research \ud83d\udd2c"
      ],
      "created_at": "2023-07-03 08:38:55+00:00",
      "closed_at": "2023-07-11 16:18:45+00:00",
      "resolution_days": 8.319328703703704
    },
    {
      "number": 2052,
      "title": "Pool Android performance and GPU not used at all when built with OpenCL",
      "state": "closed",
      "comments": 20,
      "body_length": 1012,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-06-29 18:40:30+00:00",
      "closed_at": "2024-04-09 01:08:33+00:00",
      "resolution_days": 284.26947916666666
    },
    {
      "number": 2030,
      "title": "llama : add example for speculative sampling",
      "state": "closed",
      "comments": 12,
      "body_length": 763,
      "label_names": [
        "performance",
        "generation quality",
        "research \ud83d\udd2c"
      ],
      "created_at": "2023-06-28 05:20:52+00:00",
      "closed_at": "2023-09-03 12:29:06+00:00",
      "resolution_days": 67.29738425925926
    },
    {
      "number": 1971,
      "title": "[ENHANCEMENT] New MPT 30B + CUDA support.",
      "state": "closed",
      "comments": 18,
      "body_length": 3177,
      "label_names": [],
      "created_at": "2023-06-22 19:44:30+00:00",
      "closed_at": "2023-11-05 03:46:24+00:00",
      "resolution_days": 135.3346527777778
    },
    {
      "number": 1955,
      "title": "Investigate PagedAttention KV-cache memory management for faster inference",
      "state": "closed",
      "comments": 37,
      "body_length": 1105,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-06-20 22:47:57+00:00",
      "closed_at": "2024-04-10 01:06:50+00:00",
      "resolution_days": 294.09644675925927
    },
    {
      "number": 1909,
      "title": "[User] nonsense responses with q2_k llama in Termux when using GPU",
      "state": "closed",
      "comments": 27,
      "body_length": 15165,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-06-17 13:48:07+00:00",
      "closed_at": "2024-04-10 01:06:54+00:00",
      "resolution_days": 297.4713773148148
    },
    {
      "number": 1870,
      "title": "[User] GPU Memory problem on Apple M2 Max 64GB ",
      "state": "closed",
      "comments": 22,
      "body_length": 1467,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-06-15 10:45:55+00:00",
      "closed_at": "2024-04-10 01:07:03+00:00",
      "resolution_days": 299.59800925925924
    },
    {
      "number": 1866,
      "title": "CUDA out of memory - but there's plenty of memory",
      "state": "closed",
      "comments": 24,
      "body_length": 4193,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-06-15 03:46:02+00:00",
      "closed_at": "2024-04-10 01:07:07+00:00",
      "resolution_days": 299.8896412037037
    },
    {
      "number": 1815,
      "title": "[METAL] GPU Inference fails due to buffer error (buffer \"data\" size is larger than buffer maximum)",
      "state": "closed",
      "comments": 20,
      "body_length": 4983,
      "label_names": [],
      "created_at": "2023-06-12 08:10:37+00:00",
      "closed_at": "2023-06-18 06:09:48+00:00",
      "resolution_days": 5.916099537037037
    },
    {
      "number": 1809,
      "title": "Output halts for 10 seconds every 10-20 lines in long output using Apple Silicon",
      "state": "closed",
      "comments": 12,
      "body_length": 1808,
      "label_names": [],
      "created_at": "2023-06-12 00:20:07+00:00",
      "closed_at": "2023-06-24 08:51:33+00:00",
      "resolution_days": 12.355162037037037
    },
    {
      "number": 1778,
      "title": "when compiling, the following error occurred: 'identifier cublasGetStatusString is undefined'",
      "state": "closed",
      "comments": 15,
      "body_length": 4520,
      "label_names": [],
      "created_at": "2023-06-09 13:29:14+00:00",
      "closed_at": "2023-06-14 23:43:28+00:00",
      "resolution_days": 5.426550925925926
    },
    {
      "number": 1739,
      "title": "Feature request: documented machine-readable output as a top tier feature",
      "state": "closed",
      "comments": 17,
      "body_length": 865,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-06-07 09:24:55+00:00",
      "closed_at": "2024-04-10 01:07:41+00:00",
      "resolution_days": 307.6546990740741
    },
    {
      "number": 1735,
      "title": "with the newest builds i only get gibberish output",
      "state": "closed",
      "comments": 81,
      "body_length": 663,
      "label_names": [
        "bug",
        "high priority"
      ],
      "created_at": "2023-06-07 08:06:19+00:00",
      "closed_at": "2023-06-15 08:50:50+00:00",
      "resolution_days": 8.030914351851852
    },
    {
      "number": 1730,
      "title": "Apple M1 metal lag",
      "state": "closed",
      "comments": 19,
      "body_length": 918,
      "label_names": [],
      "created_at": "2023-06-07 04:41:24+00:00",
      "closed_at": "2023-10-02 20:00:35+00:00",
      "resolution_days": 117.63832175925926
    },
    {
      "number": 1713,
      "title": "[Feature request] AWQ (activation-aware weight quantization) 4-bit quantization",
      "state": "closed",
      "comments": 14,
      "body_length": 360,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-06-06 08:50:48+00:00",
      "closed_at": "2024-04-10 01:07:48+00:00",
      "resolution_days": 308.67847222222224
    },
    {
      "number": 1602,
      "title": "llama : add Falcon LLM support",
      "state": "closed",
      "comments": 210,
      "body_length": 362,
      "label_names": [
        "help wanted",
        "model"
      ],
      "created_at": "2023-05-26 17:45:06+00:00",
      "closed_at": "2023-08-23 20:11:44+00:00",
      "resolution_days": 89.1018287037037
    },
    {
      "number": 1590,
      "title": "Quantization does not write the quantization version to `ftype`",
      "state": "closed",
      "comments": 13,
      "body_length": 1700,
      "label_names": [
        "good first issue",
        "high priority"
      ],
      "created_at": "2023-05-25 00:30:00+00:00",
      "closed_at": "2023-07-28 19:23:43+00:00",
      "resolution_days": 64.78730324074074
    },
    {
      "number": 1585,
      "title": "Bug when prompt stored in `--prompt-cache` is longer than the new one",
      "state": "closed",
      "comments": 14,
      "body_length": 6244,
      "label_names": [],
      "created_at": "2023-05-24 09:15:49+00:00",
      "closed_at": "2023-05-29 12:13:43+00:00",
      "resolution_days": 5.123541666666667
    },
    {
      "number": 1575,
      "title": "Extend ggml format to include a description of the model.",
      "state": "closed",
      "comments": 14,
      "body_length": 365,
      "label_names": [
        "enhancement",
        "help wanted"
      ],
      "created_at": "2023-05-23 16:39:04+00:00",
      "closed_at": "2024-05-17 12:37:14+00:00",
      "resolution_days": 359.8320601851852
    },
    {
      "number": 1572,
      "title": "WHY WHY WHY ?????",
      "state": "closed",
      "comments": 39,
      "body_length": 613,
      "label_names": [],
      "created_at": "2023-05-23 14:53:36+00:00",
      "closed_at": "2023-05-31 10:11:58+00:00",
      "resolution_days": 7.804421296296296
    },
    {
      "number": 1571,
      "title": "OpenCl compiling issue",
      "state": "closed",
      "comments": 58,
      "body_length": 25188,
      "label_names": [],
      "created_at": "2023-05-23 14:00:31+00:00",
      "closed_at": "2023-05-29 12:53:30+00:00",
      "resolution_days": 5.953460648148148
    },
    {
      "number": 1557,
      "title": "runtime error in example/server",
      "state": "closed",
      "comments": 15,
      "body_length": 1035,
      "label_names": [],
      "created_at": "2023-05-22 04:20:31+00:00",
      "closed_at": "2023-05-25 00:54:02+00:00",
      "resolution_days": 2.8566087962962965
    },
    {
      "number": 1552,
      "title": "[Feature] Is there a way to get response with embeddings as input?",
      "state": "closed",
      "comments": 14,
      "body_length": 1169,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-05-21 13:32:32+00:00",
      "closed_at": "2024-04-09 01:08:55+00:00",
      "resolution_days": 323.48359953703704
    },
    {
      "number": 1545,
      "title": "Apple Silicon GPU Support Possible?",
      "state": "closed",
      "comments": 35,
      "body_length": 208,
      "label_names": [],
      "created_at": "2023-05-21 05:51:06+00:00",
      "closed_at": "2023-06-09 08:05:30+00:00",
      "resolution_days": 19.093333333333334
    },
    {
      "number": 1515,
      "title": "Slower Response on large context size",
      "state": "closed",
      "comments": 16,
      "body_length": 2222,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-05-18 12:58:30+00:00",
      "closed_at": "2024-04-09 01:08:59+00:00",
      "resolution_days": 326.50728009259257
    },
    {
      "number": 1503,
      "title": "Multi-part GGML files: do they still work? And how hard would it be to modify convert.py to create them?",
      "state": "closed",
      "comments": 18,
      "body_length": 2076,
      "label_names": [],
      "created_at": "2023-05-17 13:25:20+00:00",
      "closed_at": "2023-05-18 09:38:04+00:00",
      "resolution_days": 0.8421759259259259
    },
    {
      "number": 1499,
      "title": "[Feature request] Any plans for AMD XDNA AI Engine support on Ryzen 7x40 processors?",
      "state": "closed",
      "comments": 92,
      "body_length": 833,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-05-17 09:57:42+00:00",
      "closed_at": "2025-04-07 01:09:20+00:00",
      "resolution_days": 690.6330787037037
    },
    {
      "number": 1470,
      "title": "Error on make LLAMA_CUBLAS=1",
      "state": "closed",
      "comments": 12,
      "body_length": 2196,
      "label_names": [],
      "created_at": "2023-05-15 21:19:17+00:00",
      "closed_at": "2023-05-16 16:40:07+00:00",
      "resolution_days": 0.8061342592592593
    },
    {
      "number": 1456,
      "title": "CUDA/OpenCL error, out of memory when reload.",
      "state": "closed",
      "comments": 25,
      "body_length": 2675,
      "label_names": [
        "bug",
        "high priority",
        "hardware"
      ],
      "created_at": "2023-05-14 17:56:02+00:00",
      "closed_at": "2023-06-09 23:16:05+00:00",
      "resolution_days": 26.222256944444446
    },
    {
      "number": 1455,
      "title": "Can't compile on Jetson Orin NX with \"LLAMA_CUBLAS=1\"",
      "state": "closed",
      "comments": 12,
      "body_length": 6582,
      "label_names": [],
      "created_at": "2023-05-14 15:55:12+00:00",
      "closed_at": "2023-05-20 11:29:38+00:00",
      "resolution_days": 5.815578703703704
    },
    {
      "number": 1437,
      "title": "LLaMA NUMA could be better",
      "state": "closed",
      "comments": 48,
      "body_length": 6727,
      "label_names": [
        "performance",
        "stale"
      ],
      "created_at": "2023-05-13 20:49:45+00:00",
      "closed_at": "2024-06-20 01:17:33+00:00",
      "resolution_days": 403.18597222222223
    },
    {
      "number": 1429,
      "title": "Anyone got CLBLAST working on Intel macOS with AMD GPU? Is it meant to work?",
      "state": "closed",
      "comments": 19,
      "body_length": 7423,
      "label_names": [],
      "created_at": "2023-05-13 13:11:32+00:00",
      "closed_at": "2023-05-14 13:23:13+00:00",
      "resolution_days": 1.008113425925926
    },
    {
      "number": 1408,
      "title": " this format is no longer supported",
      "state": "closed",
      "comments": 21,
      "body_length": 1080,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-05-11 22:15:12+00:00",
      "closed_at": "2024-04-09 01:09:21+00:00",
      "resolution_days": 333.1209375
    },
    {
      "number": 1398,
      "title": "[Feature Request] --prompt-cache-all + user input",
      "state": "closed",
      "comments": 17,
      "body_length": 342,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-05-11 00:38:58+00:00",
      "closed_at": "2024-04-09 01:09:25+00:00",
      "resolution_days": 334.0211458333333
    },
    {
      "number": 1382,
      "title": "multiline-input",
      "state": "closed",
      "comments": 30,
      "body_length": 436,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-05-09 17:07:27+00:00",
      "closed_at": "2024-04-09 01:09:31+00:00",
      "resolution_days": 335.3347685185185
    },
    {
      "number": 1354,
      "title": "[User] Chinese alpaca need more REQ memory",
      "state": "closed",
      "comments": 16,
      "body_length": 6013,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-05-07 13:18:14+00:00",
      "closed_at": "2024-04-09 01:09:35+00:00",
      "resolution_days": 337.49399305555556
    },
    {
      "number": 1344,
      "title": "How to quantize a model ?",
      "state": "closed",
      "comments": 15,
      "body_length": 612,
      "label_names": [],
      "created_at": "2023-05-06 14:18:54+00:00",
      "closed_at": "2023-05-08 17:21:26+00:00",
      "resolution_days": 2.1267592592592592
    },
    {
      "number": 1337,
      "title": "Implement Together Computer's Red Pajama 3B Base/Chat model",
      "state": "closed",
      "comments": 23,
      "body_length": 417,
      "label_names": [
        "model",
        "stale"
      ],
      "created_at": "2023-05-06 01:48:53+00:00",
      "closed_at": "2024-04-09 01:09:36+00:00",
      "resolution_days": 338.9727199074074
    },
    {
      "number": 1333,
      "title": "Implement MosiacML's 7B model.",
      "state": "closed",
      "comments": 25,
      "body_length": 164,
      "label_names": [
        "help wanted",
        "model"
      ],
      "created_at": "2023-05-05 17:44:48+00:00",
      "closed_at": "2023-11-02 00:54:44+00:00",
      "resolution_days": 180.29856481481482
    },
    {
      "number": 1291,
      "title": "Try whether OpenLLaMa works",
      "state": "closed",
      "comments": 82,
      "body_length": 299,
      "label_names": [
        "\ud83e\udd99.",
        "model",
        "stale"
      ],
      "created_at": "2023-05-02 21:53:20+00:00",
      "closed_at": "2024-04-09 01:09:41+00:00",
      "resolution_days": 342.13635416666665
    },
    {
      "number": 1281,
      "title": "[Feature Request] Ability to rewind model evaluation by a fixed number of tokens",
      "state": "closed",
      "comments": 13,
      "body_length": 433,
      "label_names": [
        "enhancement",
        "good first issue",
        "high priority"
      ],
      "created_at": "2023-05-02 15:00:02+00:00",
      "closed_at": "2023-05-05 01:52:30+00:00",
      "resolution_days": 2.453101851851852
    },
    {
      "number": 1230,
      "title": "WSL: CUDA error 2 at ggml-cuda.cu:359: out of memory (Fix found)",
      "state": "closed",
      "comments": 17,
      "body_length": 8827,
      "label_names": [],
      "created_at": "2023-04-29 12:20:06+00:00",
      "closed_at": "2023-04-29 16:34:50+00:00",
      "resolution_days": 0.17689814814814814
    },
    {
      "number": 1224,
      "title": "Llama Ignoring Reverse Prompt Every Other Time",
      "state": "closed",
      "comments": 29,
      "body_length": 6685,
      "label_names": [
        "bug",
        "help wanted"
      ],
      "created_at": "2023-04-29 06:44:31+00:00",
      "closed_at": "2023-05-04 10:02:32+00:00",
      "resolution_days": 5.137511574074074
    },
    {
      "number": 1217,
      "title": "ClBlast - no gpu load, no perfomans difference.",
      "state": "closed",
      "comments": 28,
      "body_length": 1288,
      "label_names": [
        "performance",
        "hardware",
        "build"
      ],
      "created_at": "2023-04-28 16:05:41+00:00",
      "closed_at": "2023-05-05 00:51:53+00:00",
      "resolution_days": 6.3654166666666665
    },
    {
      "number": 1129,
      "title": "Performance issues with cuBLAS and a bug",
      "state": "closed",
      "comments": 29,
      "body_length": 3764,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-04-22 15:26:30+00:00",
      "closed_at": "2024-04-09 01:10:00+00:00",
      "resolution_days": 352.40520833333335
    },
    {
      "number": 1063,
      "title": "Support StableLM From StabilityAI",
      "state": "closed",
      "comments": 20,
      "body_length": 1527,
      "label_names": [
        "enhancement",
        "help wanted",
        "model"
      ],
      "created_at": "2023-04-19 15:48:06+00:00",
      "closed_at": "2023-07-28 19:50:06+00:00",
      "resolution_days": 100.16805555555555
    },
    {
      "number": 1060,
      "title": "rocBLAS support",
      "state": "closed",
      "comments": 36,
      "body_length": 195,
      "label_names": [],
      "created_at": "2023-04-19 13:42:53+00:00",
      "closed_at": "2023-04-25 19:23:26+00:00",
      "resolution_days": 6.236493055555556
    },
    {
      "number": 1059,
      "title": "Any chance of adding Clblast support?",
      "state": "closed",
      "comments": 12,
      "body_length": 192,
      "label_names": [],
      "created_at": "2023-04-19 13:27:11+00:00",
      "closed_at": "2023-04-28 14:57:19+00:00",
      "resolution_days": 9.062592592592592
    },
    {
      "number": 1055,
      "title": "Too many releases",
      "state": "closed",
      "comments": 13,
      "body_length": 135,
      "label_names": [
        "enhancement",
        "help wanted",
        "good first issue"
      ],
      "created_at": "2023-04-19 07:51:03+00:00",
      "closed_at": "2023-07-28 19:21:59+00:00",
      "resolution_days": 100.47981481481482
    },
    {
      "number": 1037,
      "title": "4bit version of gpt4all-alpaca-oa-codealpaca-Lora-13b?",
      "state": "closed",
      "comments": 22,
      "body_length": 563,
      "label_names": [],
      "created_at": "2023-04-18 04:47:11+00:00",
      "closed_at": "2023-05-08 09:34:10+00:00",
      "resolution_days": 20.199293981481482
    },
    {
      "number": 1010,
      "title": "[User] Interactive mode immediately exits on Windows with Zig",
      "state": "closed",
      "comments": 16,
      "body_length": 8730,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-04-16 03:52:41+00:00",
      "closed_at": "2024-04-09 01:10:11+00:00",
      "resolution_days": 358.88715277777777
    },
    {
      "number": 999,
      "title": "how could i stop printing  time execution",
      "state": "closed",
      "comments": 16,
      "body_length": 451,
      "label_names": [],
      "created_at": "2023-04-15 14:45:51+00:00",
      "closed_at": "2023-04-16 01:16:55+00:00",
      "resolution_days": 0.43824074074074076
    },
    {
      "number": 989,
      "title": "85%+ of the llama model could be redundant",
      "state": "closed",
      "comments": 12,
      "body_length": 333,
      "label_names": [],
      "created_at": "2023-04-14 21:29:29+00:00",
      "closed_at": "2023-07-28 19:51:22+00:00",
      "resolution_days": 104.93186342592593
    },
    {
      "number": 984,
      "title": "GPT4All-J conversion",
      "state": "closed",
      "comments": 12,
      "body_length": 1479,
      "label_names": [],
      "created_at": "2023-04-14 20:13:08+00:00",
      "closed_at": "2023-04-18 22:41:31+00:00",
      "resolution_days": 4.103043981481481
    },
    {
      "number": 956,
      "title": "perf(ggml): tall and skinny GEMM for LoRA: F32 `mul_mat([16 X 5120], [16 X 5120])` takes 120ms - 24x slower than expected",
      "state": "closed",
      "comments": 13,
      "body_length": 936,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-04-14 04:01:56+00:00",
      "closed_at": "2024-04-09 01:10:20+00:00",
      "resolution_days": 360.8808333333333
    },
    {
      "number": 942,
      "title": "start to present code automatically in interactive mode.",
      "state": "closed",
      "comments": 14,
      "body_length": 478,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-04-13 08:42:49+00:00",
      "closed_at": "2024-04-11 01:06:33+00:00",
      "resolution_days": 363.6831481481481
    },
    {
      "number": 899,
      "title": "[User] Embedding doesn't seem to work?",
      "state": "closed",
      "comments": 56,
      "body_length": 4115,
      "label_names": [
        "generation quality"
      ],
      "created_at": "2023-04-11 17:22:57+00:00",
      "closed_at": "2024-03-14 13:24:32+00:00",
      "resolution_days": 337.8344328703704
    },
    {
      "number": 894,
      "title": "The procedure entry point PrefetchVirtualMemory could not be located in the dynamic link library KERNEL32.dll",
      "state": "closed",
      "comments": 16,
      "body_length": 643,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-04-11 13:42:44+00:00",
      "closed_at": "2024-08-07 02:03:39+00:00",
      "resolution_days": 483.514525462963
    },
    {
      "number": 864,
      "title": "[User] Memory usage is extremely low when running 65b 4-bit models. (Only use 5GB)",
      "state": "closed",
      "comments": 22,
      "body_length": 3207,
      "label_names": [],
      "created_at": "2023-04-09 14:54:14+00:00",
      "closed_at": "2023-04-12 12:05:00+00:00",
      "resolution_days": 2.8824768518518518
    },
    {
      "number": 846,
      "title": "llama : add RWKV models support",
      "state": "closed",
      "comments": 39,
      "body_length": 1631,
      "label_names": [
        "help wanted",
        "good first issue",
        "model"
      ],
      "created_at": "2023-04-08 06:32:31+00:00",
      "closed_at": "2024-09-01 14:38:18+00:00",
      "resolution_days": 512.3373495370371
    },
    {
      "number": 842,
      "title": "Performance e-core bug(?) -  only 50% CPU utilization when using all threads -  (Win11, Intel 13900k)",
      "state": "closed",
      "comments": 24,
      "body_length": 1679,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-04-08 03:07:10+00:00",
      "closed_at": "2024-04-11 01:06:51+00:00",
      "resolution_days": 368.91644675925926
    },
    {
      "number": 829,
      "title": "Port to Google Tensor G2/G3 on Pixel Phones",
      "state": "closed",
      "comments": 13,
      "body_length": 62,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-04-07 10:57:33+00:00",
      "closed_at": "2024-05-20 01:09:08+00:00",
      "resolution_days": 408.5913773148148
    },
    {
      "number": 808,
      "title": "How do i use convert-unversioned-ggml-to-ggml.py?",
      "state": "closed",
      "comments": 12,
      "body_length": 1198,
      "label_names": [
        "bug",
        "model"
      ],
      "created_at": "2023-04-06 12:22:58+00:00",
      "closed_at": "2023-04-14 13:12:39+00:00",
      "resolution_days": 8.034502314814814
    },
    {
      "number": 802,
      "title": "llama.cpp acts too dumb while running on phone!!",
      "state": "closed",
      "comments": 13,
      "body_length": 909,
      "label_names": [
        "generation quality"
      ],
      "created_at": "2023-04-06 06:43:33+00:00",
      "closed_at": "2023-04-15 17:23:12+00:00",
      "resolution_days": 9.444201388888889
    },
    {
      "number": 799,
      "title": "What would it take to 100x the context window?",
      "state": "closed",
      "comments": 31,
      "body_length": 507,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2023-04-06 03:18:40+00:00",
      "closed_at": "2024-04-11 01:06:59+00:00",
      "resolution_days": 370.90855324074073
    },
    {
      "number": 788,
      "title": "Compilation failed on macOS 10.7-8-9: 'clock_gettime' produce warnings and errors",
      "state": "closed",
      "comments": 20,
      "body_length": 9515,
      "label_names": [
        "bug",
        "build",
        "macos",
        "stale"
      ],
      "created_at": "2023-04-05 19:16:14+00:00",
      "closed_at": "2024-04-11 01:07:01+00:00",
      "resolution_days": 371.243599537037
    },
    {
      "number": 771,
      "title": "Running a Vicuna-13B 4it model ?",
      "state": "closed",
      "comments": 25,
      "body_length": 420,
      "label_names": [
        "model",
        "generation quality"
      ],
      "created_at": "2023-04-05 07:33:04+00:00",
      "closed_at": "2023-07-28 19:47:57+00:00",
      "resolution_days": 114.51033564814814
    },
    {
      "number": 767,
      "title": "Token generation is extremely slow when using 13B models on an M1 Pro with llama.cpp, but it runs at a fine speed with Dalai (which uses an older version of llama.cpp)",
      "state": "closed",
      "comments": 30,
      "body_length": 834,
      "label_names": [
        "performance"
      ],
      "created_at": "2023-04-04 17:33:04+00:00",
      "closed_at": "2023-07-28 19:47:23+00:00",
      "resolution_days": 115.09327546296296
    },
    {
      "number": 730,
      "title": "New kv_cache API insufficient to restore model state",
      "state": "closed",
      "comments": 23,
      "body_length": 3248,
      "label_names": [
        "bug",
        "help wanted",
        "good first issue",
        "high priority"
      ],
      "created_at": "2023-04-03 03:28:49+00:00",
      "closed_at": "2023-04-23 13:51:21+00:00",
      "resolution_days": 20.432314814814816
    },
    {
      "number": 722,
      "title": "Rockchip RK3588 perf",
      "state": "closed",
      "comments": 103,
      "body_length": 4586,
      "label_names": [],
      "created_at": "2023-04-02 20:39:28+00:00",
      "closed_at": "2023-04-02 22:14:36+00:00",
      "resolution_days": 0.06606481481481481
    },
    {
      "number": 719,
      "title": "Question: Why prompt is being run trough the network before generating new tokens?",
      "state": "closed",
      "comments": 31,
      "body_length": 401,
      "label_names": [],
      "created_at": "2023-04-02 19:48:07+00:00",
      "closed_at": "2023-04-03 14:43:18+00:00",
      "resolution_days": 0.7883217592592593
    },
    {
      "number": 705,
      "title": "Windows page fault disk i/o slow on first load",
      "state": "closed",
      "comments": 37,
      "body_length": 1289,
      "label_names": [
        "performance",
        "windows",
        "stale"
      ],
      "created_at": "2023-04-02 10:04:24+00:00",
      "closed_at": "2024-04-11 01:07:14+00:00",
      "resolution_days": 374.6269675925926
    },
    {
      "number": 693,
      "title": "Regression: \"The first main on the moon was \"",
      "state": "closed",
      "comments": 14,
      "body_length": 269,
      "label_names": [
        "question"
      ],
      "created_at": "2023-04-01 23:16:25+00:00",
      "closed_at": "2023-05-16 19:10:10+00:00",
      "resolution_days": 44.82899305555556
    },
    {
      "number": 647,
      "title": "Confusion about the model versioning",
      "state": "closed",
      "comments": 21,
      "body_length": 3686,
      "label_names": [
        "documentation",
        "enhancement"
      ],
      "created_at": "2023-03-31 07:20:01+00:00",
      "closed_at": "2023-05-03 18:46:36+00:00",
      "resolution_days": 33.47679398148148
    },
    {
      "number": 639,
      "title": "the new mmap method does not work on Windows 11 ?",
      "state": "closed",
      "comments": 19,
      "body_length": 117,
      "label_names": [
        "need more info"
      ],
      "created_at": "2023-03-30 22:26:30+00:00",
      "closed_at": "2023-03-31 01:32:34+00:00",
      "resolution_days": 0.12921296296296297
    },
    {
      "number": 637,
      "title": "Performance investigation using AMD BLIS instead of OpenBLAS on 16 core AMD Zen1",
      "state": "closed",
      "comments": 30,
      "body_length": 21689,
      "label_names": [
        "enhancement",
        "performance"
      ],
      "created_at": "2023-03-30 22:14:53+00:00",
      "closed_at": "2023-04-13 08:09:16+00:00",
      "resolution_days": 13.412766203703704
    },
    {
      "number": 630,
      "title": "Combine large LLM with small LLM for faster inference",
      "state": "closed",
      "comments": 44,
      "body_length": 1346,
      "label_names": [
        "question",
        "research \ud83d\udd2c",
        "stale"
      ],
      "created_at": "2023-03-30 17:54:01+00:00",
      "closed_at": "2024-04-12 01:07:17+00:00",
      "resolution_days": 378.3008796296296
    },
    {
      "number": 627,
      "title": "How to activate BLAS?",
      "state": "closed",
      "comments": 19,
      "body_length": 562,
      "label_names": [
        "need more info",
        "stale"
      ],
      "created_at": "2023-03-30 16:56:25+00:00",
      "closed_at": "2024-05-10 01:28:42+00:00",
      "resolution_days": 406.3557523148148
    },
    {
      "number": 603,
      "title": "Performance Discrepancy: gpt4all Faster than Optimized llama.cpp",
      "state": "closed",
      "comments": 67,
      "body_length": 2021,
      "label_names": [
        "performance"
      ],
      "created_at": "2023-03-29 18:46:33+00:00",
      "closed_at": "2023-04-12 15:30:22+00:00",
      "resolution_days": 13.863761574074074
    },
    {
      "number": 599,
      "title": "Support tensors with 64-bit number of elements in ggml",
      "state": "closed",
      "comments": 21,
      "body_length": 17736,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2023-03-29 16:15:55+00:00",
      "closed_at": "2023-06-16 06:53:12+00:00",
      "resolution_days": 78.60922453703704
    },
    {
      "number": 537,
      "title": "Docker Issus ''Illegal instruction''",
      "state": "closed",
      "comments": 24,
      "body_length": 1007,
      "label_names": [
        "bug",
        "hardware",
        "stale"
      ],
      "created_at": "2023-03-26 19:18:11+00:00",
      "closed_at": "2024-04-12 01:07:28+00:00",
      "resolution_days": 382.2425578703704
    },
    {
      "number": 522,
      "title": "\"Not enough context memory\" on raspberry pi",
      "state": "closed",
      "comments": 13,
      "body_length": 31193,
      "label_names": [],
      "created_at": "2023-03-26 12:10:06+00:00",
      "closed_at": "2023-03-26 16:26:45+00:00",
      "resolution_days": 0.17822916666666666
    },
    {
      "number": 507,
      "title": "Comparison of Windows Build VS Unix Build (through WSL2)",
      "state": "closed",
      "comments": 24,
      "body_length": 6870,
      "label_names": [
        "question",
        "build",
        "stale"
      ],
      "created_at": "2023-03-25 20:09:51+00:00",
      "closed_at": "2024-04-12 01:07:40+00:00",
      "resolution_days": 383.20681712962966
    },
    {
      "number": 493,
      "title": "Docker error:  Cannot access '/models//7B/ggml-model-f16.bin*': No such file or directory",
      "state": "closed",
      "comments": 15,
      "body_length": 1821,
      "label_names": [
        "bug",
        "build"
      ],
      "created_at": "2023-03-25 12:11:35+00:00",
      "closed_at": "2023-04-16 09:32:00+00:00",
      "resolution_days": 21.88917824074074
    },
    {
      "number": 466,
      "title": "How do we finetune the model with new data?",
      "state": "closed",
      "comments": 17,
      "body_length": 408,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2023-03-24 16:12:02+00:00",
      "closed_at": "2024-04-10 01:07:59+00:00",
      "resolution_days": 382.3721875
    },
    {
      "number": 456,
      "title": "2-bit integer quantization ",
      "state": "closed",
      "comments": 16,
      "body_length": 996,
      "label_names": [
        "enhancement",
        "research \ud83d\udd2c"
      ],
      "created_at": "2023-03-24 06:55:44+00:00",
      "closed_at": "2023-06-24 19:17:24+00:00",
      "resolution_days": 92.51504629629629
    },
    {
      "number": 442,
      "title": "Converting alpaca-native-GPTQ models into ggml models",
      "state": "closed",
      "comments": 21,
      "body_length": 1137,
      "label_names": [
        "enhancement",
        "model"
      ],
      "created_at": "2023-03-23 22:02:03+00:00",
      "closed_at": "2023-07-28 19:39:46+00:00",
      "resolution_days": 126.90119212962964
    },
    {
      "number": 412,
      "title": "Add Shared Library Build Target",
      "state": "closed",
      "comments": 12,
      "body_length": 222,
      "label_names": [
        "bug",
        "enhancement",
        "build"
      ],
      "created_at": "2023-03-22 22:45:26+00:00",
      "closed_at": "2023-03-23 20:16:51+00:00",
      "resolution_days": 0.8968171296296297
    },
    {
      "number": 410,
      "title": "Download ggml-alpaca-7b-q4.bin failed CHECKSUM",
      "state": "closed",
      "comments": 15,
      "body_length": 148,
      "label_names": [
        "model"
      ],
      "created_at": "2023-03-22 21:31:37+00:00",
      "closed_at": "2023-03-23 09:22:24+00:00",
      "resolution_days": 0.493599537037037
    },
    {
      "number": 402,
      "title": "illegal instructions error on Android",
      "state": "closed",
      "comments": 29,
      "body_length": 870,
      "label_names": [
        "need more info",
        "android"
      ],
      "created_at": "2023-03-22 17:33:25+00:00",
      "closed_at": "2023-07-28 19:38:12+00:00",
      "resolution_days": 128.0866550925926
    },
    {
      "number": 397,
      "title": "Investigate alternative approach for Q4 quantization ",
      "state": "closed",
      "comments": 58,
      "body_length": 1851,
      "label_names": [
        "help wanted",
        "good first issue",
        "research \ud83d\udd2c"
      ],
      "created_at": "2023-03-22 16:03:20+00:00",
      "closed_at": "2023-04-25 17:20:48+00:00",
      "resolution_days": 34.0537962962963
    },
    {
      "number": 382,
      "title": "Add proper instructions for using Alpaca models",
      "state": "closed",
      "comments": 22,
      "body_length": 931,
      "label_names": [
        "documentation",
        "help wanted",
        "good first issue",
        "high priority",
        "\ud83e\udd99."
      ],
      "created_at": "2023-03-22 07:26:07+00:00",
      "closed_at": "2023-07-28 19:20:56+00:00",
      "resolution_days": 128.49640046296295
    },
    {
      "number": 374,
      "title": "SHA256 checksums correctness",
      "state": "closed",
      "comments": 12,
      "body_length": 996,
      "label_names": [
        "bug",
        "model"
      ],
      "created_at": "2023-03-21 23:05:19+00:00",
      "closed_at": "2023-03-23 17:51:06+00:00",
      "resolution_days": 1.7817939814814814
    },
    {
      "number": 361,
      "title": "Invalid model error : too old, regenerate your model files!",
      "state": "closed",
      "comments": 14,
      "body_length": 851,
      "label_names": [
        "documentation",
        "model"
      ],
      "created_at": "2023-03-21 16:51:17+00:00",
      "closed_at": "2023-03-22 05:54:53+00:00",
      "resolution_days": 0.5441666666666667
    },
    {
      "number": 353,
      "title": "Improve the Chat Mode with some tricks and considerations",
      "state": "closed",
      "comments": 13,
      "body_length": 745,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2023-03-21 13:44:45+00:00",
      "closed_at": "2024-04-10 01:08:05+00:00",
      "resolution_days": 385.474537037037
    },
    {
      "number": 331,
      "title": "Improving the repetition penalty",
      "state": "closed",
      "comments": 12,
      "body_length": 842,
      "label_names": [
        "enhancement",
        "generation quality"
      ],
      "created_at": "2023-03-20 15:43:12+00:00",
      "closed_at": "2023-09-14 13:23:49+00:00",
      "resolution_days": 177.90320601851852
    },
    {
      "number": 324,
      "title": "Breaking change of models since PR #252",
      "state": "closed",
      "comments": 24,
      "body_length": 611,
      "label_names": [
        "bug",
        "model"
      ],
      "created_at": "2023-03-20 12:48:57+00:00",
      "closed_at": "2023-05-09 21:01:11+00:00",
      "resolution_days": 50.341828703703705
    },
    {
      "number": 317,
      "title": "segmentation fault Alpaca",
      "state": "closed",
      "comments": 35,
      "body_length": 4085,
      "label_names": [
        "hardware"
      ],
      "created_at": "2023-03-20 09:56:07+00:00",
      "closed_at": "2023-04-17 07:12:17+00:00",
      "resolution_days": 27.886226851851852
    },
    {
      "number": 309,
      "title": "Is the --ignore-eos flag redundant?",
      "state": "closed",
      "comments": 12,
      "body_length": 346,
      "label_names": [
        "enhancement",
        "question"
      ],
      "created_at": "2023-03-19 23:02:33+00:00",
      "closed_at": "2023-03-20 18:50:19+00:00",
      "resolution_days": 0.824837962962963
    },
    {
      "number": 302,
      "title": "Improve Alpaca integration to match it's trained prompt syntax",
      "state": "closed",
      "comments": 12,
      "body_length": 1739,
      "label_names": [
        "enhancement",
        "help wanted",
        "high priority"
      ],
      "created_at": "2023-03-19 19:17:47+00:00",
      "closed_at": "2023-07-28 19:35:22+00:00",
      "resolution_days": 131.01221064814814
    },
    {
      "number": 247,
      "title": "How to use ggml for Flan-T5",
      "state": "closed",
      "comments": 36,
      "body_length": 440,
      "label_names": [
        "enhancement",
        "model",
        "generation quality",
        "stale"
      ],
      "created_at": "2023-03-17 22:38:08+00:00",
      "closed_at": "2024-04-14 01:06:18+00:00",
      "resolution_days": 393.1028935185185
    },
    {
      "number": 196,
      "title": "Error: inlining failed in call to \u2018always_inline\u2019 \u2018_mm256_cvtph_ps\u2019 on x86_64 - better support for different x86_64 CPU instruction extensions",
      "state": "closed",
      "comments": 35,
      "body_length": 539,
      "label_names": [
        "bug",
        "performance",
        "hardware",
        "build"
      ],
      "created_at": "2023-03-16 04:17:08+00:00",
      "closed_at": "2023-03-30 08:31:50+00:00",
      "resolution_days": 14.176875
    },
    {
      "number": 194,
      "title": "Supported context window length for each model?",
      "state": "closed",
      "comments": 13,
      "body_length": 58,
      "label_names": [
        "model",
        "generation quality"
      ],
      "created_at": "2023-03-16 02:27:23+00:00",
      "closed_at": "2023-03-24 10:34:27+00:00",
      "resolution_days": 8.33824074074074
    },
    {
      "number": 188,
      "title": "How to? (install models)",
      "state": "closed",
      "comments": 15,
      "body_length": 133,
      "label_names": [
        "question",
        "\ud83e\udd99."
      ],
      "created_at": "2023-03-15 22:51:14+00:00",
      "closed_at": "2023-03-16 11:31:34+00:00",
      "resolution_days": 0.5280092592592592
    },
    {
      "number": 173,
      "title": "Use RMSNorm",
      "state": "closed",
      "comments": 18,
      "body_length": 353,
      "label_names": [
        "bug",
        "help wanted",
        "good first issue",
        "high priority"
      ],
      "created_at": "2023-03-15 19:05:29+00:00",
      "closed_at": "2023-03-19 15:31:53+00:00",
      "resolution_days": 3.8516666666666666
    },
    {
      "number": 172,
      "title": "Attempting to merge with alpaca-lora and its quantization",
      "state": "closed",
      "comments": 19,
      "body_length": 1397,
      "label_names": [
        "enhancement",
        "help wanted"
      ],
      "created_at": "2023-03-15 18:16:30+00:00",
      "closed_at": "2023-07-28 19:32:24+00:00",
      "resolution_days": 135.05270833333333
    },
    {
      "number": 167,
      "title": "Differences with the llama tokenizer",
      "state": "closed",
      "comments": 19,
      "body_length": 959,
      "label_names": [
        "bug"
      ],
      "created_at": "2023-03-15 16:45:04+00:00",
      "closed_at": "2023-03-20 15:21:55+00:00",
      "resolution_days": 4.942256944444445
    },
    {
      "number": 156,
      "title": "Crafting prompts to get LLaMA models to generate interesting content",
      "state": "closed",
      "comments": 18,
      "body_length": 2818,
      "label_names": [
        "good first issue",
        "model"
      ],
      "created_at": "2023-03-15 07:14:54+00:00",
      "closed_at": "2023-03-15 19:30:32+00:00",
      "resolution_days": 0.5108564814814814
    },
    {
      "number": 129,
      "title": "Quantitative measurement of model perplexity for different models and model quantization modes ",
      "state": "closed",
      "comments": 53,
      "body_length": 1794,
      "label_names": [
        "model",
        "generation quality"
      ],
      "created_at": "2023-03-14 12:38:25+00:00",
      "closed_at": "2023-03-22 22:41:53+00:00",
      "resolution_days": 8.419074074074073
    },
    {
      "number": 124,
      "title": "android port of llama.cpp",
      "state": "closed",
      "comments": 13,
      "body_length": 64,
      "label_names": [
        "build"
      ],
      "created_at": "2023-03-14 08:16:53+00:00",
      "closed_at": "2023-07-28 19:31:07+00:00",
      "resolution_days": 136.4682175925926
    },
    {
      "number": 107,
      "title": "Error: inlining failed in call to always_inline \u2018_mm256_cvtph_ps\u2019: target specific option mismatch",
      "state": "closed",
      "comments": 22,
      "body_length": 9927,
      "label_names": [
        "duplicate",
        "good first issue",
        "hardware",
        "build"
      ],
      "created_at": "2023-03-13 23:20:27+00:00",
      "closed_at": "2023-03-14 18:08:16+00:00",
      "resolution_days": 0.7832060185185186
    },
    {
      "number": 105,
      "title": "Create a logo",
      "state": "closed",
      "comments": 47,
      "body_length": 82,
      "label_names": [
        "good first issue",
        "\ud83e\udd99."
      ],
      "created_at": "2023-03-13 21:15:21+00:00",
      "closed_at": "2023-07-28 19:20:49+00:00",
      "resolution_days": 136.92046296296297
    },
    {
      "number": 103,
      "title": "How to build on windows?",
      "state": "closed",
      "comments": 22,
      "body_length": 85,
      "label_names": [
        "documentation",
        "good first issue",
        "windows"
      ],
      "created_at": "2023-03-13 20:13:14+00:00",
      "closed_at": "2023-07-28 19:20:41+00:00",
      "resolution_days": 136.96350694444445
    },
    {
      "number": 97,
      "title": "WebAssembly and emscripten headers",
      "state": "closed",
      "comments": 28,
      "body_length": 2719,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2023-03-13 17:27:58+00:00",
      "closed_at": "2024-04-10 01:08:10+00:00",
      "resolution_days": 393.31958333333336
    },
    {
      "number": 91,
      "title": "Should use `mmap` for model loading",
      "state": "closed",
      "comments": 59,
      "body_length": 123,
      "label_names": [
        "enhancement",
        "good first issue"
      ],
      "created_at": "2023-03-13 11:51:47+00:00",
      "closed_at": "2023-03-30 19:28:28+00:00",
      "resolution_days": 17.317141203703702
    },
    {
      "number": 82,
      "title": "python bindings?",
      "state": "closed",
      "comments": 19,
      "body_length": 0,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2023-03-13 07:00:42+00:00",
      "closed_at": "2023-07-28 19:29:21+00:00",
      "resolution_days": 137.51989583333332
    },
    {
      "number": 71,
      "title": "Longer and infinite output",
      "state": "closed",
      "comments": 59,
      "body_length": 521,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2023-03-13 00:29:55+00:00",
      "closed_at": "2023-07-28 19:29:06+00:00",
      "resolution_days": 137.79109953703704
    },
    {
      "number": 69,
      "title": "65B model giving incorect output",
      "state": "closed",
      "comments": 18,
      "body_length": 3665,
      "label_names": [
        "need more info"
      ],
      "created_at": "2023-03-13 00:14:33+00:00",
      "closed_at": "2023-03-16 11:55:55+00:00",
      "resolution_days": 3.487060185185185
    },
    {
      "number": 58,
      "title": "Raspberry Pi 4 4GB",
      "state": "closed",
      "comments": 45,
      "body_length": 2386,
      "label_names": [
        "bug",
        "build"
      ],
      "created_at": "2023-03-12 18:33:40+00:00",
      "closed_at": "2023-07-28 19:28:37+00:00",
      "resolution_days": 138.03815972222222
    },
    {
      "number": 57,
      "title": "Stop keywords",
      "state": "closed",
      "comments": 19,
      "body_length": 324,
      "label_names": [
        "enhancement",
        "good first issue"
      ],
      "created_at": "2023-03-12 18:31:27+00:00",
      "closed_at": "2023-06-12 14:32:13+00:00",
      "resolution_days": 91.83386574074073
    },
    {
      "number": 53,
      "title": "Improving quality with 8bit?",
      "state": "closed",
      "comments": 17,
      "body_length": 471,
      "label_names": [
        "enhancement"
      ],
      "created_at": "2023-03-12 17:06:45+00:00",
      "closed_at": "2023-04-11 12:23:28+00:00",
      "resolution_days": 29.80327546296296
    },
    {
      "number": 52,
      "title": "Segmentation Fault Error \"not enough space in the context's memory pool\"",
      "state": "closed",
      "comments": 22,
      "body_length": 3827,
      "label_names": [
        "bug",
        "need more info",
        "stale"
      ],
      "created_at": "2023-03-12 16:05:03+00:00",
      "closed_at": "2024-04-09 01:10:23+00:00",
      "resolution_days": 393.3787037037037
    },
    {
      "number": 39,
      "title": "Hows the inference speed and mem usage?",
      "state": "closed",
      "comments": 14,
      "body_length": 39,
      "label_names": [
        "performance"
      ],
      "created_at": "2023-03-12 06:39:50+00:00",
      "closed_at": "2023-03-18 21:03:27+00:00",
      "resolution_days": 6.599733796296296
    },
    {
      "number": 34,
      "title": "benchmarks?",
      "state": "closed",
      "comments": 57,
      "body_length": 66,
      "label_names": [
        "documentation",
        "question",
        "stale"
      ],
      "created_at": "2023-03-12 05:20:58+00:00",
      "closed_at": "2024-04-09 01:10:24+00:00",
      "resolution_days": 393.8259953703704
    },
    {
      "number": 29,
      "title": "ggml_new_tensor_impl: not enough space in the context's memory pool",
      "state": "closed",
      "comments": 16,
      "body_length": 709,
      "label_names": [
        "wontfix"
      ],
      "created_at": "2023-03-12 01:51:07+00:00",
      "closed_at": "2023-03-13 17:23:15+00:00",
      "resolution_days": 1.647314814814815
    },
    {
      "number": 23,
      "title": "Ability for `./main` to keep the model in memory and pass it more text",
      "state": "closed",
      "comments": 39,
      "body_length": 585,
      "label_names": [
        "enhancement",
        "stale"
      ],
      "created_at": "2023-03-11 21:00:25+00:00",
      "closed_at": "2024-04-09 01:10:26+00:00",
      "resolution_days": 394.1736226851852
    },
    {
      "number": 22,
      "title": "Windows 64-bit, Microsoft Visual Studio - it works like a charm after those fixes!",
      "state": "closed",
      "comments": 40,
      "body_length": 1300,
      "label_names": [
        "enhancement",
        "help wanted",
        "good first issue",
        "windows"
      ],
      "created_at": "2023-03-11 20:44:33+00:00",
      "closed_at": "2023-04-16 10:25:54+00:00",
      "resolution_days": 35.57038194444444
    },
    {
      "number": 18,
      "title": "faster performance on older machines",
      "state": "closed",
      "comments": 20,
      "body_length": 475,
      "label_names": [
        "question"
      ],
      "created_at": "2023-03-11 17:46:20+00:00",
      "closed_at": "2023-04-16 10:21:56+00:00",
      "resolution_days": 35.69138888888889
    },
    {
      "number": 13,
      "title": "[Q] Memory Requirements for Different Model Sizes",
      "state": "closed",
      "comments": 18,
      "body_length": 0,
      "label_names": [
        "documentation",
        "question"
      ],
      "created_at": "2023-03-11 12:19:07+00:00",
      "closed_at": "2023-03-18 21:02:00+00:00",
      "resolution_days": 7.363113425925926
    },
    {
      "number": 11,
      "title": "Unicode support",
      "state": "closed",
      "comments": 38,
      "body_length": 313,
      "label_names": [
        "bug",
        "help wanted"
      ],
      "created_at": "2023-03-11 11:08:07+00:00",
      "closed_at": "2023-03-13 16:24:21+00:00",
      "resolution_days": 2.2196064814814815
    },
    {
      "number": 9,
      "title": "GPTQ Quantization (3-bit and 4-bit)",
      "state": "closed",
      "comments": 49,
      "body_length": 1069,
      "label_names": [
        "enhancement",
        "help wanted"
      ],
      "created_at": "2023-03-11 10:00:01+00:00",
      "closed_at": "2023-07-28 19:25:46+00:00",
      "resolution_days": 139.39288194444444
    }
  ]
}