{
  "issue_type": "bug",
  "extracted_from": "/root/yunwei37/vllm-exp/bug-study/analysis_llm/sample_results/vllm/label_based/bug/issues.json",
  "issue": {
    "number": 7878,
    "title": "[Bug]:  Requests larger than 75k input tokens cause `Input prompt (512 tokens) is too long and exceeds the capacity of block_manager` error",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Amazon Linux 2 (x86_64)\r\nGCC version: (GCC) 7.3.1 20180712 (Red Hat 7.3.1-17)\r\nClang version: Could not collect\r\nCMake version: version 3.27.7\r\nLibc version: glibc-2.26\r\n\r\nPython version: 3.10.9 | packaged by conda-forge | (main, Feb  2 2023, 20:20:04) [GCC 11.3.0] (64-bit runtime)\r\nPython platform: Linux-5.10.220-209.869.amzn2.x86_64-x86_64-with-glibc2.26\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA L4\r\nNvidia driver version: 550.90.07\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              8\r\nOn-line CPU(s) list: 0-7\r\nThread(s) per core:  2\r\nCore(s) per socket:  4\r\nSocket(s):           1\r\nNUMA node(s):        1\r\nVendor ID:           AuthenticAMD\r\nCPU family:          25\r\nModel:               1\r\nModel name:          AMD EPYC 7R13 Processor\r\nStepping:            1\r\nCPU MHz:             3364.353\r\nBogoMIPS:            5299.99\r\nHypervisor vendor:   KVM\r\nVirtualization type: full\r\nL1d cache:           32K\r\nL1i cache:           32K\r\nL2 cache:            512K\r\nL3 cache:            16384K\r\nNUMA node0 CPU(s):   0-7\r\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext invpcid_single ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save vaes vpclmulqdq rdpid\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==1.9.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.2\r\n[pip3] nvidia-nccl-cu11==2.14.3\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] triton==3.0.0\r\n[pip3] vllm-nccl-cu12==2.18.1.0.4.0\r\n[conda] numpy                     1.26.2                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu11          2.14.3                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] torch                     2.4.0                    pypi_0    pypi\r\n[conda] torchvision               0.19.0                   pypi_0    pypi\r\n[conda] triton                    3.0.0                    pypi_0    pypi\r\n[conda] vllm-nccl-cu12            2.18.1.0.4.0             pypi_0    pypiROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.5\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \t0-7\t0\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n</details>\r\n\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nI have a server up and running using this\r\n```\r\nvllm serve Mistral-Nemo-Instruct-2407/ --port 8006 --gpu-memory-utilization 0.9 --max-model-len 128000 --tensor-parallel-size 1 --pipeline-parallel-size 2 --quantization fp8 --uvicorn-log-level debug \r\n```\r\n\r\non two separate `NVidia GPUs`.\r\nHowever recently I have started noticing this error that I do not recall seeing before.\r\nI am using `documents` that are up to `125k tokens` in size.\r\n\r\n```\r\nInput prompt (512 tokens) is too long and exceeds the capacity of block_manager\r\n```\r\n\r\ni have tried looking around the issues list and going through what I think would be the solutions. I have tried the v2 block manager. setting `max_num_batched_tokens` to all possible values (outrageously even the context window of the model) but I keep seeing that error (replace number of tokens with the tokens set at `max_num_batched_tokens`).\r\n\r\nI have also tried `enabling/disabling chunked prefill` and that didn't help either. I am not sure what's more left to do and looking for help around this problem.\r\n\r\n<details>\r\n<summary>The output of vllm once serve command is executed`</summary>\r\n\r\n```text\r\n\r\nINFO 08-26 19:18:49 api_server.py:440] vLLM API server version 0.5.5\r\nINFO 08-26 19:18:49 api_server.py:441] args: Namespace(model_tag='Mistral-Nemo-Instruct-2407/', host='langmodel2', port=8006, uvicorn_log_level='debug', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, model='Mistral-Nemo-Instruct-2407/', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=128000, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=2, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization='fp8', rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, engine_use_ray=False, disable_log_requests=False, max_log_len=None, dispatch_function=<function serve at 0x7fe663255ea0>)\r\nINFO 08-26 19:18:49 api_server.py:144] Multiprocessing frontend to use ipc:///tmp/db61d9b2-4054-4ebb-92a8-3a5b1d8a81ed for RPC Path.\r\nINFO 08-26 19:18:49 api_server.py:161] Started engine process with PID 11035\r\nINFO 08-26 19:18:54 config.py:813] Defaulting to use ray for distributed inference\r\nWARNING 08-26 19:18:54 arg_utils.py:839] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\r\nINFO 08-26 19:18:54 config.py:911] Chunked prefill is enabled with max_num_batched_tokens=512.\r\n2024-08-26 19:18:54,825\tINFO worker.py:1603 -- Connecting to existing Ray cluster at address: 10.0.4.226:6379...\r\n2024-08-26 19:18:54,830\tINFO worker.py:1779 -- Connected to Ray cluster. View the dashboard at 127.0.0.1:8265\r\nINFO 08-26 19:18:54 llm_engine.py:184] Initializing an LLM engine (v0.5.5) with config: model='Mistral-Nemo-Instruct-2407/', speculative_config=None, tokenizer='Mistral-Nemo-Instruct-2407/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=2, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Mistral-Nemo-Instruct-2407/, use_v2_block_manager=False, enable_prefix_caching=False)\r\nINFO 08-26 19:18:55 ray_gpu_executor.py:133] use_ray_spmd_worker: False\r\nINFO 08-26 19:19:04 utils.py:975] Found nccl from library libnccl.so.2\r\nINFO 08-26 19:19:04 pynccl.py:63] vLLM is using nccl==2.20.5\r\n(RayWorkerWrapper pid=2169, ip=10.0.4.135) INFO 08-26 19:19:04 utils.py:975] Found nccl from library libnccl.so.2\r\n(RayWorkerWrapper pid=2169, ip=10.0.4.135) INFO 08-26 19:19:04 pynccl.py:63] vLLM is using nccl==2.20.5\r\nINFO 08-26 19:19:04 model_runner.py:879] Starting to load model Mistral-Nemo-Instruct-2407/...\r\nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\r\n(RayWorkerWrapper pid=2169, ip=10.0.4.135) INFO 08-26 19:19:04 model_runner.py:879] Starting to load model Mistral-Nemo-Instruct-2407/...\r\nLoading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:01,  2.15it/s]\r\nLoading safetensors checkpoint shards:  40% Completed | 2/5 [00:01<00:01,  1.91it/s]\r\nLoading safetensors checkpoint shards:  60% Completed | 3/5 [00:01<00:00,  3.00it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:01<00:00,  4.69it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:01<00:00,  3.62it/s]\r\n\r\nINFO 08-26 19:19:06 model_runner.py:890] Loading model weights took 6.5727 GB\r\n(RayWorkerWrapper pid=2169, ip=10.0.4.135) INFO 08-26 19:19:06 model_runner.py:890] Loading model weights took 6.5727 GB\r\nINFO 08-26 19:19:06 distributed_gpu_executor.py:56] # GPU blocks: 9650, # CPU blocks: 3276\r\nINFO 08-26 19:19:09 model_runner.py:1181] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\nINFO 08-26 19:19:09 model_runner.py:1185] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n(RayWorkerWrapper pid=2169, ip=10.0.4.135) INFO 08-26 19:19:09 model_runner.py:1181] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\n(RayWorkerWrapper pid=2169, ip=10.0.4.135) INFO 08-26 19:19:09 model_runner.py:1185] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n(RayWorkerWrapper pid=2169, ip=10.0.4.135) INFO 08-26 19:19:33 model_runner.py:1300] Graph capturing finished in 24 secs.\r\nINFO 08-26 19:19:34 model_runner.py:1300] Graph capturing finished in 26 secs.\r\nINFO 08-26 19:19:35 api_server.py:209] vLLM to use /tmp/tmp841d197_ as PROMETHEUS_MULTIPROC_DIR\r\nWARNING 08-26 19:19:35 serving_embedding.py:188] embedding_mode is False. Embedding API will not work.\r\nINFO 08-26 19:19:35 launcher.py:20] Available routes are:\r\nINFO 08-26 19:19:35 launcher.py:28] Route: /openapi.json, Methods: HEAD, GET\r\nINFO 08-26 19:19:35 launcher.py:28] Route: /docs, Methods: HEAD, GET\r\nINFO 08-26 19:19:35 launcher.py:28] Route: /docs/oauth2-redirect, Methods: HEAD, GET\r\nINFO 08-26 19:19:35 launcher.py:28] Route: /redoc, Methods: HEAD, GET\r\nINFO 08-26 19:19:35 launcher.py:28] Route: /health, Methods: GET\r\nINFO 08-26 19:19:35 launcher.py:28] Route: /tokenize, Methods: POST\r\nINFO 08-26 19:19:35 launcher.py:28] Route: /detokenize, Methods: POST\r\nINFO 08-26 19:19:35 launcher.py:28] Route: /v1/models, Methods: GET\r\nINFO 08-26 19:19:35 launcher.py:28] Route: /version, Methods: GET\r\nINFO 08-26 19:19:35 launcher.py:28] Route: /v1/chat/completions, Methods: POST\r\nINFO 08-26 19:19:35 launcher.py:28] Route: /v1/completions, Methods: POST\r\nINFO 08-26 19:19:35 launcher.py:28] Route: /v1/embeddings, Methods: POST\r\nINFO 08-26 19:19:35 launcher.py:33] Launching Uvicorn with --limit_concurrency 32765. To avoid this limit at the expense of performance run with --disable-frontend-multiprocessing\r\nINFO:     Started server process [10979]\r\nINFO:     Waiting for application startup.\r\nINFO:     Application startup complete.\r\nINFO:     Uvicorn running on http://langmodel2:8006 (Press CTRL+C to quit)\r\n```\r\n</details>\r\n\r\n\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-26T19:16:25+00:00",
    "closed_at": "2025-02-21T02:00:23+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7878/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7878"
  }
}