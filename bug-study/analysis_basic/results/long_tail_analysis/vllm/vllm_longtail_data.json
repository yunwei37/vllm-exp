{
  "framework": "vLLM",
  "analysis_date": "2025-07-29T17:16:39.340994",
  "total_issues": 9631,
  "longtail_count": 1023,
  "longtail_threshold": 11.0,
  "results": {
    "comment_stats": {
      "mean": 4.854220745509293,
      "median": 3.0,
      "std": 6.29038830590901,
      "p75": 6.0,
      "p90": 11.0,
      "p95": 16.0,
      "p99": 29.0,
      "max": "119"
    },
    "comparisons": {
      "long_tail_count": 1023,
      "regular_count": 8608,
      "avg_body_length": {
        "long_tail": 7782.396871945259,
        "regular": 6099.078415427509
      },
      "resolution_rate": {
        "long_tail": 76.93059628543499,
        "regular": 81.4707249070632
      },
      "avg_resolution_days": {
        "long_tail": 115.82583078909595,
        "regular": 78.738304618473
      },
      "avg_labels": {
        "long_tail": 1.305962854349951,
        "regular": 1.1804135687732342
      }
    },
    "label_distribution": {
      "total_labels": 28,
      "most_common": [
        [
          "bug",
          496
        ],
        [
          "stale",
          292
        ],
        [
          "feature request",
          115
        ],
        [
          "usage",
          110
        ],
        [
          "unstale",
          48
        ],
        [
          "RFC",
          47
        ],
        [
          "installation",
          39
        ],
        [
          "performance",
          35
        ],
        [
          "good first issue",
          24
        ],
        [
          "new-model",
          24
        ]
      ],
      "label_count_distribution": {
        "0": 103,
        "1": 538,
        "2": 349,
        "3": 32,
        "4": 1
      }
    },
    "top_issues": [
      {
        "number": 14452,
        "title": "[Doc]: Steps to run vLLM on your RTX5080 or 5090!",
        "comments": 119,
        "state": "open",
        "labels": "documentation",
        "created_days_ago": 143,
        "body_length": 3303,
        "url": "https://github.com/vllm-project/vllm/issues/14452",
        "resolution_days": "N/A"
      },
      {
        "number": 1441,
        "title": "Does vllm support the Mac/Metal/MPS? ",
        "comments": 110,
        "state": "closed",
        "labels": "",
        "created_days_ago": 647,
        "body_length": 143,
        "url": "https://github.com/vllm-project/vllm/issues/1441",
        "resolution_days": 1.3
      },
      {
        "number": 4194,
        "title": "[RFC]: Multi-modality Support on vLLM",
        "comments": 98,
        "state": "open",
        "labels": "RFC, multi-modality",
        "created_days_ago": 466,
        "body_length": 15303,
        "url": "https://github.com/vllm-project/vllm/issues/4194",
        "resolution_days": "N/A"
      },
      {
        "number": 12568,
        "title": "[V1] Feedback Thread",
        "comments": 92,
        "state": "open",
        "labels": "v1",
        "created_days_ago": 180,
        "body_length": 300,
        "url": "https://github.com/vllm-project/vllm/issues/12568",
        "resolution_days": "N/A"
      },
      {
        "number": 299,
        "title": "Support Multiple Models",
        "comments": 89,
        "state": "closed",
        "labels": "feature request",
        "created_days_ago": 761,
        "body_length": 181,
        "url": "https://github.com/vllm-project/vllm/issues/299",
        "resolution_days": 433.4
      },
      {
        "number": 17327,
        "title": "[Usage] Qwen3 Usage Guide",
        "comments": 88,
        "state": "open",
        "labels": "usage",
        "created_days_ago": 91,
        "body_length": 1387,
        "url": "https://github.com/vllm-project/vllm/issues/17327",
        "resolution_days": "N/A"
      },
      {
        "number": 6689,
        "title": "[Model] Meta Llama 3.1 Know Issues & FAQ",
        "comments": 85,
        "state": "closed",
        "labels": "",
        "created_days_ago": 371,
        "body_length": 1466,
        "url": "https://github.com/vllm-project/vllm/issues/6689",
        "resolution_days": 42.6
      },
      {
        "number": 3567,
        "title": "[Misc]: Throughput/Latency for guided_json with ~100% GPU cache utilization",
        "comments": 67,
        "state": "closed",
        "labels": "structured-output, misc, stale",
        "created_days_ago": 494,
        "body_length": 1276,
        "url": "https://github.com/vllm-project/vllm/issues/3567",
        "resolution_days": 420.6
      },
      {
        "number": 2747,
        "title": "ImportError: /ramyapra/vllm/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol:",
        "comments": 64,
        "state": "closed",
        "labels": "",
        "created_days_ago": 541,
        "body_length": 1201,
        "url": "https://github.com/vllm-project/vllm/issues/2747",
        "resolution_days": 49.5
      },
      {
        "number": 14696,
        "title": "[Feature]: Support gemma3 architecture",
        "comments": 56,
        "state": "closed",
        "labels": "feature request",
        "created_days_ago": 138,
        "body_length": 1563,
        "url": "https://github.com/vllm-project/vllm/issues/14696",
        "resolution_days": 0.4
      },
      {
        "number": 2021,
        "title": "ARM aarch-64 server build failed (host OS: Ubuntu22.04.3) ",
        "comments": 54,
        "state": "closed",
        "labels": "",
        "created_days_ago": 596,
        "body_length": 3604,
        "url": "https://github.com/vllm-project/vllm/issues/2021",
        "resolution_days": 286.2
      },
      {
        "number": 188,
        "title": "CUDA error: out of memory",
        "comments": 54,
        "state": "closed",
        "labels": "bug",
        "created_days_ago": 769,
        "body_length": 2667,
        "url": "https://github.com/vllm-project/vllm/issues/188",
        "resolution_days": 6.0
      },
      {
        "number": 8826,
        "title": "Llama3.2 Vision Model: Guides and Issues",
        "comments": 54,
        "state": "open",
        "labels": "stale",
        "created_days_ago": 306,
        "body_length": 856,
        "url": "https://github.com/vllm-project/vllm/issues/8826",
        "resolution_days": "N/A"
      },
      {
        "number": 11286,
        "title": "[Performance]: decoding speed on long context",
        "comments": 53,
        "state": "open",
        "labels": "performance, stale",
        "created_days_ago": 223,
        "body_length": 1785,
        "url": "https://github.com/vllm-project/vllm/issues/11286",
        "resolution_days": "N/A"
      },
      {
        "number": 2248,
        "title": "Recent vLLMs ask for too much memory: ValueError: No available memory for the cache blocks. Try incr...",
        "comments": 53,
        "state": "open",
        "labels": "bug, unstale",
        "created_days_ago": 583,
        "body_length": 5784,
        "url": "https://github.com/vllm-project/vllm/issues/2248",
        "resolution_days": "N/A"
      },
      {
        "number": 9701,
        "title": "[Installation] pip install vllm (0.6.3) will force a reinstallation of the CPU version torch and rep...",
        "comments": 50,
        "state": "closed",
        "labels": "installation, stale",
        "created_days_ago": 277,
        "body_length": 8157,
        "url": "https://github.com/vllm-project/vllm/issues/9701",
        "resolution_days": 157.4
      },
      {
        "number": 2484,
        "title": "Aborted request without reason",
        "comments": 50,
        "state": "closed",
        "labels": "",
        "created_days_ago": 558,
        "body_length": 2641,
        "url": "https://github.com/vllm-project/vllm/issues/2484",
        "resolution_days": 273.6
      },
      {
        "number": 11539,
        "title": "[Model] DeepSeek-V3 Enhancements ",
        "comments": 50,
        "state": "open",
        "labels": "performance, new-model, unstale",
        "created_days_ago": 214,
        "body_length": 1025,
        "url": "https://github.com/vllm-project/vllm/issues/11539",
        "resolution_days": "N/A"
      },
      {
        "number": 1002,
        "title": "GGUF support",
        "comments": 49,
        "state": "closed",
        "labels": "feature request",
        "created_days_ago": 688,
        "body_length": 505,
        "url": "https://github.com/vllm-project/vllm/issues/1002",
        "resolution_days": 331.0
      },
      {
        "number": 18571,
        "title": "[RFC]: Deprecating vLLM V0",
        "comments": 48,
        "state": "open",
        "labels": "RFC",
        "created_days_ago": 67,
        "body_length": 3524,
        "url": "https://github.com/vllm-project/vllm/issues/18571",
        "resolution_days": "N/A"
      }
    ],
    "author_stats": {
      "unique_authors_regular": 5041,
      "unique_authors_longtail": 812,
      "avg_issues_per_author_regular": 1.707597698869272,
      "avg_issues_per_author_longtail": 1.2598522167487685
    }
  },
  "longtail_issues": [
    {
      "number": 20478,
      "title": "[Bug]: Qwen3 Rerank \u6a21\u578b\u7684\u51c6\u786e\u7387\u5b58\u5728\u95ee\u9898",
      "state": "closed",
      "comments": 23,
      "body_length": 1536,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-07-04 08:28:27+00:00",
      "closed_at": "2025-07-09 01:23:38+00:00",
      "resolution_days": 4.704988425925926
    },
    {
      "number": 20407,
      "title": "[Bug]: TypeError: Qwen2_5_VLProcessor.__init__() got multiple values for argument 'image_processor' with transformers 4.53",
      "state": "closed",
      "comments": 12,
      "body_length": 680,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-07-03 01:52:51+00:00",
      "closed_at": "2025-07-03 05:40:15+00:00",
      "resolution_days": 0.15791666666666668
    },
    {
      "number": 19876,
      "title": "[Bug]: InternVL3 poor (random) output with 8bit quantization",
      "state": "closed",
      "comments": 15,
      "body_length": 12674,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-06-19 17:38:21+00:00",
      "closed_at": "2025-06-23 19:42:07+00:00",
      "resolution_days": 4.085949074074074
    },
    {
      "number": 19853,
      "title": "[Bug]: Unable to deploy NVFP4 quantized model",
      "state": "closed",
      "comments": 11,
      "body_length": 61386,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-06-19 10:48:35+00:00",
      "closed_at": "2025-07-08 16:14:18+00:00",
      "resolution_days": 19.22619212962963
    },
    {
      "number": 19229,
      "title": "[Feature]: Support Qwen3 Embedding & Reranker",
      "state": "closed",
      "comments": 12,
      "body_length": 659,
      "label_names": [
        "feature request"
      ],
      "created_at": "2025-06-05 17:45:29+00:00",
      "closed_at": "2025-06-11 03:07:31+00:00",
      "resolution_days": 5.390300925925926
    },
    {
      "number": 19025,
      "title": "[Bug]: Dual a6000 pros not working. Arch 120.",
      "state": "closed",
      "comments": 23,
      "body_length": 7701,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-06-02 14:32:32+00:00",
      "closed_at": "2025-06-26 17:19:26+00:00",
      "resolution_days": 24.115902777777777
    },
    {
      "number": 18995,
      "title": "[Bug]: Unable to get vLLM working with RTX 5090",
      "state": "closed",
      "comments": 16,
      "body_length": 3643,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-05-31 20:32:29+00:00",
      "closed_at": "2025-06-26 17:19:16+00:00",
      "resolution_days": 25.86582175925926
    },
    {
      "number": 18983,
      "title": "[Bug]: Failed to load the fine-tuned Qwen2.5-VL-7B-Instruct model.",
      "state": "closed",
      "comments": 15,
      "body_length": 10849,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-05-31 00:55:31+00:00",
      "closed_at": "2025-05-31 08:51:53+00:00",
      "resolution_days": 0.3308101851851852
    },
    {
      "number": 18916,
      "title": "[Bug]: VLLM Docker v0.9.0 produces Runtime Error: Cuda Error on Blackwell using Qwen0.6B",
      "state": "closed",
      "comments": 18,
      "body_length": 56075,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-05-29 16:35:08+00:00",
      "closed_at": "2025-06-26 17:17:58+00:00",
      "resolution_days": 28.02974537037037
    },
    {
      "number": 18841,
      "title": "[Bug]: vllm-openai:0.9.0 docker image raise 'CUDA error: no kernel image is available for execution on the device' for Llama4 Maverick FP8",
      "state": "closed",
      "comments": 20,
      "body_length": 63799,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-05-28 15:59:20+00:00",
      "closed_at": "2025-06-14 18:44:16+00:00",
      "resolution_days": 17.11453703703704
    },
    {
      "number": 18707,
      "title": "[Doc]: Newest documentation for engine arguments is significantly worse than v0.8.5 and prior",
      "state": "closed",
      "comments": 18,
      "body_length": 2353,
      "label_names": [
        "documentation"
      ],
      "created_at": "2025-05-26 10:52:48+00:00",
      "closed_at": "2025-07-10 15:02:42+00:00",
      "resolution_days": 45.173541666666665
    },
    {
      "number": 18553,
      "title": "[Bug]: data_parallel.py not working in multi-node case",
      "state": "closed",
      "comments": 16,
      "body_length": 14146,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-05-22 12:51:55+00:00",
      "closed_at": "2025-05-31 15:34:53+00:00",
      "resolution_days": 9.113171296296295
    },
    {
      "number": 18421,
      "title": "[Bug]: Cutlass MoE for Llama 4 FP8 broken in 0.9.0 ?",
      "state": "closed",
      "comments": 11,
      "body_length": 10543,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-05-20 15:44:59+00:00",
      "closed_at": "2025-05-21 12:30:51+00:00",
      "resolution_days": 0.8651851851851852
    },
    {
      "number": 18301,
      "title": "[Bug]: Not support device type: cpu",
      "state": "closed",
      "comments": 22,
      "body_length": 14525,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-05-17 08:21:47+00:00",
      "closed_at": "2025-05-21 08:21:18+00:00",
      "resolution_days": 3.9996643518518518
    },
    {
      "number": 18120,
      "title": "[Feature]: Qwen 3 MoE Lora adapter support.",
      "state": "closed",
      "comments": 16,
      "body_length": 728,
      "label_names": [
        "feature request"
      ],
      "created_at": "2025-05-14 06:50:30+00:00",
      "closed_at": "2025-07-17 18:32:53+00:00",
      "resolution_days": 64.4877662037037
    },
    {
      "number": 17965,
      "title": "[Bug]: TimeoutError and EngineDeadError in vLLM: RPC Call to execute_model Timed Out and EngineCore Failure",
      "state": "closed",
      "comments": 36,
      "body_length": 22100,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-05-11 16:29:17+00:00",
      "closed_at": "2025-05-23 02:22:12+00:00",
      "resolution_days": 11.411747685185185
    },
    {
      "number": 17896,
      "title": "[Bug]: qwen2.5-vl throw 500 Internal Server Error",
      "state": "closed",
      "comments": 15,
      "body_length": 806,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-05-09 09:40:16+00:00",
      "closed_at": "2025-07-01 03:15:14+00:00",
      "resolution_days": 52.73261574074074
    },
    {
      "number": 17640,
      "title": "[Feature]: provide a way to configure rope-scaling that isn't inline JSON",
      "state": "closed",
      "comments": 23,
      "body_length": 580,
      "label_names": [
        "feature request"
      ],
      "created_at": "2025-05-04 17:40:07+00:00",
      "closed_at": "2025-05-16 04:05:36+00:00",
      "resolution_days": 11.434363425925927
    },
    {
      "number": 17568,
      "title": "[Performance]: \u5355\u6b21\u8bf7\u6c42\u901f\u5ea630t/s \uff0c\u5e76\u53d1\u8bf7\u6c42\u53ea\u67091.5t/s",
      "state": "closed",
      "comments": 24,
      "body_length": 8718,
      "label_names": [
        "performance"
      ],
      "created_at": "2025-05-02 00:12:30+00:00",
      "closed_at": "2025-05-03 03:41:18+00:00",
      "resolution_days": 1.145
    },
    {
      "number": 17533,
      "title": "[Bug]: AttributeError: 'MultiprocExecutor' object has no attribute 'workers' when VLLM_USE_V1=1 on rocm platform serve deepseek-r1 671B",
      "state": "closed",
      "comments": 13,
      "body_length": 34935,
      "label_names": [
        "bug",
        "rocm"
      ],
      "created_at": "2025-05-01 12:20:58+00:00",
      "closed_at": "2025-06-16 23:18:30+00:00",
      "resolution_days": 46.45662037037037
    },
    {
      "number": 17468,
      "title": "[Bug]: `v0.8.5`: Special tokens (`<think>`, `</think>`) are split during streaming with Qwen3-FP8",
      "state": "closed",
      "comments": 16,
      "body_length": 1890,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-30 10:46:19+00:00",
      "closed_at": "2025-05-09 10:37:29+00:00",
      "resolution_days": 8.993865740740741
    },
    {
      "number": 17448,
      "title": "[Bug]: Exception: Invalid prefix encountered",
      "state": "closed",
      "comments": 18,
      "body_length": 11974,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-30 05:48:35+00:00",
      "closed_at": "2025-06-12 06:43:21+00:00",
      "resolution_days": 43.03803240740741
    },
    {
      "number": 17393,
      "title": "[Bug]: qwen3 structure output None",
      "state": "closed",
      "comments": 11,
      "body_length": 4800,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-29 14:27:45+00:00",
      "closed_at": "2025-06-12 06:15:18+00:00",
      "resolution_days": 43.65802083333333
    },
    {
      "number": 17385,
      "title": "[Bug]: v1 AsyncLLM model hangs if provided with 2 successive batches of items to process",
      "state": "closed",
      "comments": 17,
      "body_length": 5955,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-29 12:36:42+00:00",
      "closed_at": "2025-04-29 16:47:23+00:00",
      "resolution_days": 0.17408564814814814
    },
    {
      "number": 17344,
      "title": "[Bug]: Running `vllm serve Qwen2.5-VL-72B-Instruct-AWQ` results in an error when upgrading the vLLM version to 0.8.5.",
      "state": "closed",
      "comments": 15,
      "body_length": 31477,
      "label_names": [
        "bug",
        "torch.compile"
      ],
      "created_at": "2025-04-29 02:51:02+00:00",
      "closed_at": "2025-05-13 04:18:28+00:00",
      "resolution_days": 14.060717592592592
    },
    {
      "number": 17250,
      "title": "[Bug]: The same startup command works fine in version 0.8.2, but it throws an error in version 0.8.4.",
      "state": "closed",
      "comments": 12,
      "body_length": 38319,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-27 08:36:59+00:00",
      "closed_at": "2025-04-28 11:04:32+00:00",
      "resolution_days": 1.1024652777777777
    },
    {
      "number": 17236,
      "title": "[Bug]: GLM-4-32B-0414 model output is empty",
      "state": "closed",
      "comments": 12,
      "body_length": 13143,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-27 01:38:27+00:00",
      "closed_at": "2025-04-28 07:09:13+00:00",
      "resolution_days": 1.229699074074074
    },
    {
      "number": 17227,
      "title": "[Bug]: VLLM unexpectedly exited after serve due to cache hashkey",
      "state": "closed",
      "comments": 13,
      "body_length": 14050,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-26 11:58:48+00:00",
      "closed_at": "2025-04-28 03:42:50+00:00",
      "resolution_days": 1.6555787037037037
    },
    {
      "number": 17038,
      "title": "[Bug]:  Many endpoints are returning 500 Internal Server Error",
      "state": "closed",
      "comments": 11,
      "body_length": 20778,
      "label_names": [
        "bug",
        "good first issue"
      ],
      "created_at": "2025-04-23 08:17:37+00:00",
      "closed_at": "2025-05-15 00:04:37+00:00",
      "resolution_days": 21.65763888888889
    },
    {
      "number": 16901,
      "title": "[Bug]: RuntimeError on RTX 5090: \"no kernel image is available for execution on the device",
      "state": "closed",
      "comments": 28,
      "body_length": 1666,
      "label_names": [
        "installation"
      ],
      "created_at": "2025-04-21 04:26:48+00:00",
      "closed_at": "2025-06-26 17:16:10+00:00",
      "resolution_days": 66.5342824074074
    },
    {
      "number": 16875,
      "title": "[Bug]: KeyError in mm_input_cache when processing multimodal requests with Qwen2.5-VL-72B",
      "state": "closed",
      "comments": 22,
      "body_length": 22854,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-19 09:43:14+00:00",
      "closed_at": "2025-04-25 22:09:04+00:00",
      "resolution_days": 6.517939814814815
    },
    {
      "number": 16842,
      "title": "[Bug]: Calling the load_weights method of the MOE model failed",
      "state": "closed",
      "comments": 11,
      "body_length": 14683,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-18 12:35:33+00:00",
      "closed_at": "2025-05-08 02:40:58+00:00",
      "resolution_days": 19.58709490740741
    },
    {
      "number": 16826,
      "title": "[Bug]: The Transformers implementation of My Model is not compatible with vLLM.",
      "state": "closed",
      "comments": 33,
      "body_length": 14360,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-18 08:30:45+00:00",
      "closed_at": "2025-05-19 10:04:03+00:00",
      "resolution_days": 31.064791666666668
    },
    {
      "number": 16815,
      "title": "[Bug]: batch processing affects embedding accuracy",
      "state": "closed",
      "comments": 11,
      "body_length": 10842,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-18 04:54:44+00:00",
      "closed_at": "2025-04-18 16:21:44+00:00",
      "resolution_days": 0.47708333333333336
    },
    {
      "number": 16782,
      "title": "[Bug]: InternVL3-9B call is hanging",
      "state": "closed",
      "comments": 11,
      "body_length": 9865,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-17 12:39:32+00:00",
      "closed_at": "2025-04-28 03:54:06+00:00",
      "resolution_days": 10.63511574074074
    },
    {
      "number": 16777,
      "title": "[Bug]:  Could't deploy c4ai-command-a-03-2025 with VLLM docker",
      "state": "closed",
      "comments": 19,
      "body_length": 15460,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-17 10:19:05+00:00",
      "closed_at": "2025-04-18 07:01:47+00:00",
      "resolution_days": 0.8629861111111111
    },
    {
      "number": 16715,
      "title": "[Installation]: Kimi-VL-A3B failed to be deployed using vllm mirroring",
      "state": "closed",
      "comments": 28,
      "body_length": 8380,
      "label_names": [
        "installation"
      ],
      "created_at": "2025-04-16 10:11:58+00:00",
      "closed_at": "2025-04-17 12:53:32+00:00",
      "resolution_days": 1.1121990740740741
    },
    {
      "number": 16675,
      "title": "[Bug]: Mistral 3.1 Small Image inference is broken on 0.8.4",
      "state": "closed",
      "comments": 19,
      "body_length": 6114,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-15 17:29:30+00:00",
      "closed_at": "2025-04-28 02:40:07+00:00",
      "resolution_days": 12.382372685185185
    },
    {
      "number": 16570,
      "title": "[Bug]: qwen2.5-vl-72b oom in 4 A100 in 0.8.3",
      "state": "closed",
      "comments": 37,
      "body_length": 12569,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-14 06:25:47+00:00",
      "closed_at": "2025-04-16 10:29:50+00:00",
      "resolution_days": 2.1694791666666666
    },
    {
      "number": 16564,
      "title": "[Bug]: After wake up from level 2 sleep, model cannot load weights properly",
      "state": "closed",
      "comments": 16,
      "body_length": 14748,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-14 04:11:42+00:00",
      "closed_at": "2025-04-20 12:50:56+00:00",
      "resolution_days": 6.360578703703704
    },
    {
      "number": 16551,
      "title": "[Bug]: Severe OOM in 0.8.3 \uff08ok in 0.7.2\uff09",
      "state": "closed",
      "comments": 25,
      "body_length": 8604,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-13 10:14:13+00:00",
      "closed_at": "2025-04-16 10:29:50+00:00",
      "resolution_days": 3.0108449074074075
    },
    {
      "number": 16515,
      "title": "[Installation]: Dual 5090's (sm120, cu128) Issues Running vLLM",
      "state": "closed",
      "comments": 19,
      "body_length": 11946,
      "label_names": [
        "installation"
      ],
      "created_at": "2025-04-11 21:20:49+00:00",
      "closed_at": "2025-07-02 12:47:20+00:00",
      "resolution_days": 81.64341435185185
    },
    {
      "number": 16358,
      "title": "[New Model]: Kimi-VL-A3B",
      "state": "closed",
      "comments": 13,
      "body_length": 516,
      "label_names": [
        "new-model"
      ],
      "created_at": "2025-04-09 19:04:26+00:00",
      "closed_at": "2025-04-28 02:59:13+00:00",
      "resolution_days": 18.329710648148147
    },
    {
      "number": 16245,
      "title": "[Bug]: OPEA/Mistral-Small-3.1-24B-Instruct-2503-int4-AutoRound-awq-sym, VLLM Chat error :- can only concatenate str (not \"list\") to str",
      "state": "closed",
      "comments": 13,
      "body_length": 3360,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-08 07:56:22+00:00",
      "closed_at": "2025-04-09 04:44:07+00:00",
      "resolution_days": 0.8664930555555556
    },
    {
      "number": 16227,
      "title": "[Feature]: Llama4 Pretrained Model support",
      "state": "closed",
      "comments": 14,
      "body_length": 1088,
      "label_names": [
        "feature request"
      ],
      "created_at": "2025-04-08 02:56:02+00:00",
      "closed_at": "2025-04-11 20:15:11+00:00",
      "resolution_days": 3.7216319444444443
    },
    {
      "number": 16220,
      "title": "[RFC]: Changes to support attention + quant fusion",
      "state": "closed",
      "comments": 14,
      "body_length": 3800,
      "label_names": [
        "RFC"
      ],
      "created_at": "2025-04-07 23:18:42+00:00",
      "closed_at": "2025-06-30 22:03:35+00:00",
      "resolution_days": 83.94783564814814
    },
    {
      "number": 16197,
      "title": "[Bug]: Not able to deploy Llama-4-Scout-17B-16E-Instruct on vllm-openai v0.8.3",
      "state": "closed",
      "comments": 17,
      "body_length": 17460,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-07 16:30:31+00:00",
      "closed_at": "2025-05-13 05:50:39+00:00",
      "resolution_days": 35.55564814814815
    },
    {
      "number": 16185,
      "title": "[Bug]: Huge memory overhead with V1 (multiprocessing) when handling several multimodal inputs",
      "state": "closed",
      "comments": 20,
      "body_length": 9669,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-07 12:22:54+00:00",
      "closed_at": "2025-04-17 02:28:33+00:00",
      "resolution_days": 9.587256944444444
    },
    {
      "number": 16136,
      "title": "[Bug]: llama 4 scout instruct does not support torch.compile",
      "state": "closed",
      "comments": 18,
      "body_length": 1802,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-06 19:00:18+00:00",
      "closed_at": "2025-05-11 04:02:57+00:00",
      "resolution_days": 34.37684027777778
    },
    {
      "number": 16128,
      "title": "[Bug]: Can't serve mistral3.1-AWQ on 24G GPU",
      "state": "closed",
      "comments": 32,
      "body_length": 1960,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-06 13:55:49+00:00",
      "closed_at": "2025-04-07 11:01:39+00:00",
      "resolution_days": 0.8790509259259259
    },
    {
      "number": 16127,
      "title": "[Bug]: Llama 4 EOFError",
      "state": "closed",
      "comments": 26,
      "body_length": 31504,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-06 13:53:01+00:00",
      "closed_at": "2025-05-13 05:50:52+00:00",
      "resolution_days": 36.66517361111111
    },
    {
      "number": 16074,
      "title": "[Bug]: vLLM Server Crashes with \"Socket operation on non-socket\" Error with VLLM_USE_V1=1 and LoRA and llama3.1-8b",
      "state": "closed",
      "comments": 11,
      "body_length": 21205,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-04 21:45:46+00:00",
      "closed_at": "2025-06-12 13:41:46+00:00",
      "resolution_days": 68.66388888888889
    },
    {
      "number": 15959,
      "title": "[Bug]: Possible shape mismatch for weights of gemma3 27b bnb 4bit quant from Unsloth",
      "state": "closed",
      "comments": 14,
      "body_length": 38898,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-02 18:24:04+00:00",
      "closed_at": "2025-04-04 02:04:32+00:00",
      "resolution_days": 1.3197685185185186
    },
    {
      "number": 15941,
      "title": "[Bug]: torch.ops._C.silu_and_mul  does not exist",
      "state": "closed",
      "comments": 30,
      "body_length": 6372,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-02 10:06:59+00:00",
      "closed_at": "2025-04-04 11:37:42+00:00",
      "resolution_days": 2.0629976851851852
    },
    {
      "number": 15896,
      "title": "[Feature]: Enable CUDA Graph without turn on torch.compile / Inductor for V1",
      "state": "closed",
      "comments": 12,
      "body_length": 519,
      "label_names": [
        "feature request",
        "torch.compile"
      ],
      "created_at": "2025-04-01 17:19:26+00:00",
      "closed_at": "2025-05-29 02:16:53+00:00",
      "resolution_days": 57.37322916666667
    },
    {
      "number": 15869,
      "title": "[Performance]: Qwen2.5VL preprocessing extremely slow with large image, leading low gpu usage",
      "state": "closed",
      "comments": 15,
      "body_length": 535,
      "label_names": [
        "performance"
      ],
      "created_at": "2025-04-01 09:29:06+00:00",
      "closed_at": "2025-05-30 11:05:40+00:00",
      "resolution_days": 59.067060185185184
    },
    {
      "number": 15802,
      "title": "[Bug]: Confuse about the `computed max_num_seqs < 1` for Qwen2.5-VL-7B",
      "state": "closed",
      "comments": 16,
      "body_length": 1123,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-03-31 08:57:26+00:00",
      "closed_at": "2025-04-01 10:17:12+00:00",
      "resolution_days": 1.0553935185185186
    },
    {
      "number": 15779,
      "title": "[Bug]: Disagreement and misalignment between supported models in documentation and actual testing",
      "state": "closed",
      "comments": 26,
      "body_length": 1583,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-03-30 21:10:24+00:00",
      "closed_at": "2025-04-07 06:09:23+00:00",
      "resolution_days": 7.374293981481482
    },
    {
      "number": 15735,
      "title": "[Roadmap] vLLM Roadmap Q2 2025",
      "state": "closed",
      "comments": 19,
      "body_length": 4629,
      "label_names": [],
      "created_at": "2025-03-29 00:21:57+00:00",
      "closed_at": "2025-07-01 21:08:45+00:00",
      "resolution_days": 94.86583333333333
    },
    {
      "number": 15676,
      "title": "[Bug]:  vllm 0.8.2 serve Gemma3 error",
      "state": "closed",
      "comments": 25,
      "body_length": 11282,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-03-28 06:12:29+00:00",
      "closed_at": "2025-03-28 12:37:20+00:00",
      "resolution_days": 0.2672569444444444
    },
    {
      "number": 15664,
      "title": "[Bug]: VLLM 0.8.2 OOM error (No error in 0.7.3 version)",
      "state": "closed",
      "comments": 36,
      "body_length": 1160,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-03-28 01:57:15+00:00",
      "closed_at": "2025-04-16 10:29:49+00:00",
      "resolution_days": 19.355949074074076
    },
    {
      "number": 15622,
      "title": "[Bug]: vllm 0.8.2 have severe quality problem",
      "state": "closed",
      "comments": 20,
      "body_length": 14684,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-03-27 13:59:34+00:00",
      "closed_at": "2025-04-08 12:58:56+00:00",
      "resolution_days": 11.95789351851852
    },
    {
      "number": 15614,
      "title": "[Installation]: ValueError: size must contain 'shortest_edge' and 'longest_edge' keys.",
      "state": "closed",
      "comments": 22,
      "body_length": 14308,
      "label_names": [
        "installation"
      ],
      "created_at": "2025-03-27 10:21:12+00:00",
      "closed_at": "2025-03-28 01:51:49+00:00",
      "resolution_days": 0.646261574074074
    },
    {
      "number": 15597,
      "title": "[Bug]:vllm\u4ece0.7.0\u5f00\u59cb\u7248\u672c\u90e8\u7f72Qwen2_vl\u670d\u52a1\u5b58\u5728\u5185\u5b58(\u4e0d\u662fGPU\u663e\u5b58)\u6cc4\u6f0f\u95ee\u9898",
      "state": "closed",
      "comments": 23,
      "body_length": 644,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-03-27 04:45:50+00:00",
      "closed_at": "2025-03-27 07:13:46+00:00",
      "resolution_days": 0.10273148148148148
    },
    {
      "number": 15592,
      "title": "[Bug]:ModuleNotFoundError: No module named 'vllm._C' ",
      "state": "closed",
      "comments": 18,
      "body_length": 5508,
      "label_names": [
        "bug",
        "torch.compile"
      ],
      "created_at": "2025-03-27 02:52:34+00:00",
      "closed_at": "2025-05-28 17:19:27+00:00",
      "resolution_days": 62.60200231481482
    },
    {
      "number": 15531,
      "title": "[Installation]: install vllm with CUDA 12.8 in 5090D error",
      "state": "closed",
      "comments": 11,
      "body_length": 2080,
      "label_names": [
        "installation"
      ],
      "created_at": "2025-03-26 08:11:43+00:00",
      "closed_at": "2025-04-07 08:53:38+00:00",
      "resolution_days": 12.029108796296295
    },
    {
      "number": 15472,
      "title": "[Usage]: How to run format check locally?",
      "state": "closed",
      "comments": 13,
      "body_length": 603,
      "label_names": [
        "usage"
      ],
      "created_at": "2025-03-25 14:20:18+00:00",
      "closed_at": "2025-03-26 11:29:55+00:00",
      "resolution_days": 0.8816782407407407
    },
    {
      "number": 15469,
      "title": "[Usage]: RuntimeError: NCCL error: invalid usage (run with NCCL_DEBUG=WARN for details)",
      "state": "closed",
      "comments": 22,
      "body_length": 25015,
      "label_names": [
        "usage"
      ],
      "created_at": "2025-03-25 13:20:24+00:00",
      "closed_at": "2025-04-02 14:49:18+00:00",
      "resolution_days": 8.061736111111111
    },
    {
      "number": 15465,
      "title": "[Feature]: Embedding API dimensions  is currently not supported.",
      "state": "closed",
      "comments": 21,
      "body_length": 1184,
      "label_names": [
        "help wanted",
        "good first issue",
        "feature request"
      ],
      "created_at": "2025-03-25 12:14:17+00:00",
      "closed_at": "2025-04-12 06:23:11+00:00",
      "resolution_days": 17.756180555555556
    },
    {
      "number": 15411,
      "title": "[Bug]: Uncaught exception | <class 'ValueError'>; Qwen2_5_VLModel has no vLLM implementation and the Transformers implementation is not compatible with vLLM",
      "state": "closed",
      "comments": 15,
      "body_length": 5200,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-03-24 18:21:38+00:00",
      "closed_at": "2025-03-25 08:24:28+00:00",
      "resolution_days": 0.585300925925926
    },
    {
      "number": 15241,
      "title": "[Bug]: Temperature is ignored in vLLM 0.8.0/0.8.1",
      "state": "closed",
      "comments": 11,
      "body_length": 2179,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-03-20 18:30:40+00:00",
      "closed_at": "2025-03-21 11:01:03+00:00",
      "resolution_days": 0.6877662037037037
    },
    {
      "number": 15212,
      "title": "[Feature]: Mistral Small 3.1 HF support",
      "state": "closed",
      "comments": 23,
      "body_length": 562,
      "label_names": [
        "feature request"
      ],
      "created_at": "2025-03-20 11:22:38+00:00",
      "closed_at": "2025-04-02 08:45:55+00:00",
      "resolution_days": 12.891168981481481
    },
    {
      "number": 15207,
      "title": "[Bug]: msgspec.DecodeError: MessagePack data is malformed: trailing characters (byte 13)",
      "state": "closed",
      "comments": 21,
      "body_length": 632,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-03-20 10:48:12+00:00",
      "closed_at": "2025-04-26 06:00:09+00:00",
      "resolution_days": 36.79996527777778
    },
    {
      "number": 15174,
      "title": "[Misc]: missing python inside the container v0.8.1",
      "state": "closed",
      "comments": 18,
      "body_length": 1211,
      "label_names": [
        "misc"
      ],
      "created_at": "2025-03-20 01:11:27+00:00",
      "closed_at": "2025-03-24 12:53:12+00:00",
      "resolution_days": 4.487326388888889
    },
    {
      "number": 15125,
      "title": "[Usage]: Qwen2.5-VL-7B-Instruct <|vision_start|><|image_pad|><|vision_end|> always appears before the user text, even i set image after user text",
      "state": "closed",
      "comments": 11,
      "body_length": 5033,
      "label_names": [
        "usage"
      ],
      "created_at": "2025-03-19 11:43:26+00:00",
      "closed_at": "2025-03-20 03:56:29+00:00",
      "resolution_days": 0.6757291666666667
    },
    {
      "number": 15085,
      "title": "[Bug]: 0.8.0 ram leakage",
      "state": "closed",
      "comments": 18,
      "body_length": 1127,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-03-19 04:34:55+00:00",
      "closed_at": "2025-03-19 12:53:20+00:00",
      "resolution_days": 0.3461226851851852
    },
    {
      "number": 15078,
      "title": "[Bug]: new bug after loosening type check on `llava_onevision.py`",
      "state": "closed",
      "comments": 24,
      "body_length": 13050,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-03-19 03:19:23+00:00",
      "closed_at": "2025-03-20 11:24:46+00:00",
      "resolution_days": 1.3370717592592594
    },
    {
      "number": 15031,
      "title": "[Usage]: There is no module or parameter named 'language_model' in Gemma3ForCausalLM",
      "state": "closed",
      "comments": 15,
      "body_length": 13936,
      "label_names": [
        "usage"
      ],
      "created_at": "2025-03-18 11:57:26+00:00",
      "closed_at": "2025-06-28 05:44:58+00:00",
      "resolution_days": 101.74134259259259
    },
    {
      "number": 15027,
      "title": "[Feature]: Mistral-Small-3.1 supported?",
      "state": "closed",
      "comments": 43,
      "body_length": 409,
      "label_names": [
        "feature request"
      ],
      "created_at": "2025-03-18 11:07:58+00:00",
      "closed_at": "2025-03-20 04:46:07+00:00",
      "resolution_days": 1.7348263888888888
    },
    {
      "number": 15012,
      "title": "[Usage]: Transcription \"Maximum clip duration (30s) exceeded",
      "state": "closed",
      "comments": 16,
      "body_length": 8322,
      "label_names": [
        "usage"
      ],
      "created_at": "2025-03-18 08:40:00+00:00",
      "closed_at": "2025-06-17 03:34:01+00:00",
      "resolution_days": 90.78751157407407
    },
    {
      "number": 14992,
      "title": "[Bug]: ValueError: No available memory for the cache blocks on main branch after commit 46f98893",
      "state": "closed",
      "comments": 11,
      "body_length": 41495,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-03-18 02:36:10+00:00",
      "closed_at": "2025-03-22 04:26:54+00:00",
      "resolution_days": 4.076898148148148
    },
    {
      "number": 14923,
      "title": "[Feature]: Ensure benchmark serving do not import vLLM",
      "state": "closed",
      "comments": 15,
      "body_length": 1077,
      "label_names": [
        "good first issue",
        "feature request"
      ],
      "created_at": "2025-03-17 04:51:59+00:00",
      "closed_at": "2025-06-03 18:02:33+00:00",
      "resolution_days": 78.54900462962964
    },
    {
      "number": 14897,
      "title": "[Bug]: Gemma3 Offline Batch Inference: Attempted to assign XXX multimodal tokens to YYY placeholders",
      "state": "closed",
      "comments": 18,
      "body_length": 16896,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-03-16 17:39:36+00:00",
      "closed_at": "2025-03-19 06:58:23+00:00",
      "resolution_days": 2.554710648148148
    },
    {
      "number": 14817,
      "title": "[Bug]:  Gemma-3 models output empty text.",
      "state": "closed",
      "comments": 18,
      "body_length": 4816,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-03-14 11:29:33+00:00",
      "closed_at": "2025-03-14 14:22:49+00:00",
      "resolution_days": 0.12032407407407407
    },
    {
      "number": 14752,
      "title": "[Feature]: Suggestion of modifying raising ValueError when current_count > allowed_count for multimodal inputs",
      "state": "closed",
      "comments": 11,
      "body_length": 1215,
      "label_names": [
        "feature request"
      ],
      "created_at": "2025-03-13 10:28:20+00:00",
      "closed_at": "2025-03-18 05:29:36+00:00",
      "resolution_days": 4.792546296296297
    },
    {
      "number": 14734,
      "title": "[Usage]: Tool calling for gemma-3",
      "state": "closed",
      "comments": 31,
      "body_length": 1949,
      "label_names": [
        "usage"
      ],
      "created_at": "2025-03-13 07:22:44+00:00",
      "closed_at": "2025-03-19 11:06:14+00:00",
      "resolution_days": 6.155208333333333
    },
    {
      "number": 14696,
      "title": "[Feature]: Support gemma3 architecture",
      "state": "closed",
      "comments": 56,
      "body_length": 1563,
      "label_names": [
        "feature request"
      ],
      "created_at": "2025-03-12 18:21:45+00:00",
      "closed_at": "2025-03-13 03:11:13+00:00",
      "resolution_days": 0.36768518518518517
    },
    {
      "number": 14421,
      "title": "[Bug]: low quality of deepseek-vl2 when using vllm",
      "state": "closed",
      "comments": 13,
      "body_length": 1304,
      "label_names": [
        "documentation"
      ],
      "created_at": "2025-03-07 09:03:24+00:00",
      "closed_at": "2025-03-11 04:37:13+00:00",
      "resolution_days": 3.815150462962963
    },
    {
      "number": 14321,
      "title": "[New Model]: QwQ-32B",
      "state": "closed",
      "comments": 13,
      "body_length": 499,
      "label_names": [
        "new-model"
      ],
      "created_at": "2025-03-06 01:58:17+00:00",
      "closed_at": "2025-03-06 03:57:00+00:00",
      "resolution_days": 0.08244212962962963
    },
    {
      "number": 14281,
      "title": "[Usage]: How to bypass multimodal processor logic when inputs are already processed",
      "state": "closed",
      "comments": 24,
      "body_length": 5459,
      "label_names": [
        "usage"
      ],
      "created_at": "2025-03-05 11:24:33+00:00",
      "closed_at": "2025-03-07 10:59:40+00:00",
      "resolution_days": 1.9827199074074073
    },
    {
      "number": 14226,
      "title": "[Feature]: How to handle concurrent request in single instance of Qwen2-VL model.",
      "state": "closed",
      "comments": 43,
      "body_length": 621,
      "label_names": [
        "feature request"
      ],
      "created_at": "2025-03-04 18:06:27+00:00",
      "closed_at": "2025-03-05 17:16:27+00:00",
      "resolution_days": 0.9652777777777778
    },
    {
      "number": 14126,
      "title": "VLLM for Qwen 2.5 72B produces all !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! outputs, regardless of prompt given GPTQ 4 bits quantization",
      "state": "closed",
      "comments": 46,
      "body_length": 1377,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2025-03-03 09:02:14+00:00",
      "closed_at": "2025-07-13 02:15:23+00:00",
      "resolution_days": 131.7174652777778
    },
    {
      "number": 14113,
      "title": "[Bug]: DeepSeek R1 with outlines structured engine stops generation after `</think>`",
      "state": "closed",
      "comments": 13,
      "body_length": 8573,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-03-03 04:10:25+00:00",
      "closed_at": "2025-03-06 03:49:21+00:00",
      "resolution_days": 2.9853703703703705
    },
    {
      "number": 14011,
      "title": "[Bug]: Why 0 device need more memory? will it cause OOM?",
      "state": "closed",
      "comments": 11,
      "body_length": 2644,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2025-02-28 03:25:06+00:00",
      "closed_at": "2025-07-03 02:17:27+00:00",
      "resolution_days": 124.95302083333333
    },
    {
      "number": 13966,
      "title": "[Bug]: Ultravox 0.5 online mode with an audio limit greater than 1 is crashing",
      "state": "closed",
      "comments": 11,
      "body_length": 33321,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-02-27 14:38:04+00:00",
      "closed_at": "2025-02-27 16:24:04+00:00",
      "resolution_days": 0.07361111111111111
    },
    {
      "number": 13927,
      "title": "[Usage]:  Qwen2-VL-2B-Instruct Issue when passing a video URL to /chat/completions",
      "state": "closed",
      "comments": 11,
      "body_length": 2365,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2025-02-26 22:13:39+00:00",
      "closed_at": "2025-07-03 02:17:29+00:00",
      "resolution_days": 126.1693287037037
    },
    {
      "number": 13840,
      "title": "[Feature]: Add CLI Commands for Benchmarking",
      "state": "closed",
      "comments": 13,
      "body_length": 680,
      "label_names": [
        "good first issue",
        "feature request"
      ],
      "created_at": "2025-02-25 18:22:35+00:00",
      "closed_at": "2025-04-21 04:00:11+00:00",
      "resolution_days": 54.40111111111111
    },
    {
      "number": 13817,
      "title": "[Bug]: Qwen2.5-VL-3B Returning Series of !!! for Specific Image when \"dtype\" Set to \"float16\"",
      "state": "closed",
      "comments": 13,
      "body_length": 903,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-02-25 08:58:09+00:00",
      "closed_at": "2025-02-27 17:30:40+00:00",
      "resolution_days": 2.355914351851852
    },
    {
      "number": 13812,
      "title": "[Bug]: vLLM serve Deepseek-R1 on 4x8*L20 cluster failed",
      "state": "closed",
      "comments": 11,
      "body_length": 1768,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-02-25 07:48:30+00:00",
      "closed_at": "2025-02-28 11:18:32+00:00",
      "resolution_days": 3.1458564814814816
    },
    {
      "number": 13801,
      "title": "[Bug]: The accuracy of multiple cards and single card is inconsistent",
      "state": "closed",
      "comments": 13,
      "body_length": 737,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2025-02-25 03:47:34+00:00",
      "closed_at": "2025-07-18 02:28:48+00:00",
      "resolution_days": 142.94530092592592
    },
    {
      "number": 13669,
      "title": "[Usage]: vllm: error: unrecognized arguments: --lora-path",
      "state": "closed",
      "comments": 13,
      "body_length": 8196,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2025-02-21 12:45:10+00:00",
      "closed_at": "2025-06-23 02:14:59+00:00",
      "resolution_days": 121.56237268518518
    },
    {
      "number": 13655,
      "title": "[Bug]: Qwen2.5 VL Internal Server Error",
      "state": "closed",
      "comments": 22,
      "body_length": 16190,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-02-21 08:07:01+00:00",
      "closed_at": "2025-07-01 02:58:45+00:00",
      "resolution_days": 129.78592592592594
    },
    {
      "number": 13652,
      "title": "[Usage]: What normalization is applied in LLM.embed function. Can we create our own?",
      "state": "closed",
      "comments": 14,
      "body_length": 571,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2025-02-21 06:32:45+00:00",
      "closed_at": "2025-05-23 02:14:30+00:00",
      "resolution_days": 90.82065972222222
    },
    {
      "number": 13608,
      "title": "[Bug]: ImportError: /workspace/vllm-abo/vllm/_C.abi3.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb",
      "state": "closed",
      "comments": 43,
      "body_length": 4890,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-02-20 12:21:27+00:00",
      "closed_at": "2025-07-04 15:27:39+00:00",
      "resolution_days": 134.12930555555556
    },
    {
      "number": 13597,
      "title": "[Bug]: RuntimeError: No CUDA GPUs are available in transformers v4.48.0 or above when running Ray RLHF example",
      "state": "closed",
      "comments": 11,
      "body_length": 15749,
      "label_names": [
        "bug",
        "ray"
      ],
      "created_at": "2025-02-20 08:06:01+00:00",
      "closed_at": "2025-04-03 15:41:17+00:00",
      "resolution_days": 42.31615740740741
    },
    {
      "number": 13593,
      "title": "[Installation]: AttributeError: '_OpNamespace' '_C' object has no attribute 'silu_and_mul' on the CPU instance",
      "state": "closed",
      "comments": 11,
      "body_length": 10979,
      "label_names": [
        "installation"
      ],
      "created_at": "2025-02-20 07:15:40+00:00",
      "closed_at": "2025-03-19 14:37:54+00:00",
      "resolution_days": 27.307106481481483
    },
    {
      "number": 13552,
      "title": "[Bug]: cross-node tensor parallel + cudagraph issue",
      "state": "closed",
      "comments": 18,
      "body_length": 10294,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2025-02-19 11:56:33+00:00",
      "closed_at": "2025-07-04 02:18:08+00:00",
      "resolution_days": 134.59832175925925
    },
    {
      "number": 13540,
      "title": "[Feature]: support image_embeds in openai api as well",
      "state": "closed",
      "comments": 11,
      "body_length": 1049,
      "label_names": [
        "feature request"
      ],
      "created_at": "2025-02-19 10:11:07+00:00",
      "closed_at": "2025-03-10 12:36:04+00:00",
      "resolution_days": 19.100659722222222
    },
    {
      "number": 13517,
      "title": "[Usage]: Does vllm support mix deploy on GPU+CPU?",
      "state": "closed",
      "comments": 11,
      "body_length": 512,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2025-02-19 07:27:00+00:00",
      "closed_at": "2025-06-25 02:16:36+00:00",
      "resolution_days": 125.78444444444445
    },
    {
      "number": 13453,
      "title": "[Usage]: How to use pipeline parallelism in offline inference?",
      "state": "closed",
      "comments": 15,
      "body_length": 605,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2025-02-18 03:48:13+00:00",
      "closed_at": "2025-07-03 02:17:37+00:00",
      "resolution_days": 134.93708333333333
    },
    {
      "number": 13375,
      "title": "[Bug]: Deepseek resoning content is coming as null and the think content is going inside content when using vllm-openai v0.7.2 docker containers",
      "state": "closed",
      "comments": 22,
      "body_length": 9159,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2025-02-17 05:14:33+00:00",
      "closed_at": "2025-06-23 02:15:07+00:00",
      "resolution_days": 125.87539351851852
    },
    {
      "number": 13343,
      "title": "[Feature]: Application support for the InternVL2.5-78B series models.",
      "state": "closed",
      "comments": 11,
      "body_length": 1358,
      "label_names": [
        "feature request"
      ],
      "created_at": "2025-02-16 04:35:49+00:00",
      "closed_at": "2025-02-17 13:12:40+00:00",
      "resolution_days": 1.3589236111111112
    },
    {
      "number": 13337,
      "title": "[Usage]: jinja chat template for starcoder2",
      "state": "closed",
      "comments": 16,
      "body_length": 2256,
      "label_names": [
        "usage"
      ],
      "created_at": "2025-02-15 18:11:12+00:00",
      "closed_at": "2025-02-17 09:07:41+00:00",
      "resolution_days": 1.6225578703703705
    },
    {
      "number": 13332,
      "title": "[Bug]: ValueError: Model architectures ['LlamaForCausalLM'] failed to be inspected. Please check the logs for more details",
      "state": "closed",
      "comments": 11,
      "body_length": 15751,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-02-15 11:42:44+00:00",
      "closed_at": "2025-03-07 03:30:12+00:00",
      "resolution_days": 19.657962962962962
    },
    {
      "number": 13306,
      "title": "[Feature]: Support for RTX 5090 (CUDA 12.8)",
      "state": "closed",
      "comments": 21,
      "body_length": 998,
      "label_names": [
        "feature request"
      ],
      "created_at": "2025-02-14 20:41:08+00:00",
      "closed_at": "2025-06-26 17:16:55+00:00",
      "resolution_days": 131.85818287037037
    },
    {
      "number": 13294,
      "title": "[Usage]: restarting vllm --> \"WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL\"",
      "state": "closed",
      "comments": 12,
      "body_length": 4814,
      "label_names": [
        "usage"
      ],
      "created_at": "2025-02-14 14:44:42+00:00",
      "closed_at": "2025-02-25 17:54:16+00:00",
      "resolution_days": 11.13164351851852
    },
    {
      "number": 13284,
      "title": "[V1]: Unable to serve Qwen model on V1 alpha.",
      "state": "closed",
      "comments": 21,
      "body_length": 61528,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2025-02-14 10:56:13+00:00",
      "closed_at": "2025-06-25 02:16:40+00:00",
      "resolution_days": 130.6392013888889
    },
    {
      "number": 13273,
      "title": "[Usage]: tensor-parallel-size=2\uff0cThe program just kept hanging",
      "state": "closed",
      "comments": 25,
      "body_length": 7764,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2025-02-14 09:21:14+00:00",
      "closed_at": "2025-06-25 02:16:41+00:00",
      "resolution_days": 130.70517361111112
    },
    {
      "number": 13179,
      "title": "[Doc]: Clarify QLoRA (Quantized Model + LoRA) Support in Documentation",
      "state": "closed",
      "comments": 11,
      "body_length": 1439,
      "label_names": [
        "documentation"
      ],
      "created_at": "2025-02-12 23:20:11+00:00",
      "closed_at": "2025-02-17 16:25:05+00:00",
      "resolution_days": 4.711736111111111
    },
    {
      "number": 13143,
      "title": "[Usage]: Qwen2-VL keyword argument `max_pixels` is not a valid argument for this processor and will be ignored.",
      "state": "closed",
      "comments": 13,
      "body_length": 6184,
      "label_names": [
        "usage"
      ],
      "created_at": "2025-02-12 09:21:20+00:00",
      "closed_at": "2025-03-13 09:23:15+00:00",
      "resolution_days": 29.001331018518517
    },
    {
      "number": 13125,
      "title": "[Bug]: Use DeepSeek-R1-Distill-Qwen-32B, the result don't have start <think>  and can not parse reasoning_content.",
      "state": "closed",
      "comments": 33,
      "body_length": 8660,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-02-12 04:34:56+00:00",
      "closed_at": "2025-02-26 14:12:48+00:00",
      "resolution_days": 14.401296296296296
    },
    {
      "number": 13122,
      "title": "[Bug]:unsloth/DeepSeek-R1-Distill-Llama-70B-bnb-4bit",
      "state": "closed",
      "comments": 11,
      "body_length": 9793,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-02-12 01:09:00+00:00",
      "closed_at": "2025-02-28 11:49:43+00:00",
      "resolution_days": 16.44494212962963
    },
    {
      "number": 13121,
      "title": "[Usage]: The deepseek-ai/DeepSeek-R1 model is not running properly on H100x8*2 devices. Requesting assistance.",
      "state": "closed",
      "comments": 14,
      "body_length": 17543,
      "label_names": [
        "usage"
      ],
      "created_at": "2025-02-12 00:05:06+00:00",
      "closed_at": "2025-02-26 14:21:17+00:00",
      "resolution_days": 14.59457175925926
    },
    {
      "number": 13074,
      "title": "[Usage]: Context Size Limitation and CUDA OOM with DeepSeek R1 on 2 Nodes (TP8 PP2, 16 GPUs with 141GB VRAM Each)",
      "state": "closed",
      "comments": 13,
      "body_length": 15422,
      "label_names": [
        "usage"
      ],
      "created_at": "2025-02-11 04:18:34+00:00",
      "closed_at": "2025-05-03 16:45:02+00:00",
      "resolution_days": 81.51837962962964
    },
    {
      "number": 13050,
      "title": "[Feature]: Add qwen2.5-VL-7B-Instruct video support",
      "state": "closed",
      "comments": 13,
      "body_length": 499,
      "label_names": [
        "feature request",
        "stale"
      ],
      "created_at": "2025-02-10 19:05:05+00:00",
      "closed_at": "2025-07-13 02:15:35+00:00",
      "resolution_days": 152.29895833333333
    },
    {
      "number": 13038,
      "title": "[Bug]: vllm doesn't honor the specified json schema using outlines/xgrammar with qwen 2.5 vl",
      "state": "closed",
      "comments": 11,
      "body_length": 16367,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2025-02-10 12:50:34+00:00",
      "closed_at": "2025-05-21 02:26:19+00:00",
      "resolution_days": 99.56649305555555
    },
    {
      "number": 13030,
      "title": "[Usage]: ARM architecture usage",
      "state": "closed",
      "comments": 12,
      "body_length": 555,
      "label_names": [
        "usage"
      ],
      "created_at": "2025-02-10 10:34:30+00:00",
      "closed_at": "2025-02-17 10:14:27+00:00",
      "resolution_days": 6.986076388888889
    },
    {
      "number": 12997,
      "title": "[Usage]: \u5982\u4f55\u7528vllm\u53cc\u673a\u90e8\u7f72Deepseek-R1",
      "state": "closed",
      "comments": 19,
      "body_length": 8835,
      "label_names": [
        "usage"
      ],
      "created_at": "2025-02-10 02:00:33+00:00",
      "closed_at": "2025-02-26 12:17:13+00:00",
      "resolution_days": 16.42824074074074
    },
    {
      "number": 12988,
      "title": "[Bug]: Qwen2.5-VL-72B-Instruct-AWQ error with TP=2 and low throughput (~2 tokens/s) on VLLM_USE_V1=1",
      "state": "closed",
      "comments": 12,
      "body_length": 6700,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-02-09 14:54:38+00:00",
      "closed_at": "2025-02-21 15:39:58+00:00",
      "resolution_days": 12.031481481481482
    },
    {
      "number": 12973,
      "title": "[Bug]: When using the VLLM framework to load visual models, CPU memory overflow occurs while continuously processing data with images.",
      "state": "closed",
      "comments": 27,
      "body_length": 2989,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-02-09 02:48:16+00:00",
      "closed_at": "2025-03-19 06:28:50+00:00",
      "resolution_days": 38.15317129629629
    },
    {
      "number": 12965,
      "title": "[Installation]: subprocess-exited-with-error while installing vllm",
      "state": "closed",
      "comments": 12,
      "body_length": 1914,
      "label_names": [
        "installation",
        "stale"
      ],
      "created_at": "2025-02-08 17:37:44+00:00",
      "closed_at": "2025-07-07 02:15:01+00:00",
      "resolution_days": 148.35922453703705
    },
    {
      "number": 12950,
      "title": "[Usage]: How to do offline inference on multi-node with tensor-parallel and pipeline-parallel",
      "state": "closed",
      "comments": 20,
      "body_length": 10839,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2025-02-08 07:49:29+00:00",
      "closed_at": "2025-07-03 02:17:47+00:00",
      "resolution_days": 144.76965277777776
    },
    {
      "number": 12932,
      "title": "[Bug] ValueError: Model architectures ['Qwen2_5_VLForConditionalGeneration'] failed to be inspected. Please check the logs for more details.",
      "state": "closed",
      "comments": 24,
      "body_length": 15829,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-02-08 02:39:44+00:00",
      "closed_at": "2025-02-08 05:51:18+00:00",
      "resolution_days": 0.1330324074074074
    },
    {
      "number": 12929,
      "title": "[Bug]: V1 engine fails with offline batched inference code in V0 engine",
      "state": "closed",
      "comments": 12,
      "body_length": 18755,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2025-02-08 02:02:34+00:00",
      "closed_at": "2025-06-20 02:13:40+00:00",
      "resolution_days": 132.00770833333334
    },
    {
      "number": 12904,
      "title": "[Bug]: TypeError: a bytes-like object is required, not 'str'",
      "state": "closed",
      "comments": 12,
      "body_length": 11064,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-02-07 15:01:36+00:00",
      "closed_at": "2025-02-10 04:59:20+00:00",
      "resolution_days": 2.5817592592592593
    },
    {
      "number": 12900,
      "title": "[Usage]: Failure to Init Qwen2.5-VL-7B-Instruct with inflight bnb quantization",
      "state": "closed",
      "comments": 11,
      "body_length": 20081,
      "label_names": [
        "usage"
      ],
      "created_at": "2025-02-07 13:20:49+00:00",
      "closed_at": "2025-02-08 05:02:54+00:00",
      "resolution_days": 0.6542245370370371
    },
    {
      "number": 12890,
      "title": "[Performance]: When deployed DeepSeek-V3 on 8*H20(96GB), maximum model length only reaches 6500 using vllm, but with sglang can achieve 163840.",
      "state": "closed",
      "comments": 13,
      "body_length": 917,
      "label_names": [
        "performance",
        "stale"
      ],
      "created_at": "2025-02-07 11:15:57+00:00",
      "closed_at": "2025-06-24 02:14:55+00:00",
      "resolution_days": 136.6242824074074
    },
    {
      "number": 12847,
      "title": "[Bug]: `RuntimeError: Failed to infer device type` with v0.7.2",
      "state": "closed",
      "comments": 34,
      "body_length": 14766,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-02-06 19:19:08+00:00",
      "closed_at": "2025-02-09 07:00:01+00:00",
      "resolution_days": 2.486724537037037
    },
    {
      "number": 12841,
      "title": "[Bug]: Speculative decoding reports errors when loading target model using distributed inference (VLLM's offical Ray setup)",
      "state": "closed",
      "comments": 11,
      "body_length": 13445,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-02-06 16:21:01+00:00",
      "closed_at": "2025-02-19 14:13:16+00:00",
      "resolution_days": 12.911284722222222
    },
    {
      "number": 12769,
      "title": "[Bug]:  crash when receive a request",
      "state": "closed",
      "comments": 12,
      "body_length": 17567,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-02-05 07:37:27+00:00",
      "closed_at": "2025-02-25 07:46:41+00:00",
      "resolution_days": 20.006412037037038
    },
    {
      "number": 12742,
      "title": "[Usage]: Empty mm_placeholders when running Qwen2-VL-7B",
      "state": "closed",
      "comments": 31,
      "body_length": 5240,
      "label_names": [
        "usage"
      ],
      "created_at": "2025-02-04 16:32:48+00:00",
      "closed_at": "2025-02-05 19:01:43+00:00",
      "resolution_days": 1.1034143518518518
    },
    {
      "number": 12741,
      "title": "[Bug]: V1 Engine Non-Coherent output",
      "state": "closed",
      "comments": 11,
      "body_length": 1289,
      "label_names": [
        "bug",
        "v1"
      ],
      "created_at": "2025-02-04 16:29:10+00:00",
      "closed_at": "2025-02-22 16:45:38+00:00",
      "resolution_days": 18.011435185185185
    },
    {
      "number": 12705,
      "title": "[RFC]: Scale the API server across multiple CPUs",
      "state": "closed",
      "comments": 12,
      "body_length": 1024,
      "label_names": [
        "RFC"
      ],
      "created_at": "2025-02-03 19:14:29+00:00",
      "closed_at": "2025-06-11 22:52:20+00:00",
      "resolution_days": 128.15128472222221
    },
    {
      "number": 12700,
      "title": "v0.7.2 Release Tracker",
      "state": "closed",
      "comments": 12,
      "body_length": 385,
      "label_names": [
        "release"
      ],
      "created_at": "2025-02-03 17:14:45+00:00",
      "closed_at": "2025-02-06 18:55:16+00:00",
      "resolution_days": 3.069803240740741
    },
    {
      "number": 12693,
      "title": "[Usage]: V0 Does Qwen2-VL Support torch.compile in vllm?",
      "state": "closed",
      "comments": 12,
      "body_length": 1309,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2025-02-03 14:31:02+00:00",
      "closed_at": "2025-06-20 02:13:44+00:00",
      "resolution_days": 136.4879861111111
    },
    {
      "number": 12664,
      "title": "[Bug]: vllm spins gpus at 100%",
      "state": "closed",
      "comments": 11,
      "body_length": 21996,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2025-02-02 20:05:51+00:00",
      "closed_at": "2025-06-09 02:16:31+00:00",
      "resolution_days": 126.25740740740741
    },
    {
      "number": 12573,
      "title": "[Usage]: Does DeepSeek-R1 1.58-bit Dynamic Quant work on VLLM?",
      "state": "closed",
      "comments": 19,
      "body_length": 718,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2025-01-30 09:21:56+00:00",
      "closed_at": "2025-06-18 02:13:42+00:00",
      "resolution_days": 138.70261574074075
    },
    {
      "number": 12554,
      "title": "[Bug]: Engine fails to start when running Qwen2.5 Deepseek r1",
      "state": "closed",
      "comments": 11,
      "body_length": 19894,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-01-29 15:41:57+00:00",
      "closed_at": "2025-03-09 21:41:24+00:00",
      "resolution_days": 39.24961805555556
    },
    {
      "number": 12541,
      "title": "[Bug]: DeepseekR1 model load fails with weights tied error",
      "state": "closed",
      "comments": 27,
      "body_length": 13654,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2025-01-29 09:22:03+00:00",
      "closed_at": "2025-07-07 02:15:05+00:00",
      "resolution_days": 158.70349537037038
    },
    {
      "number": 12529,
      "title": "[Performance]: V1 higher memory usage",
      "state": "closed",
      "comments": 22,
      "body_length": 1466,
      "label_names": [
        "performance",
        "v1"
      ],
      "created_at": "2025-01-28 22:23:50+00:00",
      "closed_at": "2025-03-14 03:40:24+00:00",
      "resolution_days": 44.21983796296296
    },
    {
      "number": 12505,
      "title": "[Installation]: Nvidia runtime issue? On new VLLM 0.7.0",
      "state": "closed",
      "comments": 18,
      "body_length": 864,
      "label_names": [
        "installation"
      ],
      "created_at": "2025-01-28 08:59:49+00:00",
      "closed_at": "2025-02-16 09:00:35+00:00",
      "resolution_days": 19.00053240740741
    },
    {
      "number": 12468,
      "title": "[Feature] `reasoning_content` in API for reasoning models like DeepSeek R1",
      "state": "closed",
      "comments": 26,
      "body_length": 667,
      "label_names": [
        "feature request"
      ],
      "created_at": "2025-01-27 07:27:57+00:00",
      "closed_at": "2025-01-29 03:38:09+00:00",
      "resolution_days": 1.8404166666666666
    },
    {
      "number": 12442,
      "title": "[Usage]: Shape mismatch when batch requests with openai chat completion apis and qwen2-vl",
      "state": "closed",
      "comments": 13,
      "body_length": 1705,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2025-01-26 08:49:00+00:00",
      "closed_at": "2025-06-07 02:12:15+00:00",
      "resolution_days": 131.72447916666667
    },
    {
      "number": 12441,
      "title": "[Bug]: Could not run '_C::rms_norm' with arguments from the 'CUDA' backend.",
      "state": "closed",
      "comments": 15,
      "body_length": 36917,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-01-26 07:00:43+00:00",
      "closed_at": "2025-02-28 11:42:24+00:00",
      "resolution_days": 33.19561342592593
    },
    {
      "number": 12378,
      "title": "[Usage]: vLLM serving with local model",
      "state": "closed",
      "comments": 12,
      "body_length": 1417,
      "label_names": [
        "usage"
      ],
      "created_at": "2025-01-24 02:09:46+00:00",
      "closed_at": "2025-01-30 02:51:15+00:00",
      "resolution_days": 6.02880787037037
    },
    {
      "number": 12365,
      "title": "Release v0.7.0",
      "state": "closed",
      "comments": 15,
      "body_length": 525,
      "label_names": [
        "release"
      ],
      "created_at": "2025-01-23 18:17:20+00:00",
      "closed_at": "2025-02-11 18:29:43+00:00",
      "resolution_days": 19.008599537037036
    },
    {
      "number": 12256,
      "title": "[Usage]: deepseek v3 can not set tensor_parallel_size=32",
      "state": "closed",
      "comments": 18,
      "body_length": 2861,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2025-01-21 09:05:19+00:00",
      "closed_at": "2025-06-25 02:16:51+00:00",
      "resolution_days": 154.71634259259258
    },
    {
      "number": 12219,
      "title": "[Bug]: ValueError: Model architectures ['LlamaForCausalLM'] failed to be inspected. Please check the logs for more details.",
      "state": "closed",
      "comments": 15,
      "body_length": 15725,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-01-20 10:08:36+00:00",
      "closed_at": "2025-01-22 15:25:34+00:00",
      "resolution_days": 2.2201157407407406
    },
    {
      "number": 12183,
      "title": "[Bug]: Fail to use beamsearch with llm.chat",
      "state": "closed",
      "comments": 23,
      "body_length": 12634,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2025-01-18 08:27:20+00:00",
      "closed_at": "2025-05-24 02:07:50+00:00",
      "resolution_days": 125.73645833333333
    },
    {
      "number": 12174,
      "title": "[RFC]: Distribute LoRA adapters across deployment",
      "state": "closed",
      "comments": 16,
      "body_length": 9703,
      "label_names": [
        "RFC"
      ],
      "created_at": "2025-01-17 19:19:32+00:00",
      "closed_at": "2025-05-12 17:39:11+00:00",
      "resolution_days": 114.9303125
    },
    {
      "number": 12165,
      "title": "ValueError: The prompt (total length 25938) is too long to fit into the model (context length 4096). Make sure that `max_model_len` is no smaller than the number of text tokens plus multimodal tokens.",
      "state": "closed",
      "comments": 16,
      "body_length": 15101,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-01-17 13:04:53+00:00",
      "closed_at": "2025-01-18 06:21:45+00:00",
      "resolution_days": 0.7200462962962964
    },
    {
      "number": 12118,
      "title": "[Bug]: Fail to use deepseek-vl2",
      "state": "closed",
      "comments": 18,
      "body_length": 17814,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-01-16 13:56:25+00:00",
      "closed_at": "2025-01-17 06:34:50+00:00",
      "resolution_days": 0.6933449074074074
    },
    {
      "number": 12008,
      "title": "[Bug]: Nccl Test Error",
      "state": "closed",
      "comments": 21,
      "body_length": 19752,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2025-01-13 12:22:34+00:00",
      "closed_at": "2025-06-20 02:13:52+00:00",
      "resolution_days": 157.57729166666667
    },
    {
      "number": 11945,
      "title": "[RFC]: Pipeline-Parallelism for vLLM V1",
      "state": "closed",
      "comments": 19,
      "body_length": 7192,
      "label_names": [
        "RFC",
        "stale",
        "v1"
      ],
      "created_at": "2025-01-10 23:01:10+00:00",
      "closed_at": "2025-05-09 02:14:11+00:00",
      "resolution_days": 118.13403935185185
    },
    {
      "number": 11886,
      "title": "[Installation]:  Could not find a version that satisfies the requirement xgrammar>=0.1.6; platform_machine == \"x86_64\" (from vllm) (from versions: none)",
      "state": "closed",
      "comments": 38,
      "body_length": 2131,
      "label_names": [
        "installation",
        "structured-output"
      ],
      "created_at": "2025-01-09 07:25:11+00:00",
      "closed_at": "2025-03-11 15:27:39+00:00",
      "resolution_days": 61.3350462962963
    },
    {
      "number": 11865,
      "title": "[Bug]: Mistral's Pixtral error for vllm>=0.6.5 on 4 T4's",
      "state": "closed",
      "comments": 14,
      "body_length": 24836,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-01-08 21:12:29+00:00",
      "closed_at": "2025-02-03 22:02:20+00:00",
      "resolution_days": 26.034618055555555
    },
    {
      "number": 11842,
      "title": "[Bug]: \u201climit-mm-per-prompt\u201d can't be set correctly as in 0.6.5",
      "state": "closed",
      "comments": 29,
      "body_length": 3504,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-01-08 11:38:24+00:00",
      "closed_at": "2025-02-12 08:53:06+00:00",
      "resolution_days": 34.88520833333333
    },
    {
      "number": 11802,
      "title": "[Bug]:  Some weights are not initialized from checkpoints For Gemma2ForSequenceClassification",
      "state": "closed",
      "comments": 26,
      "body_length": 28499,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2025-01-07 07:45:33+00:00",
      "closed_at": "2025-07-11 02:16:53+00:00",
      "resolution_days": 184.77175925925926
    },
    {
      "number": 11781,
      "title": "[Bug]: base64 string leads to gibberish with latest vLLM server and pixtral-12b",
      "state": "closed",
      "comments": 23,
      "body_length": 7157,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-01-06 22:23:14+00:00",
      "closed_at": "2025-01-09 17:19:39+00:00",
      "resolution_days": 2.789178240740741
    },
    {
      "number": 11767,
      "title": "[Feature]: Qwen2-VL-72B-Instruct-GPTQ-Int4 model runs very slowly in A100 machine 80GB",
      "state": "closed",
      "comments": 21,
      "body_length": 240,
      "label_names": [
        "feature request",
        "stale"
      ],
      "created_at": "2025-01-06 09:54:31+00:00",
      "closed_at": "2025-05-19 02:13:27+00:00",
      "resolution_days": 132.67981481481482
    },
    {
      "number": 11715,
      "title": "[Bug]: ValueError: Model architectures ['LlamaForCausalLM'] failed to be inspected",
      "state": "closed",
      "comments": 17,
      "body_length": 20597,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2025-01-03 10:12:48+00:00",
      "closed_at": "2025-07-18 02:29:02+00:00",
      "resolution_days": 195.6779398148148
    },
    {
      "number": 11683,
      "title": "[Bug]: Error while importing vllm since v0.6.6",
      "state": "closed",
      "comments": 18,
      "body_length": 8698,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-01-02 10:58:59+00:00",
      "closed_at": "2025-01-06 06:46:43+00:00",
      "resolution_days": 3.8248148148148147
    },
    {
      "number": 11681,
      "title": "[Usage]: Trying to add codeshell 7b model, but garbled characters",
      "state": "closed",
      "comments": 18,
      "body_length": 22192,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2025-01-02 10:12:12+00:00",
      "closed_at": "2025-05-10 02:06:47+00:00",
      "resolution_days": 127.6629050925926
    },
    {
      "number": 11643,
      "title": "[Bug]: I try to use vllm==0.6.5 for GLM4-9b-chat but error \"/usr/bin/ld: cannot find -lcuda\"",
      "state": "closed",
      "comments": 11,
      "body_length": 25795,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-12-31 03:17:31+00:00",
      "closed_at": "2025-06-27 06:04:38+00:00",
      "resolution_days": 178.11605324074074
    },
    {
      "number": 11606,
      "title": "[Feature]: Confidence score for Qwen/Qwen2-VL-7B-Instruct",
      "state": "closed",
      "comments": 16,
      "body_length": 3304,
      "label_names": [
        "feature request",
        "stale"
      ],
      "created_at": "2024-12-29 18:06:59+00:00",
      "closed_at": "2025-05-01 02:13:13+00:00",
      "resolution_days": 122.33766203703703
    },
    {
      "number": 11569,
      "title": "[Usage]: Does serving the model in **manual** way differ than the **predefined** *(OpenAI)* way? A quick question, please guide",
      "state": "closed",
      "comments": 33,
      "body_length": 3798,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2024-12-27 14:04:11+00:00",
      "closed_at": "2025-05-07 02:09:55+00:00",
      "resolution_days": 130.50398148148147
    },
    {
      "number": 11568,
      "title": "[Performance]: The performance of bge-rerank model on vllm and huggingface is inconsistent",
      "state": "closed",
      "comments": 29,
      "body_length": 1396,
      "label_names": [
        "performance"
      ],
      "created_at": "2024-12-27 11:04:23+00:00",
      "closed_at": "2025-01-15 16:31:03+00:00",
      "resolution_days": 19.22685185185185
    },
    {
      "number": 11564,
      "title": "[Usage]: zmq.error.ZMQError: Operation not supported",
      "state": "closed",
      "comments": 20,
      "body_length": 13709,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2024-12-27 09:25:13+00:00",
      "closed_at": "2025-06-15 02:15:33+00:00",
      "resolution_days": 169.70162037037036
    },
    {
      "number": 11560,
      "title": "[Usage]:  torch.OutOfMemoryError: CUDA out of memory.",
      "state": "closed",
      "comments": 18,
      "body_length": 7868,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-12-27 08:57:15+00:00",
      "closed_at": "2025-03-07 13:48:29+00:00",
      "resolution_days": 70.20224537037036
    },
    {
      "number": 11526,
      "title": "[Bug]: Different sampled output when running on different GPUs",
      "state": "closed",
      "comments": 17,
      "body_length": 1513,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-12-26 15:27:55+00:00",
      "closed_at": "2025-05-02 02:08:48+00:00",
      "resolution_days": 126.44505787037038
    },
    {
      "number": 11479,
      "title": "[New Model]: QVQ-72B-Preview",
      "state": "closed",
      "comments": 11,
      "body_length": 698,
      "label_names": [
        "new-model",
        "stale"
      ],
      "created_at": "2024-12-25 04:25:56+00:00",
      "closed_at": "2025-03-28 05:27:00+00:00",
      "resolution_days": 93.04240740740741
    },
    {
      "number": 11451,
      "title": "[Usage]: Trying to add codeshell 7b model, but got an error",
      "state": "closed",
      "comments": 21,
      "body_length": 30959,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-12-24 05:56:51+00:00",
      "closed_at": "2025-01-09 04:02:05+00:00",
      "resolution_days": 15.920300925925925
    },
    {
      "number": 11449,
      "title": "[Bug]: How to run microsoft/llava-med-v1.5-mistral-7b by vllm",
      "state": "closed",
      "comments": 13,
      "body_length": 13923,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-12-24 05:39:20+00:00",
      "closed_at": "2024-12-24 08:03:18+00:00",
      "resolution_days": 0.09997685185185186
    },
    {
      "number": 11436,
      "title": "[Performance]: Prefill is not using cuda graph and become very slow when LORA enabled",
      "state": "closed",
      "comments": 19,
      "body_length": 9672,
      "label_names": [
        "performance",
        "stale"
      ],
      "created_at": "2024-12-23 15:15:55+00:00",
      "closed_at": "2025-05-22 02:11:15+00:00",
      "resolution_days": 149.4550925925926
    },
    {
      "number": 11372,
      "title": "[Feature]: Will vLLM support flash-attention 3 ?",
      "state": "closed",
      "comments": 11,
      "body_length": 577,
      "label_names": [
        "feature request"
      ],
      "created_at": "2024-12-20 11:07:43+00:00",
      "closed_at": "2025-01-25 19:42:56+00:00",
      "resolution_days": 36.35778935185185
    },
    {
      "number": 11371,
      "title": "[Bug]: Prefix caching doesn't work for LlavaOneVision",
      "state": "closed",
      "comments": 13,
      "body_length": 3764,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-12-20 10:28:19+00:00",
      "closed_at": "2025-04-26 02:38:06+00:00",
      "resolution_days": 126.67346064814815
    },
    {
      "number": 11347,
      "title": "[New Model]: answerdotai/ModernBERT-large",
      "state": "closed",
      "comments": 17,
      "body_length": 493,
      "label_names": [
        "new-model"
      ],
      "created_at": "2024-12-19 20:30:56+00:00",
      "closed_at": "2025-04-16 12:30:17+00:00",
      "resolution_days": 117.66621527777778
    },
    {
      "number": 11346,
      "title": "[Bug]: no output of profile when VLLM_TORCH_PROFILER_DIR is enabled for vllm serve",
      "state": "closed",
      "comments": 16,
      "body_length": 1034,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-12-19 19:42:12+00:00",
      "closed_at": "2025-04-23 02:08:25+00:00",
      "resolution_days": 124.26820601851851
    },
    {
      "number": 11337,
      "title": "[Usage]: How to expand inference context length to longer (such as 128k, 256k) on multi modality models.",
      "state": "closed",
      "comments": 17,
      "body_length": 1043,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-12-19 13:52:45+00:00",
      "closed_at": "2024-12-20 13:34:11+00:00",
      "resolution_days": 0.9871064814814815
    },
    {
      "number": 11320,
      "title": "[Performance]: Performance degradation due to CPU bottleneck when serving embedding models to GPUs",
      "state": "closed",
      "comments": 18,
      "body_length": 9954,
      "label_names": [
        "performance",
        "stale"
      ],
      "created_at": "2024-12-19 05:38:19+00:00",
      "closed_at": "2025-04-23 02:08:27+00:00",
      "resolution_days": 124.85425925925927
    },
    {
      "number": 11317,
      "title": "[Performance]: vllm0.6.5\u52a0\u8f7dGLM4-9B-Chat\uff0c\u52a8\u6001\u52a0\u8f7dlora\uff0c\u8f93\u5165\u957f\u6587\u672c\u65f6\u63a8\u7406\u6027\u80fd\u4e0b\u964d\u8f83\u591a",
      "state": "closed",
      "comments": 14,
      "body_length": 1751,
      "label_names": [
        "performance",
        "stale"
      ],
      "created_at": "2024-12-19 03:37:08+00:00",
      "closed_at": "2025-03-21 08:44:54+00:00",
      "resolution_days": 92.21372685185185
    },
    {
      "number": 11282,
      "title": "[Usage]: How to serve my custom model?",
      "state": "closed",
      "comments": 13,
      "body_length": 7970,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-12-18 04:26:02+00:00",
      "closed_at": "2024-12-18 06:55:53+00:00",
      "resolution_days": 0.1040625
    },
    {
      "number": 11271,
      "title": "[Bug]: AttributeError: 'CachedPreTrainedTokenizerFast' object has no attribute 'default_chat_template'. Did you mean: 'get_chat_template'?",
      "state": "closed",
      "comments": 14,
      "body_length": 1043,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-12-17 22:31:15+00:00",
      "closed_at": "2025-01-09 05:20:14+00:00",
      "resolution_days": 22.284016203703704
    },
    {
      "number": 11251,
      "title": "[Bug]: torch.OutOfMemoryError for 0.6.4.post1 but 0.6.3.post1 is working",
      "state": "closed",
      "comments": 14,
      "body_length": 45319,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-12-17 05:49:43+00:00",
      "closed_at": "2025-04-28 02:10:10+00:00",
      "resolution_days": 131.84753472222224
    },
    {
      "number": 11250,
      "title": "[Bug]: Hermes tool choice can not supprot format 'string'",
      "state": "closed",
      "comments": 12,
      "body_length": 13127,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-12-17 04:48:26+00:00",
      "closed_at": "2025-06-03 02:15:42+00:00",
      "resolution_days": 167.89393518518517
    },
    {
      "number": 11229,
      "title": "[Usage]: Cannot register custom model (Out-of-Tree Model Integration)",
      "state": "closed",
      "comments": 27,
      "body_length": 16026,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-12-16 09:58:20+00:00",
      "closed_at": "2024-12-17 13:49:46+00:00",
      "resolution_days": 1.1607175925925926
    },
    {
      "number": 11224,
      "title": "[Bug]: vLLM throws error when sampling from Cerebras GPT Models",
      "state": "closed",
      "comments": 19,
      "body_length": 2552,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-12-16 04:32:36+00:00",
      "closed_at": "2025-01-16 05:03:31+00:00",
      "resolution_days": 31.021469907407408
    },
    {
      "number": 11126,
      "title": "[Usage]: how to use EAGLE on vLLM?",
      "state": "closed",
      "comments": 29,
      "body_length": 596,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2024-12-12 08:16:44+00:00",
      "closed_at": "2025-04-20 02:10:50+00:00",
      "resolution_days": 128.7459027777778
    },
    {
      "number": 11072,
      "title": "[Bug]: vllm serve kv cache does not seem to persist properly across requests",
      "state": "closed",
      "comments": 11,
      "body_length": 7842,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-12-10 21:28:46+00:00",
      "closed_at": "2025-04-13 02:16:51+00:00",
      "resolution_days": 123.20005787037037
    },
    {
      "number": 11070,
      "title": "[Usage]: Run OpenAI api_server.py on Neuron Device Failed, Model architectures ['OPTForCausalLM'] are not supported on Neuron for now. Supported architectures: ['LlamaForCausalLM', 'MistralForCausalLM']",
      "state": "closed",
      "comments": 16,
      "body_length": 24893,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-12-10 20:27:36+00:00",
      "closed_at": "2024-12-11 14:30:20+00:00",
      "resolution_days": 0.7518981481481481
    },
    {
      "number": 11017,
      "title": "[Misc]: Has anyone tried to run Microsoft Graphrag with vllm? ",
      "state": "closed",
      "comments": 13,
      "body_length": 592,
      "label_names": [
        "misc"
      ],
      "created_at": "2024-12-09 13:11:29+00:00",
      "closed_at": "2024-12-16 13:37:04+00:00",
      "resolution_days": 7.017766203703704
    },
    {
      "number": 10994,
      "title": "[Usage]: Qwen/Qwen2-VL-7B-Instruct",
      "state": "closed",
      "comments": 16,
      "body_length": 1054,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-12-08 19:46:28+00:00",
      "closed_at": "2024-12-25 16:33:56+00:00",
      "resolution_days": 16.866296296296298
    },
    {
      "number": 10939,
      "title": "[Feature]: Add classification Task with AutoModelForSequenceClassification and BertForSequenceClassification",
      "state": "closed",
      "comments": 11,
      "body_length": 4338,
      "label_names": [
        "feature request",
        "stale"
      ],
      "created_at": "2024-12-06 02:05:50+00:00",
      "closed_at": "2025-03-07 03:28:06+00:00",
      "resolution_days": 91.05712962962963
    },
    {
      "number": 10898,
      "title": "[Performance]: Server launched with LoRA adapter but queried without is much slower than standard server",
      "state": "closed",
      "comments": 12,
      "body_length": 1409,
      "label_names": [
        "performance",
        "stale"
      ],
      "created_at": "2024-12-04 16:09:54+00:00",
      "closed_at": "2025-05-09 02:10:25+00:00",
      "resolution_days": 155.41702546296295
    },
    {
      "number": 10856,
      "title": "[Bug]: Docker deployment returns zmq.error.ZMQError: Operation not supported",
      "state": "closed",
      "comments": 24,
      "body_length": 18486,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-12-03 09:51:54+00:00",
      "closed_at": "2025-06-15 02:15:44+00:00",
      "resolution_days": 193.6832175925926
    },
    {
      "number": 10830,
      "title": "[Bug]: [single CPU][config] ValueError: not enough values to unpack (expected 2, got 1)",
      "state": "closed",
      "comments": 14,
      "body_length": 9996,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-12-02 11:39:08+00:00",
      "closed_at": "2024-12-04 03:26:06+00:00",
      "resolution_days": 1.6576157407407408
    },
    {
      "number": 10759,
      "title": "[Usage]: ValueError: Model architectures ['Qwen2ForCausalLM'] failed to be inspected. Please check the logs for more details.",
      "state": "closed",
      "comments": 17,
      "body_length": 7899,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-11-29 03:21:59+00:00",
      "closed_at": "2024-12-16 09:37:37+00:00",
      "resolution_days": 17.260856481481483
    },
    {
      "number": 10752,
      "title": "[Feature]: Enable `/score` endpoint for all embedding models",
      "state": "closed",
      "comments": 11,
      "body_length": 630,
      "label_names": [
        "feature request"
      ],
      "created_at": "2024-11-28 16:08:38+00:00",
      "closed_at": "2025-02-21 17:52:32+00:00",
      "resolution_days": 85.07215277777777
    },
    {
      "number": 10737,
      "title": "[New Model]: Qwen/QwQ-32B-Preview",
      "state": "closed",
      "comments": 27,
      "body_length": 1444,
      "label_names": [
        "new-model",
        "stale"
      ],
      "created_at": "2024-11-28 08:29:05+00:00",
      "closed_at": "2025-04-20 02:28:27+00:00",
      "resolution_days": 142.74956018518517
    },
    {
      "number": 10718,
      "title": "[Bug]: documentation say XLMRobertaForSequenceClassification is supported but logs say ['XLMRobertaForSequenceClassification'] are not supported for now",
      "state": "closed",
      "comments": 19,
      "body_length": 5607,
      "label_names": [
        "bug",
        "unstale"
      ],
      "created_at": "2024-11-27 16:28:36+00:00",
      "closed_at": "2025-04-01 15:32:28+00:00",
      "resolution_days": 124.96101851851851
    },
    {
      "number": 10706,
      "title": "[Bug]: VLLM run very very slow in ARM cpu",
      "state": "closed",
      "comments": 11,
      "body_length": 4406,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-11-27 10:33:11+00:00",
      "closed_at": "2025-04-10 02:06:23+00:00",
      "resolution_days": 133.64805555555554
    },
    {
      "number": 10689,
      "title": "[Bug]: CPU Docker build fail.",
      "state": "closed",
      "comments": 13,
      "body_length": 626,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-11-27 03:20:51+00:00",
      "closed_at": "2024-11-28 04:59:02+00:00",
      "resolution_days": 1.0681828703703704
    },
    {
      "number": 10673,
      "title": "[Usage]: Llama-2-7b-chat-hf as embedding model",
      "state": "closed",
      "comments": 11,
      "body_length": 7544,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-11-26 13:48:28+00:00",
      "closed_at": "2025-01-21 06:01:07+00:00",
      "resolution_days": 55.67545138888889
    },
    {
      "number": 10653,
      "title": "[Bug]: AMD GPU RX 7900XT: Failed to infer device type",
      "state": "closed",
      "comments": 12,
      "body_length": 12212,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-11-26 03:19:04+00:00",
      "closed_at": "2025-03-27 02:04:34+00:00",
      "resolution_days": 120.94826388888889
    },
    {
      "number": 10648,
      "title": "[Bug]: Llama 3.2 90b crash",
      "state": "closed",
      "comments": 19,
      "body_length": 24273,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-11-26 01:04:20+00:00",
      "closed_at": "2025-04-12 06:26:57+00:00",
      "resolution_days": 137.22403935185184
    },
    {
      "number": 10637,
      "title": "[Bug]:The parameter gpu_memory_utilization does not take effect",
      "state": "closed",
      "comments": 16,
      "body_length": 672,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-11-25 13:22:25+00:00",
      "closed_at": "2025-04-11 03:25:31+00:00",
      "resolution_days": 136.58548611111112
    },
    {
      "number": 10589,
      "title": "[Bug] Streaming output error of tool calling has still not been resolved.\n\n",
      "state": "closed",
      "comments": 14,
      "body_length": 1573,
      "label_names": [],
      "created_at": "2024-11-23 04:06:19+00:00",
      "closed_at": "2024-12-12 01:10:14+00:00",
      "resolution_days": 18.877719907407407
    },
    {
      "number": 10537,
      "title": "[Usage]: How to use ROPE scaling for llama3.1 and gemma2?",
      "state": "closed",
      "comments": 26,
      "body_length": 16395,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2024-11-21 13:36:19+00:00",
      "closed_at": "2025-04-12 02:04:27+00:00",
      "resolution_days": 141.51953703703703
    },
    {
      "number": 10512,
      "title": "[Bug]: I'm trying to run Pixtral-Large-Instruct-2411 using vllm, following the documentation at https://huggingface.co/mistralai/Pixtral-Large-Instruct-2411, but I encountered an error.",
      "state": "closed",
      "comments": 11,
      "body_length": 28664,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-11-21 03:04:46+00:00",
      "closed_at": "2025-06-29 02:15:28+00:00",
      "resolution_days": 219.9657638888889
    },
    {
      "number": 10444,
      "title": "[Bug]: request reward model report 500 Internal Server Error",
      "state": "closed",
      "comments": 17,
      "body_length": 1523,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-11-19 10:32:04+00:00",
      "closed_at": "2024-11-20 05:12:52+00:00",
      "resolution_days": 0.7783333333333333
    },
    {
      "number": 10419,
      "title": "[Bug]: NCCL error with 2-way pipeline parallelism.",
      "state": "closed",
      "comments": 14,
      "body_length": 17354,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-11-18 11:16:23+00:00",
      "closed_at": "2024-11-27 13:24:31+00:00",
      "resolution_days": 9.08898148148148
    },
    {
      "number": 10389,
      "title": "[Bug]: v0.6.4.post1 crashed\uff1aError in model execution: CUDA error: an illegal memory access was encountered",
      "state": "closed",
      "comments": 32,
      "body_length": 16404,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-11-16 11:23:36+00:00",
      "closed_at": "2025-02-07 03:54:08+00:00",
      "resolution_days": 82.68787037037038
    },
    {
      "number": 10360,
      "title": "[Bug]: v0.6.4 requires more GPU memory than v0.6.3",
      "state": "closed",
      "comments": 16,
      "body_length": 372,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-11-15 09:31:23+00:00",
      "closed_at": "2025-04-27 02:11:43+00:00",
      "resolution_days": 162.69467592592594
    },
    {
      "number": 10343,
      "title": "[Feature]: Allow head_size smaller than 128 on TPU with Pallas backend",
      "state": "closed",
      "comments": 14,
      "body_length": 857,
      "label_names": [
        "feature request",
        "tpu"
      ],
      "created_at": "2024-11-14 21:51:45+00:00",
      "closed_at": "2025-07-10 05:02:05+00:00",
      "resolution_days": 237.2988425925926
    },
    {
      "number": 10322,
      "title": "[Usage]: using open-webui with vLLM inference engine instead of ollama",
      "state": "closed",
      "comments": 17,
      "body_length": 559,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-11-14 11:02:00+00:00",
      "closed_at": "2024-11-15 10:32:30+00:00",
      "resolution_days": 0.9795138888888889
    },
    {
      "number": 10289,
      "title": "[Feature]: 2D TP & EP",
      "state": "closed",
      "comments": 13,
      "body_length": 489,
      "label_names": [
        "feature request",
        "stale"
      ],
      "created_at": "2024-11-13 08:59:07+00:00",
      "closed_at": "2025-03-20 02:03:47+00:00",
      "resolution_days": 126.71157407407408
    },
    {
      "number": 10286,
      "title": "[Bug]: vllm serve works incorrect for (some) Vision LM models ",
      "state": "closed",
      "comments": 28,
      "body_length": 9892,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-11-13 08:00:36+00:00",
      "closed_at": "2024-11-14 14:17:22+00:00",
      "resolution_days": 1.2616435185185184
    },
    {
      "number": 10168,
      "title": "[Bug]: KeyError: 'layers.0.self_attn.qkv_proj.weight' when using mistral or phi ",
      "state": "closed",
      "comments": 23,
      "body_length": 9265,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-11-08 20:27:41+00:00",
      "closed_at": "2025-03-13 02:04:14+00:00",
      "resolution_days": 124.23371527777778
    },
    {
      "number": 10158,
      "title": "[Bug]: Error in benchmark model with vllm backend for endpoint /v1/chat/completions",
      "state": "closed",
      "comments": 23,
      "body_length": 2591,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-11-08 13:16:34+00:00",
      "closed_at": "2025-07-03 02:18:01+00:00",
      "resolution_days": 236.5426736111111
    },
    {
      "number": 10156,
      "title": "[Bug]: Unable to load Llama-3.1-70B-Instruct using either `vllm serve` or `vllm-openai` docker",
      "state": "closed",
      "comments": 18,
      "body_length": 16621,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-11-08 12:00:58+00:00",
      "closed_at": "2024-11-12 12:24:04+00:00",
      "resolution_days": 4.016041666666666
    },
    {
      "number": 10155,
      "title": "[Installation]: VLLM does not support TPU v5p-16 (Multi-Host) with Ray Cluster",
      "state": "closed",
      "comments": 12,
      "body_length": 15063,
      "label_names": [
        "installation",
        "tpu",
        "ray"
      ],
      "created_at": "2024-11-08 11:36:17+00:00",
      "closed_at": "2025-02-07 02:16:05+00:00",
      "resolution_days": 90.61097222222222
    },
    {
      "number": 10142,
      "title": "[Bug]: H100 - Your GPU does not have native support for FP8 computation",
      "state": "closed",
      "comments": 22,
      "body_length": 7891,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-11-08 04:13:58+00:00",
      "closed_at": "2024-11-18 02:04:42+00:00",
      "resolution_days": 9.910231481481482
    },
    {
      "number": 10126,
      "title": "[Usage]: VLLM failing to stream response after 512+ prompt tokens.",
      "state": "closed",
      "comments": 15,
      "body_length": 823,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-11-07 18:05:52+00:00",
      "closed_at": "2024-11-12 15:48:15+00:00",
      "resolution_days": 4.90443287037037
    },
    {
      "number": 10119,
      "title": "[New Model]: dunzhang/stella_en_1.5B_v5 ",
      "state": "closed",
      "comments": 24,
      "body_length": 1182,
      "label_names": [
        "new-model",
        "stale"
      ],
      "created_at": "2024-11-07 14:40:39+00:00",
      "closed_at": "2025-06-04 02:13:20+00:00",
      "resolution_days": 208.4810300925926
    },
    {
      "number": 10116,
      "title": "[Bug]: vllm0.6.3.post1  7B model can not use cmd vllm.entrypoints.openai.api_server on wsl",
      "state": "closed",
      "comments": 43,
      "body_length": 23035,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-11-07 10:24:57+00:00",
      "closed_at": "2024-11-16 14:16:00+00:00",
      "resolution_days": 9.16045138888889
    },
    {
      "number": 10114,
      "title": "[RFC]: Merge input processor and input mapper for multi-modal models",
      "state": "closed",
      "comments": 13,
      "body_length": 7572,
      "label_names": [
        "RFC",
        "multi-modality"
      ],
      "created_at": "2024-11-07 09:57:55+00:00",
      "closed_at": "2025-04-28 07:38:50+00:00",
      "resolution_days": 171.90341435185186
    },
    {
      "number": 10102,
      "title": "[Bug]: Engine loop has died for Meta-Llama-3.1-8B-Instruct TP=2",
      "state": "closed",
      "comments": 12,
      "body_length": 11683,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-11-07 04:05:45+00:00",
      "closed_at": "2024-11-22 08:07:38+00:00",
      "resolution_days": 15.167974537037036
    },
    {
      "number": 10065,
      "title": "[Usage]: Model architectures ['LlavaNextForConditionalGeneration'] are not supported for now",
      "state": "closed",
      "comments": 12,
      "body_length": 663,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2024-11-06 03:04:49+00:00",
      "closed_at": "2025-03-07 02:03:32+00:00",
      "resolution_days": 120.95744212962963
    },
    {
      "number": 10062,
      "title": "[Performance]: Throughput and Latency degradation with a  single LoRA adapter on A100 40 GB",
      "state": "closed",
      "comments": 16,
      "body_length": 6569,
      "label_names": [
        "performance",
        "stale"
      ],
      "created_at": "2024-11-06 00:42:03+00:00",
      "closed_at": "2025-05-28 02:13:20+00:00",
      "resolution_days": 203.0633912037037
    },
    {
      "number": 10034,
      "title": "[Bug]: serve Llama-3.2-11B-Vision-Instruct with 2 A10 oom",
      "state": "closed",
      "comments": 22,
      "body_length": 32355,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-11-05 11:34:17+00:00",
      "closed_at": "2024-11-06 03:45:11+00:00",
      "resolution_days": 0.6742361111111111
    },
    {
      "number": 9991,
      "title": "[Bug]: Llama3.2 tool calling OpenAI API not working",
      "state": "closed",
      "comments": 21,
      "body_length": 21960,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-11-04 11:04:00+00:00",
      "closed_at": "2024-11-14 04:14:36+00:00",
      "resolution_days": 9.715694444444445
    },
    {
      "number": 9990,
      "title": "[Bug]: I cannot able to load the model on TESLA T4 GPU in Full precision ",
      "state": "closed",
      "comments": 17,
      "body_length": 18582,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-11-04 10:17:35+00:00",
      "closed_at": "2025-03-07 02:03:39+00:00",
      "resolution_days": 122.65699074074074
    },
    {
      "number": 9874,
      "title": "[Bug]: Function calling with Qwen & Streaming ('NoneType' object has no attribute 'get')",
      "state": "closed",
      "comments": 12,
      "body_length": 4889,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-10-31 08:21:19+00:00",
      "closed_at": "2025-05-28 02:13:23+00:00",
      "resolution_days": 208.74449074074073
    },
    {
      "number": 9872,
      "title": "[Bug]: You are using a model of type qwen2_vl to instantiate a model of type . This is not supported for all configurations of models and can yield errors.",
      "state": "closed",
      "comments": 17,
      "body_length": 766,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-10-31 07:53:03+00:00",
      "closed_at": "2025-03-01 02:05:30+00:00",
      "resolution_days": 120.75864583333333
    },
    {
      "number": 9865,
      "title": "[Installation]: pynvml.NVMLError_InvalidArgument: Invalid Argument",
      "state": "closed",
      "comments": 24,
      "body_length": 10678,
      "label_names": [
        "installation"
      ],
      "created_at": "2024-10-31 03:23:09+00:00",
      "closed_at": "2025-05-07 16:18:14+00:00",
      "resolution_days": 188.53825231481483
    },
    {
      "number": 9832,
      "title": "[Usage]: ValueError: Unexpected weight for Qwen2-VL GPTQ 4-bit custom model.",
      "state": "closed",
      "comments": 16,
      "body_length": 11493,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-10-30 06:46:19+00:00",
      "closed_at": "2024-11-09 03:36:47+00:00",
      "resolution_days": 9.86837962962963
    },
    {
      "number": 9831,
      "title": "[Feature]: host wheel via pypi index?",
      "state": "closed",
      "comments": 12,
      "body_length": 1988,
      "label_names": [
        "feature request",
        "stale"
      ],
      "created_at": "2024-10-30 06:44:14+00:00",
      "closed_at": "2025-03-22 06:11:41+00:00",
      "resolution_days": 142.97739583333333
    },
    {
      "number": 9774,
      "title": "[Bug]: [help wanted] MoE + TP + custom allreduce bug",
      "state": "closed",
      "comments": 11,
      "body_length": 728,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-10-28 21:00:57+00:00",
      "closed_at": "2024-10-30 00:06:25+00:00",
      "resolution_days": 1.1287962962962963
    },
    {
      "number": 9769,
      "title": "[Bug]: Nonsense output for Qwen2.5 72B after upgrading to latest vllm 0.6.3.post1 [REPROs]",
      "state": "closed",
      "comments": 14,
      "body_length": 4028,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-10-28 19:39:48+00:00",
      "closed_at": "2024-12-25 07:20:20+00:00",
      "resolution_days": 57.48648148148148
    },
    {
      "number": 9738,
      "title": "[Bug]: offline inference with ray fails on multinode",
      "state": "closed",
      "comments": 19,
      "body_length": 14692,
      "label_names": [
        "bug",
        "ray",
        "stale"
      ],
      "created_at": "2024-10-27 15:55:45+00:00",
      "closed_at": "2025-06-05 02:13:24+00:00",
      "resolution_days": 220.42892361111112
    },
    {
      "number": 9737,
      "title": "[Bug]: \"Address already in use\" for 1 minute after crash (since 0.6.2)",
      "state": "closed",
      "comments": 17,
      "body_length": 2679,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-10-27 15:00:37+00:00",
      "closed_at": "2024-11-05 15:50:59+00:00",
      "resolution_days": 9.034976851851852
    },
    {
      "number": 9732,
      "title": "[Bug]: Qwen2-VL incoherent output with OpenAI API ",
      "state": "closed",
      "comments": 18,
      "body_length": 9021,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-10-27 12:09:48+00:00",
      "closed_at": "2024-10-29 16:24:59+00:00",
      "resolution_days": 2.177210648148148
    },
    {
      "number": 9723,
      "title": "[Bug]: Incoherent Offline Inference Single Video with Qwen2-VL",
      "state": "closed",
      "comments": 20,
      "body_length": 18072,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-10-26 20:30:10+00:00",
      "closed_at": "2025-06-04 02:13:23+00:00",
      "resolution_days": 220.2383449074074
    },
    {
      "number": 9713,
      "title": "[Usage]: ValueError: Model architectures ['LlamaForCausalLM'] are not supported for now. ",
      "state": "closed",
      "comments": 21,
      "body_length": 7085,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-10-26 04:08:24+00:00",
      "closed_at": "2024-10-26 11:59:20+00:00",
      "resolution_days": 0.327037037037037
    },
    {
      "number": 9701,
      "title": "[Installation] pip install vllm (0.6.3) will force a reinstallation of the CPU version torch and replace cuda torch on windows",
      "state": "closed",
      "comments": 50,
      "body_length": 8157,
      "label_names": [
        "installation",
        "stale"
      ],
      "created_at": "2024-10-25 17:08:40+00:00",
      "closed_at": "2025-04-01 02:13:00+00:00",
      "resolution_days": 157.37800925925927
    },
    {
      "number": 9693,
      "title": "[Bug]: Function calling with stream vs without stream, arguments=None when stream option is enabled",
      "state": "closed",
      "comments": 11,
      "body_length": 2492,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-10-25 09:40:48+00:00",
      "closed_at": "2024-11-19 13:42:52+00:00",
      "resolution_days": 25.16810185185185
    },
    {
      "number": 9655,
      "title": "[Usage]: Pass multiple LoRA modules through YAML config",
      "state": "closed",
      "comments": 11,
      "body_length": 1726,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2024-10-24 10:39:28+00:00",
      "closed_at": "2025-01-23 05:45:42+00:00",
      "resolution_days": 90.79599537037036
    },
    {
      "number": 9624,
      "title": "[Bug]: ValueError: Model architectures ['LlamaForCausalLM'] are not supported for now.",
      "state": "closed",
      "comments": 30,
      "body_length": 15836,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-10-23 16:32:01+00:00",
      "closed_at": "2024-10-29 12:30:32+00:00",
      "resolution_days": 5.832303240740741
    },
    {
      "number": 9567,
      "title": "[Bug]: Models produce different output with different batch sizes",
      "state": "closed",
      "comments": 14,
      "body_length": 12985,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-10-22 00:10:53+00:00",
      "closed_at": "2024-10-23 18:41:53+00:00",
      "resolution_days": 1.7715277777777778
    },
    {
      "number": 9551,
      "title": "[Usage]: Custom LLM Generate",
      "state": "closed",
      "comments": 14,
      "body_length": 16550,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2024-10-21 13:36:15+00:00",
      "closed_at": "2025-02-22 01:57:58+00:00",
      "resolution_days": 123.51508101851852
    },
    {
      "number": 9535,
      "title": "[Performance]: bitsandbytes quantization slow",
      "state": "closed",
      "comments": 15,
      "body_length": 1035,
      "label_names": [
        "performance",
        "stale"
      ],
      "created_at": "2024-10-20 16:57:17+00:00",
      "closed_at": "2025-05-15 02:09:43+00:00",
      "resolution_days": 206.38363425925925
    },
    {
      "number": 9523,
      "title": "[Bug]: Failed to Load  8-bit Quantized Model ",
      "state": "closed",
      "comments": 13,
      "body_length": 16933,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-10-19 03:40:59+00:00",
      "closed_at": "2024-10-22 15:11:07+00:00",
      "resolution_days": 3.4792592592592593
    },
    {
      "number": 9495,
      "title": "[Feature]: LoRA support for InternVLChatModel",
      "state": "closed",
      "comments": 17,
      "body_length": 3696,
      "label_names": [
        "feature request",
        "stale"
      ],
      "created_at": "2024-10-18 08:03:53+00:00",
      "closed_at": "2025-03-05 02:02:37+00:00",
      "resolution_days": 137.74912037037038
    },
    {
      "number": 9483,
      "title": "[Performance]: InternVL multi image speed is not improved compare to original",
      "state": "closed",
      "comments": 22,
      "body_length": 519,
      "label_names": [
        "help wanted",
        "performance",
        "stale"
      ],
      "created_at": "2024-10-18 02:56:45+00:00",
      "closed_at": "2025-03-01 02:05:39+00:00",
      "resolution_days": 133.9645138888889
    },
    {
      "number": 9479,
      "title": "[Feature]: supporting MllamaForCausalLM",
      "state": "closed",
      "comments": 19,
      "body_length": 1077,
      "label_names": [
        "feature request"
      ],
      "created_at": "2024-10-18 01:48:10+00:00",
      "closed_at": "2024-10-21 05:19:15+00:00",
      "resolution_days": 3.146585648148148
    },
    {
      "number": 9476,
      "title": "[Performance]: speed regression 0.6.2 => 0.6.3?",
      "state": "closed",
      "comments": 11,
      "body_length": 3486,
      "label_names": [
        "performance",
        "stale"
      ],
      "created_at": "2024-10-17 21:45:07+00:00",
      "closed_at": "2025-06-20 02:14:10+00:00",
      "resolution_days": 245.18684027777778
    },
    {
      "number": 9454,
      "title": "[Bug]: langchain qwen2.5 function calling error",
      "state": "closed",
      "comments": 11,
      "body_length": 7313,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-10-17 08:48:26+00:00",
      "closed_at": "2024-11-14 08:52:14+00:00",
      "resolution_days": 28.00263888888889
    },
    {
      "number": 9452,
      "title": "[Usage]: vLLM's performance significantly slows down in the newer version when using multi-LoRA inference",
      "state": "closed",
      "comments": 11,
      "body_length": 884,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-10-17 08:17:41+00:00",
      "closed_at": "2024-10-18 05:59:07+00:00",
      "resolution_days": 0.9037731481481481
    },
    {
      "number": 9402,
      "title": "[Bug]: Unable to infer QLoRA adapter using vLLM Docker",
      "state": "closed",
      "comments": 12,
      "body_length": 9963,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-10-16 04:07:51+00:00",
      "closed_at": "2024-10-21 07:53:42+00:00",
      "resolution_days": 5.1568402777777775
    },
    {
      "number": 9377,
      "title": "[Bug]: Model architectures ['LlavaForConditionalGeneration'] are not supported for now.",
      "state": "closed",
      "comments": 11,
      "body_length": 8886,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-10-15 15:26:44+00:00",
      "closed_at": "2024-10-16 08:47:29+00:00",
      "resolution_days": 0.7227430555555555
    },
    {
      "number": 9283,
      "title": "[Bug]: Simultaneous mm calls lead to permanently degraded performance.",
      "state": "closed",
      "comments": 21,
      "body_length": 11720,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-10-11 10:15:41+00:00",
      "closed_at": "2025-02-17 02:01:35+00:00",
      "resolution_days": 128.656875
    },
    {
      "number": 9268,
      "title": "[RFC]: Make device agnostic for diverse hardware support",
      "state": "closed",
      "comments": 22,
      "body_length": 4692,
      "label_names": [
        "RFC",
        "stale"
      ],
      "created_at": "2024-10-11 02:37:19+00:00",
      "closed_at": "2025-06-22 02:15:16+00:00",
      "resolution_days": 253.9846875
    },
    {
      "number": 9255,
      "title": "[Usage]: running gated models offline",
      "state": "closed",
      "comments": 14,
      "body_length": 1610,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-10-10 16:14:29+00:00",
      "closed_at": "2024-11-05 01:17:56+00:00",
      "resolution_days": 25.377395833333335
    },
    {
      "number": 9183,
      "title": "[Bug]: Qwen2.5-Math-7B-Instruct vllm output garbled code, but the probability of huggingface outputing garbled code is lower.",
      "state": "closed",
      "comments": 19,
      "body_length": 15693,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-10-09 07:07:21+00:00",
      "closed_at": "2024-12-27 03:50:22+00:00",
      "resolution_days": 78.86320601851853
    },
    {
      "number": 9180,
      "title": "[Installation]: Pytorch nightly version 2.6 meets error: error: can't copy '/tmp/tmpv5hlsgcm.build-lib/vllm/_core_C.abi3.so': doesn't exist or not a regular file",
      "state": "closed",
      "comments": 13,
      "body_length": 33084,
      "label_names": [
        "installation"
      ],
      "created_at": "2024-10-09 05:23:31+00:00",
      "closed_at": "2024-11-11 01:48:06+00:00",
      "resolution_days": 32.850405092592595
    },
    {
      "number": 9153,
      "title": "[Bug]: InternVL bounding box prediction does not work",
      "state": "closed",
      "comments": 15,
      "body_length": 12567,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-10-08 12:22:30+00:00",
      "closed_at": "2025-01-28 03:22:39+00:00",
      "resolution_days": 111.62510416666667
    },
    {
      "number": 9143,
      "title": "[Bug]: vllm much slower on long context inputs when using --enable-lora even when lora is not used",
      "state": "closed",
      "comments": 11,
      "body_length": 1176,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-10-08 00:38:44+00:00",
      "closed_at": "2024-10-18 21:12:35+00:00",
      "resolution_days": 10.856840277777778
    },
    {
      "number": 9128,
      "title": "[Bug]: assert len(indices) == len(inputs) with `Qwen/Qwen2-VL-2B-Instruct`",
      "state": "closed",
      "comments": 20,
      "body_length": 8503,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-10-07 15:32:41+00:00",
      "closed_at": "2024-10-08 12:01:38+00:00",
      "resolution_days": 0.8534375
    },
    {
      "number": 9096,
      "title": "[Bug]: vllm serve Exception in ASGI application",
      "state": "closed",
      "comments": 14,
      "body_length": 22690,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-10-05 20:33:15+00:00",
      "closed_at": "2024-11-25 17:23:33+00:00",
      "resolution_days": 50.86826388888889
    },
    {
      "number": 8994,
      "title": "[Bug]: Unable to load the tokenizers of certain models",
      "state": "closed",
      "comments": 11,
      "body_length": 16082,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-10-01 09:26:09+00:00",
      "closed_at": "2025-02-09 02:01:41+00:00",
      "resolution_days": 130.6913425925926
    },
    {
      "number": 8984,
      "title": "[Bug]: Exception in worker VllmWorkerProcess while processing method init_device: CUDA error: invalid device ordinal\uff0cvLLM\u5e76\u884c\u591aGPU\u642d\u8f7d\u6a21\u578b\u51fa\u9519",
      "state": "closed",
      "comments": 13,
      "body_length": 26911,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-10-01 03:07:25+00:00",
      "closed_at": "2025-05-08 02:13:33+00:00",
      "resolution_days": 218.96259259259259
    },
    {
      "number": 8967,
      "title": "[RFC]: Reward Modelling in OpenAI Compatible Server",
      "state": "closed",
      "comments": 30,
      "body_length": 3114,
      "label_names": [
        "RFC"
      ],
      "created_at": "2024-09-30 13:45:35+00:00",
      "closed_at": "2024-11-01 08:13:36+00:00",
      "resolution_days": 31.769456018518518
    },
    {
      "number": 8947,
      "title": "[Bug]: vllm serve --config.yaml - Order of arguments matters?",
      "state": "closed",
      "comments": 11,
      "body_length": 6455,
      "label_names": [
        "bug",
        "good first issue"
      ],
      "created_at": "2024-09-29 15:06:36+00:00",
      "closed_at": "2024-10-05 17:35:13+00:00",
      "resolution_days": 6.103206018518518
    },
    {
      "number": 8937,
      "title": "[Bug]: Error when using tensor_parallel in v0.6.1.post1 or 0.6.2",
      "state": "closed",
      "comments": 21,
      "body_length": 10384,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-09-29 03:48:58+00:00",
      "closed_at": "2024-10-17 03:47:28+00:00",
      "resolution_days": 17.998958333333334
    },
    {
      "number": 8932,
      "title": "Hardware Backend Deprecation Policy",
      "state": "closed",
      "comments": 18,
      "body_length": 1267,
      "label_names": [
        "misc",
        "stale"
      ],
      "created_at": "2024-09-29 00:42:38+00:00",
      "closed_at": "2025-05-01 02:13:30+00:00",
      "resolution_days": 214.06310185185185
    },
    {
      "number": 8908,
      "title": "[Usage]: LLM with tensor_parallel_size larger than n. gpus in one node",
      "state": "closed",
      "comments": 13,
      "body_length": 8457,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-09-27 15:05:59+00:00",
      "closed_at": "2024-10-27 15:56:01+00:00",
      "resolution_days": 30.03474537037037
    },
    {
      "number": 8885,
      "title": "[Bug]: 8xV100 gpus: Failed to infer device type",
      "state": "closed",
      "comments": 18,
      "body_length": 12453,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-09-27 04:48:06+00:00",
      "closed_at": "2024-09-27 16:15:11+00:00",
      "resolution_days": 0.4771412037037037
    },
    {
      "number": 8879,
      "title": "[Usage]: OOM when using Llama-3.2-11B-Vision-Instruct",
      "state": "closed",
      "comments": 29,
      "body_length": 19507,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-09-27 02:28:43+00:00",
      "closed_at": "2024-09-27 07:26:39+00:00",
      "resolution_days": 0.20689814814814814
    },
    {
      "number": 8808,
      "title": "[New Model]: allenai/Molmo-7B-0-0924 VisionLM",
      "state": "closed",
      "comments": 17,
      "body_length": 714,
      "label_names": [
        "new-model"
      ],
      "created_at": "2024-09-25 16:34:48+00:00",
      "closed_at": "2024-10-14 14:56:25+00:00",
      "resolution_days": 18.93167824074074
    },
    {
      "number": 8802,
      "title": "[Feature]: LoRA support for Pixtral",
      "state": "closed",
      "comments": 24,
      "body_length": 652,
      "label_names": [
        "feature request",
        "stale"
      ],
      "created_at": "2024-09-25 12:18:13+00:00",
      "closed_at": "2025-03-13 02:04:36+00:00",
      "resolution_days": 168.5738773148148
    },
    {
      "number": 8791,
      "title": "[Bug]: Port binding failure when using pp > 1 after commit 7c7714d856eee6fa94aade729b67f00584f72a4c",
      "state": "closed",
      "comments": 20,
      "body_length": 7178,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-09-25 03:15:30+00:00",
      "closed_at": "2025-02-09 02:01:44+00:00",
      "resolution_days": 136.94877314814815
    },
    {
      "number": 8769,
      "title": "[Usage]: Total generated tokens in benchmarking script",
      "state": "closed",
      "comments": 11,
      "body_length": 9880,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2024-09-24 09:22:29+00:00",
      "closed_at": "2025-01-20 03:57:05+00:00",
      "resolution_days": 117.77402777777777
    },
    {
      "number": 8757,
      "title": "[Bug]: use cpu_offload_gb in gguf failed.",
      "state": "closed",
      "comments": 11,
      "body_length": 7278,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-09-24 02:26:16+00:00",
      "closed_at": "2025-01-24 01:58:48+00:00",
      "resolution_days": 121.98092592592593
    },
    {
      "number": 8730,
      "title": "[Feature]: Multi-Modality Support for Loading Local Files (Images & Videos)",
      "state": "closed",
      "comments": 14,
      "body_length": 6607,
      "label_names": [
        "good first issue",
        "feature request"
      ],
      "created_at": "2024-09-23 08:09:39+00:00",
      "closed_at": "2024-11-04 15:34:58+00:00",
      "resolution_days": 42.309247685185184
    },
    {
      "number": 8667,
      "title": "[Feature]: support out tree multimodal models",
      "state": "closed",
      "comments": 11,
      "body_length": 1349,
      "label_names": [
        "feature request"
      ],
      "created_at": "2024-09-20 11:53:40+00:00",
      "closed_at": "2024-10-04 17:38:26+00:00",
      "resolution_days": 14.239421296296296
    },
    {
      "number": 8631,
      "title": "[Feature]: Online Inference on local model with OpenAI Python SDK",
      "state": "closed",
      "comments": 12,
      "body_length": 1839,
      "label_names": [
        "feature request",
        "stale"
      ],
      "created_at": "2024-09-19 11:32:38+00:00",
      "closed_at": "2025-07-02 02:14:44+00:00",
      "resolution_days": 285.61256944444443
    },
    {
      "number": 8630,
      "title": "[Bug]: OpenGVLab/InternVL2-Llama3-76B: view size is not compatible with input tensor's size and stride",
      "state": "closed",
      "comments": 11,
      "body_length": 10528,
      "label_names": [
        "bug",
        "rocm",
        "stale"
      ],
      "created_at": "2024-09-19 10:03:19+00:00",
      "closed_at": "2025-01-13 06:24:11+00:00",
      "resolution_days": 115.84782407407407
    },
    {
      "number": 8566,
      "title": "[Feature]: Offline quantization for Pixtral-12B",
      "state": "closed",
      "comments": 15,
      "body_length": 915,
      "label_names": [
        "feature request"
      ],
      "created_at": "2024-09-18 13:41:18+00:00",
      "closed_at": "2024-10-18 19:29:57+00:00",
      "resolution_days": 30.242118055555554
    },
    {
      "number": 8536,
      "title": "[Bug]: Model load on 2 or 4-gpu A100 setup may cause default text encoding to be ascii, unless enforce_eager=True",
      "state": "closed",
      "comments": 16,
      "body_length": 16107,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-09-17 13:04:15+00:00",
      "closed_at": "2024-09-21 18:26:23+00:00",
      "resolution_days": 4.223703703703704
    },
    {
      "number": 8532,
      "title": "[Installation]: vLLM build from source errors",
      "state": "closed",
      "comments": 15,
      "body_length": 4702,
      "label_names": [
        "installation"
      ],
      "created_at": "2024-09-17 06:21:33+00:00",
      "closed_at": "2024-09-17 18:34:39+00:00",
      "resolution_days": 0.5090972222222222
    },
    {
      "number": 8483,
      "title": "[Usage]: Dose vLLM support embedding api of multimodal llm?",
      "state": "closed",
      "comments": 11,
      "body_length": 422,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-09-14 04:19:50+00:00",
      "closed_at": "2024-10-23 08:40:48+00:00",
      "resolution_days": 39.181226851851854
    },
    {
      "number": 8450,
      "title": "[Bug]: when curl /chat/completions, TypeError: Unable to evaluate type annotation 'Required[Union[str, Iterable[ChatCompletionContentPartTextParam]]]'.",
      "state": "closed",
      "comments": 23,
      "body_length": 18444,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-09-13 07:39:44+00:00",
      "closed_at": "2024-09-13 16:02:28+00:00",
      "resolution_days": 0.34912037037037036
    },
    {
      "number": 8447,
      "title": "[Bug]: 2 nodes serving hanging ",
      "state": "closed",
      "comments": 19,
      "body_length": 11104,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-09-13 06:49:12+00:00",
      "closed_at": "2024-09-16 19:10:24+00:00",
      "resolution_days": 3.5147222222222223
    },
    {
      "number": 8444,
      "title": "[Bug]: GPU can only load the model once, it gets stuck when loaded again",
      "state": "closed",
      "comments": 12,
      "body_length": 47998,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-09-13 05:55:50+00:00",
      "closed_at": "2025-01-14 01:57:07+00:00",
      "resolution_days": 122.83422453703703
    },
    {
      "number": 8421,
      "title": "[Bug]: mismatch between multimodal tokens and placeholders for Llava-Next (4 GPUs)",
      "state": "closed",
      "comments": 14,
      "body_length": 7047,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-09-12 16:30:53+00:00",
      "closed_at": "2024-09-19 03:53:42+00:00",
      "resolution_days": 6.47417824074074
    },
    {
      "number": 8416,
      "title": "[Bug]: use speculative model in vllm error: TypeError: Worker.__init__() got an unexpected keyword argument 'num_speculative_tokens'",
      "state": "closed",
      "comments": 13,
      "body_length": 2160,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-09-12 13:33:27+00:00",
      "closed_at": "2024-10-11 12:03:02+00:00",
      "resolution_days": 28.93721064814815
    },
    {
      "number": 8408,
      "title": "[Bug]: The accuracy of vllm-Qwen2-VL-7B-Instruct is low.",
      "state": "closed",
      "comments": 22,
      "body_length": 2457,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-09-12 10:26:42+00:00",
      "closed_at": "2025-01-18 01:55:50+00:00",
      "resolution_days": 127.64523148148149
    },
    {
      "number": 8404,
      "title": "[Misc]: Memory Order in Custom Allreduce",
      "state": "closed",
      "comments": 22,
      "body_length": 483,
      "label_names": [
        "misc"
      ],
      "created_at": "2024-09-12 08:23:21+00:00",
      "closed_at": "2024-09-24 09:21:28+00:00",
      "resolution_days": 12.040358796296296
    },
    {
      "number": 8402,
      "title": "[Bug]: CUDA device detection issue with KubeRay distributed inference for quantized models ",
      "state": "closed",
      "comments": 19,
      "body_length": 10476,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-09-12 07:58:02+00:00",
      "closed_at": "2024-11-21 21:13:18+00:00",
      "resolution_days": 70.55226851851852
    },
    {
      "number": 8400,
      "title": "[Bug]: Pixtral leads to Expected at least 18286 dummy tokens for profiling, but found 16640 tokens instead or seq_len 25254 should be equal to N_txt + N_img (806, 12224, 24448)",
      "state": "closed",
      "comments": 22,
      "body_length": 10982,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-09-12 05:58:16+00:00",
      "closed_at": "2024-09-16 06:10:50+00:00",
      "resolution_days": 4.0087268518518515
    },
    {
      "number": 8397,
      "title": "[Bug]: Error when using tensor_parallel in v0.6.1",
      "state": "closed",
      "comments": 11,
      "body_length": 18402,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-09-12 03:56:29+00:00",
      "closed_at": "2024-09-12 04:28:15+00:00",
      "resolution_days": 0.022060185185185186
    },
    {
      "number": 8394,
      "title": "[New Model]: qwen2-audio",
      "state": "closed",
      "comments": 11,
      "body_length": 439,
      "label_names": [
        "new-model"
      ],
      "created_at": "2024-09-12 03:33:21+00:00",
      "closed_at": "2024-10-23 17:54:24+00:00",
      "resolution_days": 41.59795138888889
    },
    {
      "number": 8382,
      "title": "[Bug]: Pixtral fails when limit_mm_per_prompt not set",
      "state": "closed",
      "comments": 12,
      "body_length": 4229,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-09-11 23:19:30+00:00",
      "closed_at": "2024-09-12 22:21:52+00:00",
      "resolution_days": 0.9599768518518519
    },
    {
      "number": 8350,
      "title": "[Bug]: guided generation can't always finish generating the requested structure",
      "state": "closed",
      "comments": 15,
      "body_length": 6272,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-09-11 02:08:48+00:00",
      "closed_at": "2024-10-28 19:13:30+00:00",
      "resolution_days": 47.711597222222224
    },
    {
      "number": 8333,
      "title": "[RFC]: Pinned Caching with Automatic Prefix Caching (Related to Anthropic Prompt Caching API)",
      "state": "closed",
      "comments": 15,
      "body_length": 2420,
      "label_names": [
        "RFC",
        "stale"
      ],
      "created_at": "2024-09-10 13:23:02+00:00",
      "closed_at": "2025-04-18 11:38:46+00:00",
      "resolution_days": 219.9275925925926
    },
    {
      "number": 8323,
      "title": "Do vLLM support `input_embeds` as input while using LLama?",
      "state": "closed",
      "comments": 20,
      "body_length": 557,
      "label_names": [
        "bug",
        "unstale"
      ],
      "created_at": "2024-09-10 07:17:49+00:00",
      "closed_at": "2025-05-02 08:06:40+00:00",
      "resolution_days": 234.0339236111111
    },
    {
      "number": 8316,
      "title": "[Performance]: Image preprocessing is executed twice for same image during VLLM(Qwen2-vl) inference",
      "state": "closed",
      "comments": 11,
      "body_length": 746,
      "label_names": [
        "performance"
      ],
      "created_at": "2024-09-10 03:27:52+00:00",
      "closed_at": "2024-09-11 06:27:58+00:00",
      "resolution_days": 1.1250694444444445
    },
    {
      "number": 8306,
      "title": "[RFC]: Reimplement and separate beam search on top of vLLM core",
      "state": "closed",
      "comments": 21,
      "body_length": 3311,
      "label_names": [
        "RFC"
      ],
      "created_at": "2024-09-09 20:17:13+00:00",
      "closed_at": "2024-10-07 05:47:05+00:00",
      "resolution_days": 27.39574074074074
    },
    {
      "number": 8281,
      "title": "[Bug]: Qwen2-VL AssertionError: assert \"factor\" in rope_scaling.",
      "state": "closed",
      "comments": 27,
      "body_length": 9851,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-09-09 02:57:20+00:00",
      "closed_at": "2024-09-11 16:31:21+00:00",
      "resolution_days": 2.565289351851852
    },
    {
      "number": 8268,
      "title": "[Bug]: AssertionError when using automatic prefix caching and prompt_logprobs",
      "state": "closed",
      "comments": 22,
      "body_length": 17546,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-09-07 12:39:59+00:00",
      "closed_at": "2025-05-25 02:14:18+00:00",
      "resolution_days": 259.56549768518516
    },
    {
      "number": 8243,
      "title": "[Installation]: NotImplementedError get_device_capability",
      "state": "closed",
      "comments": 19,
      "body_length": 8695,
      "label_names": [
        "installation"
      ],
      "created_at": "2024-09-06 17:42:09+00:00",
      "closed_at": "2024-09-10 14:02:21+00:00",
      "resolution_days": 3.8473611111111112
    },
    {
      "number": 8219,
      "title": "[Bug]: vLLM v0.6.1 Instability issue under load.",
      "state": "closed",
      "comments": 23,
      "body_length": 21051,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-09-06 00:36:46+00:00",
      "closed_at": "2024-09-13 14:58:53+00:00",
      "resolution_days": 7.5986921296296295
    },
    {
      "number": 8215,
      "title": "[Usage]: how can i perfrome multiimage inference? in MiniCPM-V-2_6 model or any vision language model with vllm?",
      "state": "closed",
      "comments": 16,
      "body_length": 1304,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-09-05 21:51:13+00:00",
      "closed_at": "2024-09-07 15:37:43+00:00",
      "resolution_days": 1.740625
    },
    {
      "number": 8194,
      "title": "[Bug]: vllm.engine.async_llm_engine.AsyncEngineDeadError",
      "state": "closed",
      "comments": 13,
      "body_length": 22446,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-09-05 13:26:49+00:00",
      "closed_at": "2024-09-13 00:29:45+00:00",
      "resolution_days": 7.46037037037037
    },
    {
      "number": 8190,
      "title": "[Usage]: how to release cuda memory ",
      "state": "closed",
      "comments": 13,
      "body_length": 763,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2024-09-05 12:17:00+00:00",
      "closed_at": "2025-01-06 02:02:40+00:00",
      "resolution_days": 122.57337962962963
    },
    {
      "number": 8184,
      "title": "[Bug]: (OOM) Find two places that cause a significant increase in GPU memory usage (probably lead to memory leak)",
      "state": "closed",
      "comments": 11,
      "body_length": 9900,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-09-05 09:01:36+00:00",
      "closed_at": "2025-06-16 02:14:49+00:00",
      "resolution_days": 283.7175115740741
    },
    {
      "number": 8147,
      "title": "[Performance]: The impact of CPU on vLLM performance is significant.",
      "state": "closed",
      "comments": 16,
      "body_length": 716,
      "label_names": [
        "performance",
        "stale"
      ],
      "created_at": "2024-09-04 06:33:15+00:00",
      "closed_at": "2025-03-22 06:20:41+00:00",
      "resolution_days": 198.99127314814814
    },
    {
      "number": 8139,
      "title": "[New Model]: Qwen2-VL",
      "state": "closed",
      "comments": 15,
      "body_length": 551,
      "label_names": [
        "new-model"
      ],
      "created_at": "2024-09-04 02:28:31+00:00",
      "closed_at": "2024-09-11 16:31:21+00:00",
      "resolution_days": 7.585300925925926
    },
    {
      "number": 8136,
      "title": "[Usage]: 125m parameter model is also showing CUDA: Out of memory error in a Nvidia16GB 4060 ",
      "state": "closed",
      "comments": 14,
      "body_length": 601,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-09-03 23:43:52+00:00",
      "closed_at": "2024-09-08 23:42:03+00:00",
      "resolution_days": 4.998738425925926
    },
    {
      "number": 8068,
      "title": "[Bug]: ValueError: could not broadcast input array from shape (513,) into shape (512,)",
      "state": "closed",
      "comments": 15,
      "body_length": 13532,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-09-01 23:38:54+00:00",
      "closed_at": "2024-09-10 23:00:36+00:00",
      "resolution_days": 8.973402777777778
    },
    {
      "number": 8042,
      "title": "[Installation]: Issues with installing vLLM on ROCM without sudo access",
      "state": "closed",
      "comments": 35,
      "body_length": 1948,
      "label_names": [
        "installation",
        "rocm"
      ],
      "created_at": "2024-08-30 18:42:57+00:00",
      "closed_at": "2024-10-28 14:29:33+00:00",
      "resolution_days": 58.82402777777778
    },
    {
      "number": 8031,
      "title": "[Usage]: How can I determine the maximum number of concurrent requests?",
      "state": "closed",
      "comments": 14,
      "body_length": 6212,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2024-08-30 12:30:29+00:00",
      "closed_at": "2025-03-27 18:40:06+00:00",
      "resolution_days": 209.25667824074074
    },
    {
      "number": 8016,
      "title": "[Bug]: v0.5.5 crash: \"AssertionError: expected running sequences\"",
      "state": "closed",
      "comments": 34,
      "body_length": 2736,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-08-30 02:45:06+00:00",
      "closed_at": "2025-01-24 01:59:09+00:00",
      "resolution_days": 146.96809027777778
    },
    {
      "number": 7996,
      "title": "[Bug]: InternVL2-26B infer error:Attempted to assign 7 x 256 = 1792 multimodal tokens to 506 placeholders",
      "state": "closed",
      "comments": 22,
      "body_length": 8036,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-08-29 13:53:38+00:00",
      "closed_at": "2024-08-30 15:20:35+00:00",
      "resolution_days": 1.0603819444444444
    },
    {
      "number": 7984,
      "title": "[New Model]: LlavaQwen2ForCausalLM",
      "state": "closed",
      "comments": 21,
      "body_length": 1142,
      "label_names": [
        "new-model"
      ],
      "created_at": "2024-08-29 06:25:58+00:00",
      "closed_at": "2024-10-23 08:02:50+00:00",
      "resolution_days": 55.06726851851852
    },
    {
      "number": 7983,
      "title": "[Usage]: Deploying multimodal retrieval models",
      "state": "closed",
      "comments": 14,
      "body_length": 714,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2024-08-29 06:24:15+00:00",
      "closed_at": "2025-02-10 02:00:08+00:00",
      "resolution_days": 164.81658564814813
    },
    {
      "number": 7968,
      "title": "[Bug]: Multistep with n>1 Fails",
      "state": "closed",
      "comments": 11,
      "body_length": 5441,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-08-28 19:43:18+00:00",
      "closed_at": "2025-02-01 02:00:57+00:00",
      "resolution_days": 156.26225694444443
    },
    {
      "number": 7960,
      "title": "[Bug]: segfault when loading MoE models",
      "state": "closed",
      "comments": 23,
      "body_length": 23229,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-08-28 17:34:30+00:00",
      "closed_at": "2024-09-20 12:46:01+00:00",
      "resolution_days": 22.799664351851852
    },
    {
      "number": 7940,
      "title": "[Bug]: RuntimeError: operator torchvision::nms does not exist",
      "state": "closed",
      "comments": 14,
      "body_length": 8983,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-08-28 06:35:38+00:00",
      "closed_at": "2025-01-31 01:58:23+00:00",
      "resolution_days": 155.80746527777777
    },
    {
      "number": 7935,
      "title": "[Performance]: 5x slower throught with openAI client/server than native one",
      "state": "closed",
      "comments": 21,
      "body_length": 8182,
      "label_names": [
        "performance"
      ],
      "created_at": "2024-08-28 02:44:31+00:00",
      "closed_at": "2024-10-28 19:10:50+00:00",
      "resolution_days": 61.68494212962963
    },
    {
      "number": 7904,
      "title": "[Bug]: ray + vllm async engine: Background loop is stopped",
      "state": "closed",
      "comments": 14,
      "body_length": 10854,
      "label_names": [
        "bug",
        "ray"
      ],
      "created_at": "2024-08-27 09:23:44+00:00",
      "closed_at": "2025-04-03 15:47:03+00:00",
      "resolution_days": 219.26619212962962
    },
    {
      "number": 7815,
      "title": "[Bug]: Error: No available node types can fulfill resource request",
      "state": "closed",
      "comments": 17,
      "body_length": 1737,
      "label_names": [
        "bug",
        "ray"
      ],
      "created_at": "2024-08-23 08:40:33+00:00",
      "closed_at": "2024-08-23 14:22:10+00:00",
      "resolution_days": 0.2372337962962963
    },
    {
      "number": 7791,
      "title": "[Bug]: Critical distributed executor bug",
      "state": "closed",
      "comments": 11,
      "body_length": 21182,
      "label_names": [
        "bug",
        "rocm"
      ],
      "created_at": "2024-08-22 18:18:36+00:00",
      "closed_at": "2025-05-29 21:37:16+00:00",
      "resolution_days": 280.13796296296294
    },
    {
      "number": 7785,
      "title": "[Bug]: install vllm ocurr the building error",
      "state": "closed",
      "comments": 14,
      "body_length": 6777,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-08-22 12:25:44+00:00",
      "closed_at": "2025-05-20 08:58:20+00:00",
      "resolution_days": 270.85597222222225
    },
    {
      "number": 7777,
      "title": "[Bug]: Using fp8 cutlass scaled_mm causes wrong output",
      "state": "closed",
      "comments": 32,
      "body_length": 6062,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-08-22 08:31:10+00:00",
      "closed_at": "2024-08-29 08:54:10+00:00",
      "resolution_days": 7.0159722222222225
    },
    {
      "number": 7718,
      "title": "[Bug]: Error loading microsoft/Phi-3.5-vision-instruct",
      "state": "closed",
      "comments": 14,
      "body_length": 9069,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-08-21 04:45:15+00:00",
      "closed_at": "2024-08-22 01:36:25+00:00",
      "resolution_days": 0.8688657407407407
    },
    {
      "number": 7714,
      "title": "[Bug]: Unable to use fp8 kv cache with chunked prefill on ampere",
      "state": "closed",
      "comments": 22,
      "body_length": 16944,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-08-21 01:33:25+00:00",
      "closed_at": "2025-04-18 02:07:14+00:00",
      "resolution_days": 240.0234837962963
    },
    {
      "number": 7669,
      "title": "[Bug]: Mismatch in the number of image tokens and placeholders during batch inference",
      "state": "closed",
      "comments": 14,
      "body_length": 2125,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-08-20 01:21:26+00:00",
      "closed_at": "2024-12-28 01:59:21+00:00",
      "resolution_days": 130.0263310185185
    },
    {
      "number": 7653,
      "title": "[Bug]: Error happened with Large scale requests based on 0.5.4 vllm",
      "state": "closed",
      "comments": 20,
      "body_length": 6856,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-08-19 15:30:55+00:00",
      "closed_at": "2024-08-22 13:55:12+00:00",
      "resolution_days": 2.9335300925925925
    },
    {
      "number": 7627,
      "title": "[Documentation request]: Add documentation on lossless guarantees of speculative decoding in vLLM",
      "state": "closed",
      "comments": 12,
      "body_length": 9012,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-08-17 16:36:32+00:00",
      "closed_at": "2024-10-16 05:49:56+00:00",
      "resolution_days": 59.55097222222222
    },
    {
      "number": 7579,
      "title": "[Bug]: fp8 performance is worse than fp16 when batch size is 1",
      "state": "closed",
      "comments": 15,
      "body_length": 7121,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-08-16 03:20:54+00:00",
      "closed_at": "2025-04-13 02:17:21+00:00",
      "resolution_days": 239.95586805555556
    },
    {
      "number": 7567,
      "title": "[Usage]: Extremely slow inference with Llama 3.1 70b Instruct",
      "state": "closed",
      "comments": 23,
      "body_length": 5817,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2024-08-15 21:13:11+00:00",
      "closed_at": "2025-05-19 02:13:48+00:00",
      "resolution_days": 276.20876157407406
    },
    {
      "number": 7558,
      "title": "[RFC]: Support for video input ",
      "state": "closed",
      "comments": 17,
      "body_length": 1080,
      "label_names": [
        "RFC"
      ],
      "created_at": "2024-08-15 15:44:12+00:00",
      "closed_at": "2024-11-08 03:40:15+00:00",
      "resolution_days": 84.49725694444444
    },
    {
      "number": 7546,
      "title": "[Feature]: Inquiry about Multi-modal Support in VLLM for MiniCPM-V2.6",
      "state": "closed",
      "comments": 19,
      "body_length": 1190,
      "label_names": [
        "feature request"
      ],
      "created_at": "2024-08-15 06:36:05+00:00",
      "closed_at": "2024-08-15 06:49:48+00:00",
      "resolution_days": 0.009525462962962963
    },
    {
      "number": 7540,
      "title": "[Performance]: Why does VLLM perform worse than TGI in Speculative decoding?",
      "state": "closed",
      "comments": 14,
      "body_length": 8573,
      "label_names": [
        "performance",
        "stale"
      ],
      "created_at": "2024-08-15 03:26:27+00:00",
      "closed_at": "2025-01-14 01:57:17+00:00",
      "resolution_days": 151.9380787037037
    },
    {
      "number": 7524,
      "title": "[Feature]: need a GB-based alternative for gpu_memory_utilization",
      "state": "closed",
      "comments": 13,
      "body_length": 1489,
      "label_names": [
        "feature request",
        "stale"
      ],
      "created_at": "2024-08-14 17:19:28+00:00",
      "closed_at": "2025-03-25 02:05:34+00:00",
      "resolution_days": 222.36534722222223
    },
    {
      "number": 7517,
      "title": "[Bug]:  AutoAWQ marlin methods error",
      "state": "closed",
      "comments": 11,
      "body_length": 874,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-08-14 13:09:39+00:00",
      "closed_at": "2025-02-24 02:01:56+00:00",
      "resolution_days": 193.53630787037036
    },
    {
      "number": 7514,
      "title": "[Bug]: error while attempting to bind on address ('0.0.0.0', 8000): address already in use",
      "state": "closed",
      "comments": 11,
      "body_length": 3246,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-08-14 10:08:26+00:00",
      "closed_at": "2025-06-29 02:15:41+00:00",
      "resolution_days": 318.6717013888889
    },
    {
      "number": 7494,
      "title": "[Bug]: DeepSeek-Coder-V2-Instruct-AWQ    assert self.quant_method is not None",
      "state": "closed",
      "comments": 21,
      "body_length": 10072,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-08-14 01:25:56+00:00",
      "closed_at": "2025-05-07 02:10:19+00:00",
      "resolution_days": 266.03082175925925
    },
    {
      "number": 7481,
      "title": "Release v0.5.5",
      "state": "closed",
      "comments": 14,
      "body_length": 217,
      "label_names": [
        "release"
      ],
      "created_at": "2024-08-13 21:01:28+00:00",
      "closed_at": "2024-08-23 19:50:56+00:00",
      "resolution_days": 9.951018518518518
    },
    {
      "number": 7478,
      "title": "[Bug]:  Support Falcon Mamba ",
      "state": "closed",
      "comments": 19,
      "body_length": 204,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-08-13 19:54:32+00:00",
      "closed_at": "2024-10-23 16:40:44+00:00",
      "resolution_days": 70.86541666666666
    },
    {
      "number": 7472,
      "title": "[Bug]: a bug in CUDA capabilities test with different GPUs available",
      "state": "closed",
      "comments": 11,
      "body_length": 16041,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-08-13 13:49:58+00:00",
      "closed_at": "2024-08-16 09:43:55+00:00",
      "resolution_days": 2.8291319444444443
    },
    {
      "number": 7470,
      "title": "[Bug] [BlockManagerV2]: Prefill for sliding window models can allocate more blocks than sliding window size",
      "state": "closed",
      "comments": 11,
      "body_length": 1266,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-08-13 13:08:06+00:00",
      "closed_at": "2024-12-19 02:04:38+00:00",
      "resolution_days": 127.53925925925925
    },
    {
      "number": 7464,
      "title": "[Bug]: Gemma-2-2b-it load model hangs by vLLM==0.5.1 on Tesla T4 GPU ",
      "state": "closed",
      "comments": 11,
      "body_length": 4812,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-08-13 07:18:51+00:00",
      "closed_at": "2024-09-02 12:19:43+00:00",
      "resolution_days": 20.208935185185187
    },
    {
      "number": 7373,
      "title": "[Bug]: Phi-3-vision: ERROR 08-09 11:41:40 async_llm_engine.py:56] RuntimeError: stack expects each tensor to be equal size, but got [1933, 4096] at entry 0 and [2509, 4096] at entry 1",
      "state": "closed",
      "comments": 14,
      "body_length": 7830,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-08-09 17:14:28+00:00",
      "closed_at": "2024-08-10 16:19:34+00:00",
      "resolution_days": 0.961875
    },
    {
      "number": 7337,
      "title": "[Bug]: Extra body don't work when response_format is also sent for serving.",
      "state": "closed",
      "comments": 11,
      "body_length": 8313,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-08-09 03:44:41+00:00",
      "closed_at": "2025-04-23 02:09:01+00:00",
      "resolution_days": 256.9335648148148
    },
    {
      "number": 7301,
      "title": "[Usage]: Acceptance rate for Speculative Decoding",
      "state": "closed",
      "comments": 13,
      "body_length": 321,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2024-08-08 13:28:20+00:00",
      "closed_at": "2024-12-21 01:59:02+00:00",
      "resolution_days": 134.52131944444446
    },
    {
      "number": 7290,
      "title": "[Bug]: \"500 Internal Server Error\" after upgrade to v0.5.4",
      "state": "closed",
      "comments": 13,
      "body_length": 8857,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-08-08 03:28:01+00:00",
      "closed_at": "2025-01-13 02:03:28+00:00",
      "resolution_days": 157.94128472222224
    },
    {
      "number": 7240,
      "title": "[Bug]: The new version (v0.5.4) cannot load the gptq model, but the old version (vllm=0.5.3.post1) can do it.",
      "state": "closed",
      "comments": 11,
      "body_length": 15579,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-08-07 03:35:38+00:00",
      "closed_at": "2024-08-08 12:45:34+00:00",
      "resolution_days": 1.381898148148148
    },
    {
      "number": 7204,
      "title": "[Bug]: GPTQ Marlin with cpu-offload-gb fails on `0.5.4`",
      "state": "closed",
      "comments": 11,
      "body_length": 22292,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-08-06 12:52:06+00:00",
      "closed_at": "2024-08-07 01:34:27+00:00",
      "resolution_days": 0.5294097222222223
    },
    {
      "number": 7196,
      "title": "[Bug]: ZMQError: Address already in use (addr='tcp://127.0.0.1:5570')",
      "state": "closed",
      "comments": 14,
      "body_length": 8472,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-08-06 09:12:13+00:00",
      "closed_at": "2024-08-08 12:46:38+00:00",
      "resolution_days": 2.148900462962963
    },
    {
      "number": 7194,
      "title": "[Bug]: Incomplete tool calling response for pipeline-parallel vllm with ray",
      "state": "closed",
      "comments": 22,
      "body_length": 3915,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-08-06 08:16:25+00:00",
      "closed_at": "2025-04-20 02:11:12+00:00",
      "resolution_days": 256.7463773148148
    },
    {
      "number": 7160,
      "title": "[Bug]: InternVL2 Mismatch in number of image tokens and image embedding size",
      "state": "closed",
      "comments": 18,
      "body_length": 13802,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-08-05 14:21:38+00:00",
      "closed_at": "2024-08-07 16:32:08+00:00",
      "resolution_days": 2.090625
    },
    {
      "number": 7124,
      "title": "[RFC]: Model architecture plugins",
      "state": "closed",
      "comments": 17,
      "body_length": 1104,
      "label_names": [
        "RFC",
        "stale"
      ],
      "created_at": "2024-08-04 12:13:33+00:00",
      "closed_at": "2025-01-03 02:41:49+00:00",
      "resolution_days": 151.60296296296298
    },
    {
      "number": 7060,
      "title": "[Usage]: Unable to run `gemma-2-2b-it`",
      "state": "closed",
      "comments": 15,
      "body_length": 242,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2024-08-02 03:25:33+00:00",
      "closed_at": "2024-12-01 02:14:03+00:00",
      "resolution_days": 120.95034722222222
    },
    {
      "number": 7033,
      "title": "[Usage]:  I updated VLLM to the latest one and I discover that when I launch serve, I cann't see the ouput of the special token like <bos>. How can I get them?",
      "state": "closed",
      "comments": 11,
      "body_length": 6973,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2024-08-01 13:11:11+00:00",
      "closed_at": "2024-11-01 03:26:05+00:00",
      "resolution_days": 91.59368055555555
    },
    {
      "number": 6988,
      "title": "[Bug]: subprocess.CalledProcessError: Command '['cmake', '--build', '.', '-j=40', '--target=_moe_C', '--target=_C']' returned non-zero exit status 1.",
      "state": "closed",
      "comments": 14,
      "body_length": 31988,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-07-31 10:24:18+00:00",
      "closed_at": "2024-09-30 21:15:09+00:00",
      "resolution_days": 61.45197916666667
    },
    {
      "number": 6967,
      "title": "[Bug]: speculative decoding doesn't work with online mode",
      "state": "closed",
      "comments": 17,
      "body_length": 14452,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-07-31 02:37:21+00:00",
      "closed_at": "2024-08-22 13:33:50+00:00",
      "resolution_days": 22.455891203703704
    },
    {
      "number": 6947,
      "title": "[Feature]: Add embeddings api for Llama ",
      "state": "closed",
      "comments": 15,
      "body_length": 490,
      "label_names": [
        "feature request",
        "stale"
      ],
      "created_at": "2024-07-30 13:05:59+00:00",
      "closed_at": "2024-11-01 04:10:21+00:00",
      "resolution_days": 93.6280324074074
    },
    {
      "number": 6938,
      "title": "[Usage]: Error with Multi Node llama 405B inference",
      "state": "closed",
      "comments": 12,
      "body_length": 12867,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-07-30 09:39:39+00:00",
      "closed_at": "2024-08-25 18:57:15+00:00",
      "resolution_days": 26.38722222222222
    },
    {
      "number": 6928,
      "title": "[Feature]: Support rerank models",
      "state": "closed",
      "comments": 19,
      "body_length": 402,
      "label_names": [
        "feature request"
      ],
      "created_at": "2024-07-30 07:11:44+00:00",
      "closed_at": "2025-04-01 15:32:28+00:00",
      "resolution_days": 245.34773148148147
    },
    {
      "number": 6890,
      "title": "[Bug]: Vllm api server does not receive supported parameter `truncate_prompt_tokens`",
      "state": "closed",
      "comments": 16,
      "body_length": 2260,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-07-29 05:56:58+00:00",
      "closed_at": "2024-12-01 02:14:32+00:00",
      "resolution_days": 124.8455324074074
    },
    {
      "number": 6879,
      "title": "[Performance]: use Python array to replace Python list for zero-copy tensor creation",
      "state": "closed",
      "comments": 19,
      "body_length": 3494,
      "label_names": [
        "performance",
        "stale"
      ],
      "created_at": "2024-07-28 23:58:48+00:00",
      "closed_at": "2024-12-01 02:14:38+00:00",
      "resolution_days": 125.09432870370371
    },
    {
      "number": 6876,
      "title": "[Bug]: vLLM takes forever to load a locally stored 7B model",
      "state": "closed",
      "comments": 18,
      "body_length": 6863,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-07-28 16:47:23+00:00",
      "closed_at": "2024-12-21 01:59:08+00:00",
      "resolution_days": 145.38315972222222
    },
    {
      "number": 6875,
      "title": "[Bug]: Error Running DeepSeek-v2-Lite w/ FP8",
      "state": "closed",
      "comments": 13,
      "body_length": 15076,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-07-28 15:36:43+00:00",
      "closed_at": "2025-01-09 02:14:14+00:00",
      "resolution_days": 164.4427199074074
    },
    {
      "number": 6870,
      "title": "[Bug]: Error: Failed to initialize the TMA descriptor 700 for LLaMa 3.1 405B on 8*H100 -- prefill error?",
      "state": "closed",
      "comments": 13,
      "body_length": 3464,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-07-28 02:17:55+00:00",
      "closed_at": "2024-07-28 12:46:13+00:00",
      "resolution_days": 0.4363194444444444
    },
    {
      "number": 6854,
      "title": "[RFC]: Multi-Step Scheduling",
      "state": "closed",
      "comments": 15,
      "body_length": 5899,
      "label_names": [
        "RFC",
        "stale"
      ],
      "created_at": "2024-07-26 23:53:31+00:00",
      "closed_at": "2024-12-08 02:10:55+00:00",
      "resolution_days": 134.09541666666667
    },
    {
      "number": 6829,
      "title": "llama 3 8b model with A10 GPU, OOM with VLLM, but holds good on HF transformer pipline",
      "state": "closed",
      "comments": 11,
      "body_length": 954,
      "label_names": [
        "stale"
      ],
      "created_at": "2024-07-26 07:44:47+00:00",
      "closed_at": "2024-11-02 16:21:41+00:00",
      "resolution_days": 99.35895833333333
    },
    {
      "number": 6823,
      "title": "[Bug]: ERROR 07-26 14:50:35 multiproc_worker_utils.py:120] Worker VllmWorkerProcess pid 214281 died, exit code: -11",
      "state": "closed",
      "comments": 19,
      "body_length": 7748,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-07-26 07:02:55+00:00",
      "closed_at": "2025-07-04 02:18:46+00:00",
      "resolution_days": 342.8026736111111
    },
    {
      "number": 6797,
      "title": "[RFC]: Isolate OpenAI Server Into Separate Process",
      "state": "closed",
      "comments": 27,
      "body_length": 2733,
      "label_names": [
        "RFC"
      ],
      "created_at": "2024-07-25 19:52:24+00:00",
      "closed_at": "2024-12-16 10:17:35+00:00",
      "resolution_days": 143.60082175925925
    },
    {
      "number": 6790,
      "title": "[Bug]: Engine iteration timed out. This should never happen!",
      "state": "closed",
      "comments": 16,
      "body_length": 17586,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-07-25 15:57:20+00:00",
      "closed_at": "2024-12-21 01:59:11+00:00",
      "resolution_days": 148.4179513888889
    },
    {
      "number": 6783,
      "title": "[Bug]: SIGSEGV received at time=1721904360 on cpu 140, Fatal Python error: Segmentation fault",
      "state": "closed",
      "comments": 22,
      "body_length": 13742,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-07-25 11:39:17+00:00",
      "closed_at": "2025-03-31 02:09:43+00:00",
      "resolution_days": 248.60446759259258
    },
    {
      "number": 6746,
      "title": "[Usage]: The 8xH100 device failed to run meta-llama/Meta-Llama-3.1-405B-Instruct-FP8.",
      "state": "closed",
      "comments": 24,
      "body_length": 32429,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-07-24 14:08:10+00:00",
      "closed_at": "2024-08-16 17:05:07+00:00",
      "resolution_days": 23.122881944444444
    },
    {
      "number": 6732,
      "title": "[Bug]: VLLM 0.5.3.post1 [rank0]: RuntimeError: NCCL error: unhandled cuda error (run with NCCL_DEBUG=INFO for details)",
      "state": "closed",
      "comments": 33,
      "body_length": 46036,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-07-24 08:31:17+00:00",
      "closed_at": "2025-03-05 02:02:57+00:00",
      "resolution_days": 223.73032407407408
    },
    {
      "number": 6727,
      "title": "[Bug]: Cannot find any of ['adapter_name_or_path'] in the model's quantization config",
      "state": "closed",
      "comments": 20,
      "body_length": 8020,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-07-24 06:23:06+00:00",
      "closed_at": "2024-12-14 02:04:26+00:00",
      "resolution_days": 142.82037037037037
    },
    {
      "number": 6723,
      "title": "[Bug]: python3: /project/lib/Analysis/Allocation.cpp:43: std::pair<llvm::SmallVector<unsigned int>, llvm::SmallVector<unsigned int> > mlir::triton::getCvtOrder(mlir::Attribute, mlir::Attribute): Assertion `!(srcMmaLayout && dstMmaLayout && !srcMmaLayout.isAmpere()) && \"mma -> mma layout conversion is only supported on Ampere\"' failed. Aborted (core dumped)",
      "state": "closed",
      "comments": 14,
      "body_length": 2680,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-07-24 03:33:23+00:00",
      "closed_at": "2024-08-04 06:38:21+00:00",
      "resolution_days": 11.128449074074075
    },
    {
      "number": 6713,
      "title": "[Bug]: RuntimeError: GET was unable to find an engine to execute this computation for llava-next model",
      "state": "closed",
      "comments": 12,
      "body_length": 10726,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-07-24 00:21:47+00:00",
      "closed_at": "2024-12-04 02:07:48+00:00",
      "resolution_days": 133.07362268518517
    },
    {
      "number": 6701,
      "title": "[Bug]: multi-GPU inference (tensor_parallel_size=2) fails on Intel GPUs",
      "state": "closed",
      "comments": 15,
      "body_length": 9662,
      "label_names": [
        "bug",
        "ray",
        "stale"
      ],
      "created_at": "2024-07-23 19:17:18+00:00",
      "closed_at": "2025-04-11 02:06:39+00:00",
      "resolution_days": 261.2842708333333
    },
    {
      "number": 6700,
      "title": "[Bug]: vLLM 0.5.3 is getting stuck at LLAMA 3.1 405B FP8 model loading",
      "state": "closed",
      "comments": 13,
      "body_length": 7505,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-07-23 18:56:31+00:00",
      "closed_at": "2024-07-25 00:37:13+00:00",
      "resolution_days": 1.2365972222222221
    },
    {
      "number": 6689,
      "title": "[Model] Meta Llama 3.1 Know Issues & FAQ",
      "state": "closed",
      "comments": 85,
      "body_length": 1466,
      "label_names": [],
      "created_at": "2024-07-23 15:19:50+00:00",
      "closed_at": "2024-09-04 06:02:40+00:00",
      "resolution_days": 42.61307870370371
    },
    {
      "number": 6646,
      "title": "[Bug]: In SamplingParams, setting n to a large value (e.g., 512) never finishes",
      "state": "closed",
      "comments": 14,
      "body_length": 8708,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-07-22 13:05:09+00:00",
      "closed_at": "2025-07-12 02:13:14+00:00",
      "resolution_days": 354.5472800925926
    },
    {
      "number": 6644,
      "title": "[Bug]: PaliGemma serving",
      "state": "closed",
      "comments": 15,
      "body_length": 9613,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-07-22 11:44:03+00:00",
      "closed_at": "2024-12-22 02:04:45+00:00",
      "resolution_days": 152.59770833333334
    },
    {
      "number": 6641,
      "title": "[Usage]: What do max_num_seqs and max_model_len do",
      "state": "closed",
      "comments": 11,
      "body_length": 616,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-07-22 09:45:30+00:00",
      "closed_at": "2025-03-27 18:34:54+00:00",
      "resolution_days": 248.36763888888888
    },
    {
      "number": 6631,
      "title": "[Bug]: Is vllm support function call mode?",
      "state": "closed",
      "comments": 33,
      "body_length": 3639,
      "label_names": [
        "bug",
        "unstale",
        "tool-calling"
      ],
      "created_at": "2024-07-22 03:38:50+00:00",
      "closed_at": "2025-06-19 08:20:17+00:00",
      "resolution_days": 332.1954513888889
    },
    {
      "number": 6607,
      "title": "[Bug]: Phi-3-mini does not work when using Ray",
      "state": "closed",
      "comments": 21,
      "body_length": 18097,
      "label_names": [
        "bug",
        "ray"
      ],
      "created_at": "2024-07-20 14:51:24+00:00",
      "closed_at": "2024-10-22 02:46:25+00:00",
      "resolution_days": 93.49653935185185
    },
    {
      "number": 6605,
      "title": "Increase supported token window for using LoRA Adapter with  mistralai/Mistral-Nemo-Instruct-2407",
      "state": "closed",
      "comments": 15,
      "body_length": 499,
      "label_names": [
        "feature request"
      ],
      "created_at": "2024-07-20 10:26:06+00:00",
      "closed_at": "2024-07-21 12:53:48+00:00",
      "resolution_days": 1.1025694444444445
    },
    {
      "number": 6603,
      "title": "[Bug]: Error when loading mistral and gemma model using VLLM docker",
      "state": "closed",
      "comments": 11,
      "body_length": 11828,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-07-20 06:34:04+00:00",
      "closed_at": "2024-11-24 02:07:48+00:00",
      "resolution_days": 126.81509259259259
    },
    {
      "number": 6601,
      "title": "[Usage]:Can vllm use a method similar to device_map in transformers ?",
      "state": "closed",
      "comments": 11,
      "body_length": 438,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2024-07-20 04:01:25+00:00",
      "closed_at": "2024-11-24 02:07:51+00:00",
      "resolution_days": 126.92113425925926
    },
    {
      "number": 6551,
      "title": "[Bug]: vllm doesn't support multi-instance GPU",
      "state": "closed",
      "comments": 19,
      "body_length": 8647,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-07-18 21:02:10+00:00",
      "closed_at": "2024-07-19 17:12:19+00:00",
      "resolution_days": 0.8403819444444445
    },
    {
      "number": 6531,
      "title": "[Bug]: inter-token latency is lower than TPOT in serving benchmark result",
      "state": "closed",
      "comments": 17,
      "body_length": 2901,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-07-18 04:55:54+00:00",
      "closed_at": "2025-01-23 01:58:43+00:00",
      "resolution_days": 188.8769560185185
    },
    {
      "number": 6478,
      "title": "[Bug]: AttributeError: '_OpNamespace' '_C' object has no attribute 'rotary_embedding' / gemma-2-9b with vllm=0.5.2 ",
      "state": "closed",
      "comments": 17,
      "body_length": 8064,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-07-16 15:18:54+00:00",
      "closed_at": "2025-03-20 02:04:28+00:00",
      "resolution_days": 246.4483101851852
    },
    {
      "number": 6473,
      "title": "[Bug]: [vllm-openvino]: ValueError: `use_cache` was set to `True` but the loaded model only supports `use_cache=False`. ",
      "state": "closed",
      "comments": 21,
      "body_length": 29064,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-07-16 12:05:32+00:00",
      "closed_at": "2024-07-19 02:04:07+00:00",
      "resolution_days": 2.582349537037037
    },
    {
      "number": 6465,
      "title": "[Bug]: failed when run Qwen2-54B-A14B-GPTQ-Int4(MOE)",
      "state": "closed",
      "comments": 13,
      "body_length": 6133,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-07-16 07:35:14+00:00",
      "closed_at": "2025-03-01 02:05:58+00:00",
      "resolution_days": 227.7713425925926
    },
    {
      "number": 6464,
      "title": "unable to run vllm model deployment ",
      "state": "closed",
      "comments": 19,
      "body_length": 35322,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-07-16 07:33:47+00:00",
      "closed_at": "2025-03-17 02:05:48+00:00",
      "resolution_days": 243.7722337962963
    },
    {
      "number": 6462,
      "title": "[Bug]: Can't load gemma-2-9b-it with vllm 0.5.2",
      "state": "closed",
      "comments": 39,
      "body_length": 10914,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-07-16 06:59:04+00:00",
      "closed_at": "2024-07-24 04:07:40+00:00",
      "resolution_days": 7.880972222222222
    },
    {
      "number": 6434,
      "title": "v0.5.2, v0.5.3, v0.6.0 Release Tracker",
      "state": "closed",
      "comments": 11,
      "body_length": 1027,
      "label_names": [
        "release"
      ],
      "created_at": "2024-07-15 03:19:27+00:00",
      "closed_at": "2024-08-05 21:39:49+00:00",
      "resolution_days": 21.76414351851852
    },
    {
      "number": 6429,
      "title": "[Bug]: illegal memory access when increase max_model_length on FP8 models",
      "state": "closed",
      "comments": 21,
      "body_length": 10837,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-07-15 02:21:31+00:00",
      "closed_at": "2025-01-06 02:02:58+00:00",
      "resolution_days": 174.98711805555556
    },
    {
      "number": 6427,
      "title": "[Bug]: Paligemma support for PNG files",
      "state": "closed",
      "comments": 19,
      "body_length": 9109,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-07-15 00:08:09+00:00",
      "closed_at": "2024-07-15 04:25:31+00:00",
      "resolution_days": 0.17872685185185186
    },
    {
      "number": 6416,
      "title": "[Feature]: Apply chat template through `LLM` class",
      "state": "closed",
      "comments": 16,
      "body_length": 2147,
      "label_names": [
        "good first issue",
        "feature request"
      ],
      "created_at": "2024-07-13 16:41:29+00:00",
      "closed_at": "2024-08-16 02:41:37+00:00",
      "resolution_days": 33.41675925925926
    },
    {
      "number": 6370,
      "title": "[Bug]: vLLM 0.5.1 tensor parallel 2 hang",
      "state": "closed",
      "comments": 12,
      "body_length": 11311,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-07-12 11:14:51+00:00",
      "closed_at": "2024-07-15 07:19:33+00:00",
      "resolution_days": 2.836597222222222
    },
    {
      "number": 6368,
      "title": "[Feature]: Request for Ascend NPU support",
      "state": "closed",
      "comments": 14,
      "body_length": 1572,
      "label_names": [
        "feature request",
        "stale"
      ],
      "created_at": "2024-07-12 08:51:07+00:00",
      "closed_at": "2024-12-13 08:04:05+00:00",
      "resolution_days": 153.96733796296297
    },
    {
      "number": 6355,
      "title": "[Installation]: Running ohereForAI/c4ai-command-r-v01 with main pytorch ",
      "state": "closed",
      "comments": 14,
      "body_length": 9717,
      "label_names": [
        "installation",
        "stale"
      ],
      "created_at": "2024-07-11 23:36:28+00:00",
      "closed_at": "2024-11-27 02:06:25+00:00",
      "resolution_days": 138.10413194444445
    },
    {
      "number": 6348,
      "title": "[Feature]: FlashAttention 3 support",
      "state": "closed",
      "comments": 17,
      "body_length": 251,
      "label_names": [
        "help wanted",
        "feature request"
      ],
      "created_at": "2024-07-11 19:11:40+00:00",
      "closed_at": "2025-02-21 16:42:32+00:00",
      "resolution_days": 224.8964351851852
    },
    {
      "number": 6329,
      "title": "[Bug]: Gloo \u5e93\u65e0\u6cd5\u5728\u4e24\u53f0\u8ba1\u7b97\u673a\u4e4b\u95f4\u8fdb\u884c\u901a\u4fe1",
      "state": "closed",
      "comments": 12,
      "body_length": 10096,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-07-11 10:03:50+00:00",
      "closed_at": "2024-11-24 02:08:28+00:00",
      "resolution_days": 135.66988425925925
    },
    {
      "number": 6324,
      "title": "[Usage]: Maximum Context Length Exceeded Due to Base64-Encoded Image in Prompt",
      "state": "closed",
      "comments": 13,
      "body_length": 2161,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-07-11 07:16:22+00:00",
      "closed_at": "2024-07-11 09:23:51+00:00",
      "resolution_days": 0.08853009259259259
    },
    {
      "number": 6308,
      "title": "[Bug]: Gloo Connection reset by peer",
      "state": "closed",
      "comments": 15,
      "body_length": 5663,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-07-10 14:28:48+00:00",
      "closed_at": "2024-07-12 14:10:41+00:00",
      "resolution_days": 1.9874189814814816
    },
    {
      "number": 6299,
      "title": "[Bug]: Vllm 0.5.1+cu118 timeout when init CustomAllreduce",
      "state": "closed",
      "comments": 22,
      "body_length": 10984,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-07-10 09:18:34+00:00",
      "closed_at": "2024-07-12 02:35:18+00:00",
      "resolution_days": 1.7199537037037036
    },
    {
      "number": 6269,
      "title": "f[Bug]: TypeError: Can't instantiate abstract class NeuronWorker with abstract method execute_worker",
      "state": "closed",
      "comments": 20,
      "body_length": 5206,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-07-09 19:50:55+00:00",
      "closed_at": "2024-07-10 23:39:03+00:00",
      "resolution_days": 1.158425925925926
    },
    {
      "number": 6258,
      "title": "[Bug]: tensor parallel (of 4 cards) gives bad answers in version 0.5.1 and later (compared to 0.4.1) with gptq marlin kernels (compared to gptq)",
      "state": "closed",
      "comments": 15,
      "body_length": 3233,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-07-09 10:03:43+00:00",
      "closed_at": "2024-07-27 21:52:35+00:00",
      "resolution_days": 18.492268518518518
    },
    {
      "number": 6250,
      "title": "[Bug]: Load LoRA adaptor for Llama3 seems not working ",
      "state": "closed",
      "comments": 12,
      "body_length": 3196,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-07-09 08:31:31+00:00",
      "closed_at": "2025-05-14 02:10:36+00:00",
      "resolution_days": 308.735474537037
    },
    {
      "number": 6240,
      "title": "[Bug]: Performance : slow inference for FP8 on L20 with 0.5.1(v0.5.0.post1 was fine)",
      "state": "closed",
      "comments": 11,
      "body_length": 2935,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-07-09 03:50:20+00:00",
      "closed_at": "2024-07-15 16:57:10+00:00",
      "resolution_days": 6.546412037037037
    },
    {
      "number": 6228,
      "title": "[Usage]: vllm openai api server never ends in most cases",
      "state": "closed",
      "comments": 12,
      "body_length": 8562,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2024-07-08 23:02:22+00:00",
      "closed_at": "2024-11-25 02:05:05+00:00",
      "resolution_days": 139.12688657407406
    },
    {
      "number": 6226,
      "title": "[RFC] Drop beam search support",
      "state": "closed",
      "comments": 41,
      "body_length": 2915,
      "label_names": [
        "RFC"
      ],
      "created_at": "2024-07-08 22:25:48+00:00",
      "closed_at": "2024-07-22 22:51:50+00:00",
      "resolution_days": 14.018078703703704
    },
    {
      "number": 6225,
      "title": "[Bug]:  benchmark_throughput gets TypeError: XFormersMetadata.__init__() got an unexpected keyword argument 'is_prompt' wit CPU ",
      "state": "closed",
      "comments": 18,
      "body_length": 7168,
      "label_names": [
        "bug",
        "x86-cpu",
        "stale"
      ],
      "created_at": "2024-07-08 21:58:11+00:00",
      "closed_at": "2025-03-14 02:02:55+00:00",
      "resolution_days": 248.1699537037037
    },
    {
      "number": 6224,
      "title": "[Bug]: Error while running inference with LLava 1.6 in v0.5.1 ",
      "state": "closed",
      "comments": 15,
      "body_length": 13450,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-07-08 21:50:31+00:00",
      "closed_at": "2024-07-12 04:29:24+00:00",
      "resolution_days": 3.2770023148148146
    },
    {
      "number": 6220,
      "title": "[Bug]: Gemma2 supports 8192 context with sliding window, but vllm only does 4196 or fails if try 8192",
      "state": "closed",
      "comments": 19,
      "body_length": 16890,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-07-08 19:42:27+00:00",
      "closed_at": "2024-11-23 04:13:56+00:00",
      "resolution_days": 137.35519675925926
    },
    {
      "number": 6201,
      "title": "[Bug]: vLLM 0.5.1 tensor parallel 2 stuck with Mixtral-8x7B-Instruct-v0.1",
      "state": "closed",
      "comments": 16,
      "body_length": 10474,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-07-08 03:10:13+00:00",
      "closed_at": "2024-07-08 05:49:47+00:00",
      "resolution_days": 0.11081018518518519
    },
    {
      "number": 6189,
      "title": "[Feature]: Precise model device placement",
      "state": "closed",
      "comments": 14,
      "body_length": 1705,
      "label_names": [
        "feature request",
        "stale"
      ],
      "created_at": "2024-07-07 14:10:43+00:00",
      "closed_at": "2025-01-04 01:58:36+00:00",
      "resolution_days": 180.49158564814815
    },
    {
      "number": 6179,
      "title": "[Usage]: Struggling to get fp8 inference working correctly on 8xL40s",
      "state": "closed",
      "comments": 18,
      "body_length": 14489,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-07-06 21:28:54+00:00",
      "closed_at": "2024-07-07 13:28:36+00:00",
      "resolution_days": 0.6664583333333334
    },
    {
      "number": 6166,
      "title": "[Bug]: When running gemma2 7b, an error is reported [rank0]: RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasGemmEx( handle, opa, opb, m, n, k, &falpha, a, CUDA_R_16BF, lda, b, CUDA_R_16BF, ldb, &fbeta, c, CUDA_R_16BF, ldc, compute_type, CUBLAS_GEMM_DEFAULT_TENSOR_OP)` Set up according to the prompts:  os.environ['VLLM_ATTENTION_BACKEND'] = 'FLASHINFER'         print(\"Environment variable set for VLLM_ATTENTION_BACKEND:\", os.getenv('VLLM_ATTENTION_BACKEND'))",
      "state": "closed",
      "comments": 18,
      "body_length": 1030,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-07-06 03:56:49+00:00",
      "closed_at": "2025-01-11 02:00:04+00:00",
      "resolution_days": 188.9189236111111
    },
    {
      "number": 6155,
      "title": "[Usage]: How to use Multi-instance in Vllm? (Model replication on multiple GPUs)",
      "state": "closed",
      "comments": 19,
      "body_length": 458,
      "label_names": [
        "usage",
        "unstale"
      ],
      "created_at": "2024-07-05 14:32:33+00:00",
      "closed_at": "2025-03-06 12:47:24+00:00",
      "resolution_days": 243.92697916666665
    },
    {
      "number": 6152,
      "title": "[Bug]: When tensor_parallel_size>1,  RuntimeError: Cannot re-initialize CUDA in forked subprocess.",
      "state": "closed",
      "comments": 16,
      "body_length": 478,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-07-05 10:08:15+00:00",
      "closed_at": "2024-07-08 02:44:38+00:00",
      "resolution_days": 2.6919328703703704
    },
    {
      "number": 6145,
      "title": "[Bug]: When using tp for inference, an error occurs: Worker VllmWorkerProcess pid 3283517 died, exit code: -15.",
      "state": "closed",
      "comments": 17,
      "body_length": 14905,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-07-04 15:42:05+00:00",
      "closed_at": "2024-07-08 01:59:23+00:00",
      "resolution_days": 3.4286805555555557
    },
    {
      "number": 6142,
      "title": "[Feature]: deepseek-v2 awq support",
      "state": "closed",
      "comments": 11,
      "body_length": 754,
      "label_names": [
        "feature request",
        "stale"
      ],
      "created_at": "2024-07-04 14:31:41+00:00",
      "closed_at": "2025-03-01 02:06:00+00:00",
      "resolution_days": 239.48216435185185
    },
    {
      "number": 6126,
      "title": "[Bug]: RuntimeError: No suitable kernel. h_in=16 h_out=7392 dtype=Float out_dtype=BFloat16",
      "state": "closed",
      "comments": 11,
      "body_length": 6919,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-07-04 02:56:10+00:00",
      "closed_at": "2024-08-02 15:28:01+00:00",
      "resolution_days": 29.522118055555556
    },
    {
      "number": 6103,
      "title": "[Bug]: fused_moe_kernel compile bug",
      "state": "closed",
      "comments": 13,
      "body_length": 9540,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-07-03 13:19:58+00:00",
      "closed_at": "2024-07-15 17:12:48+00:00",
      "resolution_days": 12.161689814814816
    },
    {
      "number": 6077,
      "title": "[RFC]: Priority Scheduling",
      "state": "closed",
      "comments": 20,
      "body_length": 3392,
      "label_names": [
        "RFC"
      ],
      "created_at": "2024-07-02 19:27:53+00:00",
      "closed_at": "2024-09-25 02:50:51+00:00",
      "resolution_days": 84.30761574074074
    },
    {
      "number": 6042,
      "title": "[Bug]: call for stack trace for \"Watchdog caught collective operation timeout\"",
      "state": "closed",
      "comments": 11,
      "body_length": 17913,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-07-01 17:39:55+00:00",
      "closed_at": "2025-01-25 01:55:04+00:00",
      "resolution_days": 207.34385416666666
    },
    {
      "number": 5934,
      "title": "[New Model]: Florence-2",
      "state": "closed",
      "comments": 20,
      "body_length": 219,
      "label_names": [
        "new-model",
        "unstale"
      ],
      "created_at": "2024-06-27 21:11:37+00:00",
      "closed_at": "2025-02-27 10:06:43+00:00",
      "resolution_days": 244.53826388888888
    },
    {
      "number": 5924,
      "title": "[Bug]: Model \"talking to itself\" and ignoring `<|im_end|>`",
      "state": "closed",
      "comments": 13,
      "body_length": 5264,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-06-27 18:53:18+00:00",
      "closed_at": "2024-06-27 21:15:05+00:00",
      "resolution_days": 0.09846064814814814
    },
    {
      "number": 5867,
      "title": "IP Bind Error on v0.5.0.post1 ",
      "state": "closed",
      "comments": 15,
      "body_length": 875,
      "label_names": [],
      "created_at": "2024-06-26 17:25:32+00:00",
      "closed_at": "2024-06-26 21:57:13+00:00",
      "resolution_days": 0.18866898148148148
    },
    {
      "number": 5863,
      "title": "[Bug]: A bug when running examples/llava_example.py with image_features as input and multiple GPUs enabled",
      "state": "closed",
      "comments": 14,
      "body_length": 19628,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-06-26 10:50:58+00:00",
      "closed_at": "2024-06-27 07:15:26+00:00",
      "resolution_days": 0.8503240740740741
    },
    {
      "number": 5814,
      "title": "[Bug]: Test_skip_speculation fails in distributed execution",
      "state": "closed",
      "comments": 11,
      "body_length": 5464,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-06-25 06:34:32+00:00",
      "closed_at": "2024-11-24 02:09:29+00:00",
      "resolution_days": 151.8159375
    },
    {
      "number": 5806,
      "title": "v0.5.1 Release Tracker",
      "state": "closed",
      "comments": 15,
      "body_length": 544,
      "label_names": [
        "release"
      ],
      "created_at": "2024-06-25 00:18:07+00:00",
      "closed_at": "2024-07-06 04:55:52+00:00",
      "resolution_days": 11.192881944444444
    },
    {
      "number": 5805,
      "title": "[Roadmap] vLLM Roadmap Q3 2024",
      "state": "closed",
      "comments": 42,
      "body_length": 4449,
      "label_names": [],
      "created_at": "2024-06-25 00:08:09+00:00",
      "closed_at": "2024-10-01 17:45:19+00:00",
      "resolution_days": 98.73414351851852
    },
    {
      "number": 5790,
      "title": "[Bug]: please set tensor_parallel_size to less than max local gpu count",
      "state": "closed",
      "comments": 19,
      "body_length": 6601,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-06-24 15:42:13+00:00",
      "closed_at": "2024-06-25 17:58:21+00:00",
      "resolution_days": 1.094537037037037
    },
    {
      "number": 5736,
      "title": "[Bug]: which torchvision version required",
      "state": "closed",
      "comments": 12,
      "body_length": 8183,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-06-21 09:26:04+00:00",
      "closed_at": "2024-06-24 04:11:54+00:00",
      "resolution_days": 2.7818287037037037
    },
    {
      "number": 5734,
      "title": "[Feature]: Support for OpenAIEmbeddings with Langchain",
      "state": "closed",
      "comments": 14,
      "body_length": 960,
      "label_names": [
        "good first issue",
        "feature request"
      ],
      "created_at": "2024-06-21 08:59:28+00:00",
      "closed_at": "2024-06-30 15:53:53+00:00",
      "resolution_days": 9.287789351851853
    },
    {
      "number": 5723,
      "title": "[RFC]: Add runtime weight update API",
      "state": "closed",
      "comments": 23,
      "body_length": 1587,
      "label_names": [
        "RFC",
        "unstale"
      ],
      "created_at": "2024-06-20 20:22:40+00:00",
      "closed_at": "2025-01-17 02:00:23+00:00",
      "resolution_days": 210.23452546296295
    },
    {
      "number": 5716,
      "title": "[RFC]: proper resource cleanup for LLM class with file-like usage",
      "state": "closed",
      "comments": 12,
      "body_length": 1428,
      "label_names": [
        "RFC",
        "stale"
      ],
      "created_at": "2024-06-20 16:54:35+00:00",
      "closed_at": "2024-11-25 02:05:27+00:00",
      "resolution_days": 157.38254629629628
    },
    {
      "number": 5713,
      "title": "[Bug]:  \"Triton Error [CUDA]: device kernel image is invalid\" when loading Mixtral-8x7B-Instruct-v0.1 in fused_moe.py",
      "state": "closed",
      "comments": 18,
      "body_length": 15279,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-06-20 15:12:52+00:00",
      "closed_at": "2025-03-21 02:05:57+00:00",
      "resolution_days": 273.4535300925926
    },
    {
      "number": 5693,
      "title": "[Bug]: vision chat completion output with odd Instruction/Output prompting.",
      "state": "closed",
      "comments": 23,
      "body_length": 54809,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-06-19 21:12:05+00:00",
      "closed_at": "2024-06-25 06:57:56+00:00",
      "resolution_days": 5.4068402777777775
    },
    {
      "number": 5674,
      "title": "[Installation]: pip install -e failed",
      "state": "closed",
      "comments": 13,
      "body_length": 1827,
      "label_names": [
        "installation",
        "stale"
      ],
      "created_at": "2024-06-19 06:45:04+00:00",
      "closed_at": "2024-11-12 14:07:37+00:00",
      "resolution_days": 146.3073263888889
    },
    {
      "number": 5657,
      "title": "[Bug]: Ray distributed backend does not support out-of-tree models via ModelRegistry APIs",
      "state": "closed",
      "comments": 19,
      "body_length": 4911,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-06-18 20:41:39+00:00",
      "closed_at": "2024-08-13 23:25:27+00:00",
      "resolution_days": 56.11375
    },
    {
      "number": 5640,
      "title": "[Installation]: vllm on NVIDIA jetson AGX orin",
      "state": "closed",
      "comments": 19,
      "body_length": 6037,
      "label_names": [
        "installation",
        "stale"
      ],
      "created_at": "2024-06-18 14:25:37+00:00",
      "closed_at": "2025-02-25 02:03:40+00:00",
      "resolution_days": 251.48475694444446
    },
    {
      "number": 5637,
      "title": "[Bug]: RuntimeError with tensor_parallel_size > 1 in Process Bootstrapping Phase",
      "state": "closed",
      "comments": 18,
      "body_length": 15387,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-06-18 11:27:53+00:00",
      "closed_at": "2024-06-21 00:06:35+00:00",
      "resolution_days": 2.526875
    },
    {
      "number": 5589,
      "title": "[Usage]: 'InternVLChatConfig' object has no attribute 'num_attention_heads'",
      "state": "closed",
      "comments": 11,
      "body_length": 6710,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-06-17 05:53:46+00:00",
      "closed_at": "2024-10-18 08:16:41+00:00",
      "resolution_days": 123.09924768518519
    },
    {
      "number": 5587,
      "title": "[Installation]: `ModuleNotFoundError: No module named 'numpy.lib.function_base'` due to NumPy 2.0 release",
      "state": "closed",
      "comments": 18,
      "body_length": 2970,
      "label_names": [
        "installation",
        "stale"
      ],
      "created_at": "2024-06-17 05:42:12+00:00",
      "closed_at": "2025-04-28 02:15:43+00:00",
      "resolution_days": 314.8566087962963
    },
    {
      "number": 5569,
      "title": "[Bug]: BitsandBytes quantization is not working as expected",
      "state": "closed",
      "comments": 32,
      "body_length": 12964,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-06-15 14:20:10+00:00",
      "closed_at": "2024-07-27 02:08:24+00:00",
      "resolution_days": 41.4918287037037
    },
    {
      "number": 5566,
      "title": "[Usage]: how to use marlin kernel for GPTQ model",
      "state": "closed",
      "comments": 11,
      "body_length": 4217,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2024-06-15 05:15:40+00:00",
      "closed_at": "2024-11-25 02:06:00+00:00",
      "resolution_days": 162.86828703703705
    },
    {
      "number": 5563,
      "title": "[Bug]: Speculative decoding server: `ValueError: could not broadcast input array from shape (513,) into shape (512,)`",
      "state": "closed",
      "comments": 18,
      "body_length": 12784,
      "label_names": [
        "bug",
        "speculative-decoding"
      ],
      "created_at": "2024-06-14 22:54:56+00:00",
      "closed_at": "2024-09-10 23:02:43+00:00",
      "resolution_days": 88.0054050925926
    },
    {
      "number": 5557,
      "title": "[RFC]: Implement disaggregated prefilling via KV cache transfer",
      "state": "closed",
      "comments": 19,
      "body_length": 1842,
      "label_names": [
        "RFC",
        "stale"
      ],
      "created_at": "2024-06-14 21:10:33+00:00",
      "closed_at": "2025-06-21 02:18:30+00:00",
      "resolution_days": 371.2138541666667
    },
    {
      "number": 5552,
      "title": "[RFC]: Refactor Worker and ModelRunner to consolidate control plane communication",
      "state": "closed",
      "comments": 13,
      "body_length": 6727,
      "label_names": [
        "RFC"
      ],
      "created_at": "2024-06-14 18:55:42+00:00",
      "closed_at": "2024-06-27 20:30:39+00:00",
      "resolution_days": 13.0659375
    },
    {
      "number": 5537,
      "title": "[Bug]: CUDA illegal memory access error when `enable_prefix_caching=True`",
      "state": "closed",
      "comments": 20,
      "body_length": 19892,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-06-14 10:50:07+00:00",
      "closed_at": "2025-03-15 02:02:04+00:00",
      "resolution_days": 273.6332986111111
    },
    {
      "number": 5533,
      "title": "[Usage]: how to use enable-chunked-prefill?",
      "state": "closed",
      "comments": 16,
      "body_length": 4352,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2024-06-14 07:13:44+00:00",
      "closed_at": "2025-01-28 03:42:22+00:00",
      "resolution_days": 227.85321759259259
    },
    {
      "number": 5501,
      "title": "[Bug]:  ailed to import from vllm._C with ImportError('/usr/local/lib/python3.8/dist-packages/vllm/_C.abi3.so: undefined symbol: _ZN5torch7LibraryC1ENS0_4KindESsSt8optionalIN3c1011DispatchKeyEEPKcj')",
      "state": "closed",
      "comments": 26,
      "body_length": 705,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-06-13 11:42:58+00:00",
      "closed_at": "2025-01-23 01:58:50+00:00",
      "resolution_days": 223.59435185185185
    },
    {
      "number": 5496,
      "title": "[Bug]: Qwen/Qwen2-72B-Instruct 128k server down ",
      "state": "closed",
      "comments": 12,
      "body_length": 10993,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-06-13 08:23:36+00:00",
      "closed_at": "2024-12-08 02:11:07+00:00",
      "resolution_days": 177.7413310185185
    },
    {
      "number": 5493,
      "title": "[Usage]: How do I get the FP8 scaling factors for KV cache?",
      "state": "closed",
      "comments": 11,
      "body_length": 809,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-06-13 08:02:45+00:00",
      "closed_at": "2024-06-18 06:41:20+00:00",
      "resolution_days": 4.9434606481481485
    },
    {
      "number": 5491,
      "title": "[Feature]: load/unload API to run multiple LLMs in a single GPU instance",
      "state": "closed",
      "comments": 11,
      "body_length": 1382,
      "label_names": [
        "feature request"
      ],
      "created_at": "2024-06-13 07:49:07+00:00",
      "closed_at": "2025-03-22 05:09:52+00:00",
      "resolution_days": 281.88940972222224
    },
    {
      "number": 5484,
      "title": "[Bug]: NCCL hangs and causes timeout",
      "state": "closed",
      "comments": 26,
      "body_length": 7654,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-06-13 03:03:11+00:00",
      "closed_at": "2024-11-11 23:21:13+00:00",
      "resolution_days": 151.8458564814815
    },
    {
      "number": 5480,
      "title": "[Usage]: Can I use vllm.LLM(quantization=\"bitsandbytes\"...) when bitsandbytes is supported in the v0.5.0 version",
      "state": "closed",
      "comments": 12,
      "body_length": 6054,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2024-06-13 01:08:48+00:00",
      "closed_at": "2024-12-21 01:59:20+00:00",
      "resolution_days": 191.0350925925926
    },
    {
      "number": 5479,
      "title": "[Bug]: Loading Mixtral-8x22B-Instruct-v0.1-FP8 on 8xL40S causes a SIGSEGV ",
      "state": "closed",
      "comments": 15,
      "body_length": 27400,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-06-13 01:01:38+00:00",
      "closed_at": "2024-12-28 01:59:50+00:00",
      "resolution_days": 198.04041666666666
    },
    {
      "number": 5454,
      "title": "[Bug]: 0.5.0 AttributeError: '_OpNamespace' '_C' object has no attribute 'rms_norm'",
      "state": "closed",
      "comments": 21,
      "body_length": 12661,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-06-12 12:16:47+00:00",
      "closed_at": "2024-06-12 13:03:51+00:00",
      "resolution_days": 0.032685185185185185
    },
    {
      "number": 5448,
      "title": "[Bug]: AttributeError: '_OpNamespace' '_C_cache_ops' object has no attribute 'reshape_and_cache_flash'",
      "state": "closed",
      "comments": 13,
      "body_length": 15249,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-06-12 10:06:54+00:00",
      "closed_at": "2024-06-12 21:47:49+00:00",
      "resolution_days": 0.4867476851851852
    },
    {
      "number": 5436,
      "title": "[Bug]: get the degree of the `outlines FSM` compilation progress from vlllm0.5.0 engine (via a route)",
      "state": "closed",
      "comments": 17,
      "body_length": 7315,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-06-12 02:41:59+00:00",
      "closed_at": "2024-07-08 18:23:25+00:00",
      "resolution_days": 26.653773148148147
    },
    {
      "number": 5423,
      "title": "[RFC]: Improve guided decoding (logit_processor) APIs and performance.",
      "state": "closed",
      "comments": 16,
      "body_length": 3814,
      "label_names": [
        "structured-output",
        "RFC",
        "stale"
      ],
      "created_at": "2024-06-11 14:59:31+00:00",
      "closed_at": "2025-04-17 08:52:29+00:00",
      "resolution_days": 309.74511574074074
    },
    {
      "number": 5405,
      "title": "[Bug]:The vllm service takes two hours to start Because of NCCL",
      "state": "closed",
      "comments": 27,
      "body_length": 29723,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-06-11 03:54:51+00:00",
      "closed_at": "2024-06-17 08:22:36+00:00",
      "resolution_days": 6.1859375
    },
    {
      "number": 5376,
      "title": "0.4.3 error CUDA error: an illegal memory access was encountered",
      "state": "closed",
      "comments": 36,
      "body_length": 35003,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-06-10 09:06:31+00:00",
      "closed_at": "2024-09-27 04:27:54+00:00",
      "resolution_days": 108.80651620370371
    },
    {
      "number": 5360,
      "title": "[Bug]: Multi GPU setup for VLLM in Openshift still does not work ",
      "state": "closed",
      "comments": 19,
      "body_length": 8888,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-06-09 00:50:12+00:00",
      "closed_at": "2024-11-27 02:07:20+00:00",
      "resolution_days": 171.05356481481482
    },
    {
      "number": 5337,
      "title": "[Bug]: non-deterministic Python gc order leads to flaky tests",
      "state": "closed",
      "comments": 13,
      "body_length": 2889,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-06-07 05:39:26+00:00",
      "closed_at": "2024-06-08 05:31:33+00:00",
      "resolution_days": 0.994525462962963
    },
    {
      "number": 5306,
      "title": "[New Model] GLM-4-9B-Chat",
      "state": "closed",
      "comments": 18,
      "body_length": 202,
      "label_names": [
        "new-model",
        "unstale"
      ],
      "created_at": "2024-06-06 03:57:39+00:00",
      "closed_at": "2024-11-28 14:53:33+00:00",
      "resolution_days": 175.4554861111111
    },
    {
      "number": 5298,
      "title": "[Bug]: After fine-tuning Qwen Lora, the inference results differ when using VLLM and Hugging Face to load",
      "state": "closed",
      "comments": 15,
      "body_length": 399,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-06-06 01:00:38+00:00",
      "closed_at": "2024-11-28 02:05:00+00:00",
      "resolution_days": 175.04469907407406
    },
    {
      "number": 5264,
      "title": "[Bug]: prompt_logprobs doesn't work with openai compatible server",
      "state": "closed",
      "comments": 12,
      "body_length": 5971,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-06-05 01:04:33+00:00",
      "closed_at": "2024-06-13 08:53:47+00:00",
      "resolution_days": 8.325856481481482
    },
    {
      "number": 5232,
      "title": "[Feature]: vllm-flash-attn cu118 compatibility ",
      "state": "closed",
      "comments": 11,
      "body_length": 870,
      "label_names": [
        "feature request",
        "stale"
      ],
      "created_at": "2024-06-03 22:25:16+00:00",
      "closed_at": "2024-11-01 18:39:09+00:00",
      "resolution_days": 150.84297453703704
    },
    {
      "number": 5199,
      "title": "[Bug]: Issues with Applying LoRA in vllm on a T4 GPU",
      "state": "closed",
      "comments": 14,
      "body_length": 16014,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-06-02 16:08:05+00:00",
      "closed_at": "2024-06-04 18:32:47+00:00",
      "resolution_days": 2.100486111111111
    },
    {
      "number": 5162,
      "title": "[Bug]: Unable to Use Prefix Caching in AsyncLLMEngine",
      "state": "closed",
      "comments": 15,
      "body_length": 89528,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-05-31 17:26:15+00:00",
      "closed_at": "2024-11-27 02:07:44+00:00",
      "resolution_days": 179.36214120370371
    },
    {
      "number": 5161,
      "title": "[Bug]:  WSL2(also Docker)  1 GPU work but 2 not,(--tensor-parallel-size 2 )",
      "state": "closed",
      "comments": 12,
      "body_length": 12890,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-05-31 17:11:49+00:00",
      "closed_at": "2024-10-14 14:34:53+00:00",
      "resolution_days": 135.8910185185185
    },
    {
      "number": 5086,
      "title": "[Installation]: Error when importing LLM from vllm",
      "state": "closed",
      "comments": 18,
      "body_length": 280,
      "label_names": [
        "installation",
        "stale"
      ],
      "created_at": "2024-05-28 10:44:11+00:00",
      "closed_at": "2025-03-25 03:04:48+00:00",
      "resolution_days": 300.6809837962963
    },
    {
      "number": 5084,
      "title": "[Bug]: The vllm is disconnected after running for some time",
      "state": "closed",
      "comments": 29,
      "body_length": 4564,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-05-28 09:00:02+00:00",
      "closed_at": "2024-07-24 00:08:43+00:00",
      "resolution_days": 56.631030092592596
    },
    {
      "number": 5082,
      "title": "curl http://localhost:8000/generate {\"detail\":\"Not Found\"}[Usage] generate relu can not ues",
      "state": "closed",
      "comments": 41,
      "body_length": 1172,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-05-28 07:39:35+00:00",
      "closed_at": "2024-06-13 08:59:10+00:00",
      "resolution_days": 16.055266203703702
    },
    {
      "number": 5067,
      "title": "[Bug]: The VRAM usage of calculating log_probs is not considered in profile run",
      "state": "closed",
      "comments": 11,
      "body_length": 7213,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-05-27 07:36:26+00:00",
      "closed_at": "2024-11-25 02:06:03+00:00",
      "resolution_days": 181.77056712962963
    },
    {
      "number": 5001,
      "title": "[Bug]: 0.4.2 error on H20",
      "state": "closed",
      "comments": 15,
      "body_length": 15462,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-05-23 07:59:56+00:00",
      "closed_at": "2025-01-02 02:00:00+00:00",
      "resolution_days": 223.7500462962963
    },
    {
      "number": 4995,
      "title": "[Feature]: Chunked prefill + lora",
      "state": "closed",
      "comments": 13,
      "body_length": 414,
      "label_names": [
        "feature request",
        "stale"
      ],
      "created_at": "2024-05-23 01:12:17+00:00",
      "closed_at": "2025-04-02 02:06:40+00:00",
      "resolution_days": 314.0377662037037
    },
    {
      "number": 4989,
      "title": "[Bug]: Loading mistral-7B-instruct-v03 KeyError: 'layers.0.attention.wk.weight'",
      "state": "closed",
      "comments": 25,
      "body_length": 11979,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-05-22 18:52:58+00:00",
      "closed_at": "2024-05-24 13:38:02+00:00",
      "resolution_days": 1.7812962962962964
    },
    {
      "number": 4917,
      "title": "[Performance]: Automatic Prefix Caching in multi-turn conversations",
      "state": "closed",
      "comments": 17,
      "body_length": 3224,
      "label_names": [
        "performance"
      ],
      "created_at": "2024-05-20 10:52:38+00:00",
      "closed_at": "2024-05-23 18:09:28+00:00",
      "resolution_days": 3.3033564814814813
    },
    {
      "number": 4915,
      "title": "[Bug]: Phi3 lora module not loading",
      "state": "closed",
      "comments": 19,
      "body_length": 443,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-05-20 08:42:05+00:00",
      "closed_at": "2025-02-27 02:01:53+00:00",
      "resolution_days": 282.72208333333333
    },
    {
      "number": 4913,
      "title": "[Installation]: Failed building editable for vllm",
      "state": "closed",
      "comments": 15,
      "body_length": 17464,
      "label_names": [
        "installation"
      ],
      "created_at": "2024-05-20 07:14:29+00:00",
      "closed_at": "2024-09-19 12:18:37+00:00",
      "resolution_days": 122.2112037037037
    },
    {
      "number": 4906,
      "title": "[Bug]: Cannot use FlashAttention-2 backend because the flash_attn package is not found",
      "state": "closed",
      "comments": 15,
      "body_length": 358,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-05-19 09:27:08+00:00",
      "closed_at": "2024-09-06 10:04:56+00:00",
      "resolution_days": 110.02625
    },
    {
      "number": 4895,
      "title": "v0.4.3 Release Tracker",
      "state": "closed",
      "comments": 16,
      "body_length": 156,
      "label_names": [
        "release"
      ],
      "created_at": "2024-05-18 01:05:16+00:00",
      "closed_at": "2024-06-03 17:04:07+00:00",
      "resolution_days": 16.665868055555556
    },
    {
      "number": 4872,
      "title": "[Bug]: Shape error encountered in speculative decoding when `enable_lora=True`",
      "state": "closed",
      "comments": 12,
      "body_length": 15605,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-05-17 01:10:51+00:00",
      "closed_at": "2024-11-27 02:08:18+00:00",
      "resolution_days": 194.03989583333333
    },
    {
      "number": 4831,
      "title": "[Bug]: llava inference result is wrong !",
      "state": "closed",
      "comments": 23,
      "body_length": 11466,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-05-15 11:58:04+00:00",
      "closed_at": "2024-05-17 12:48:40+00:00",
      "resolution_days": 2.035138888888889
    },
    {
      "number": 4784,
      "title": "[Bug]: deploy Phi-3-mini-128k-instruct AssertionError",
      "state": "closed",
      "comments": 11,
      "body_length": 6895,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-05-13 10:05:43+00:00",
      "closed_at": "2024-10-22 02:50:01+00:00",
      "resolution_days": 161.69743055555554
    },
    {
      "number": 4771,
      "title": "[Feature]: Host CPU Docker image on Docker Hub",
      "state": "closed",
      "comments": 14,
      "body_length": 495,
      "label_names": [
        "feature request"
      ],
      "created_at": "2024-05-12 10:45:45+00:00",
      "closed_at": "2024-12-19 19:46:56+00:00",
      "resolution_days": 221.37582175925925
    },
    {
      "number": 4770,
      "title": "[Feature]: CI: Test on NVLink-enabled machine",
      "state": "closed",
      "comments": 18,
      "body_length": 517,
      "label_names": [
        "feature request"
      ],
      "created_at": "2024-05-12 03:09:13+00:00",
      "closed_at": "2024-06-19 21:36:15+00:00",
      "resolution_days": 38.76877314814815
    },
    {
      "number": 4667,
      "title": "openai.BadRequestError: Error code: 400 - {'object': 'error', 'message': 'max_tokens must be at least 1, got -186.', 'type': 'BadRequestError', 'param': None, 'code': 400}",
      "state": "closed",
      "comments": 17,
      "body_length": 4610,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-05-08 02:29:17+00:00",
      "closed_at": "2024-05-31 18:56:40+00:00",
      "resolution_days": 23.68568287037037
    },
    {
      "number": 4639,
      "title": "[Bug]: with `worker_use_ray = true`, and tensor_parallel_size > 1, the process is pending forever",
      "state": "closed",
      "comments": 15,
      "body_length": 5668,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-05-07 01:52:44+00:00",
      "closed_at": "2024-11-28 02:05:22+00:00",
      "resolution_days": 205.00877314814815
    },
    {
      "number": 4630,
      "title": "[Speculative decoding] [Help wanted] [Performance] Optimize draft-model speculative decoding",
      "state": "closed",
      "comments": 28,
      "body_length": 2389,
      "label_names": [
        "help wanted",
        "performance",
        "speculative-decoding"
      ],
      "created_at": "2024-05-06 17:16:50+00:00",
      "closed_at": "2024-08-05 18:05:31+00:00",
      "resolution_days": 91.03380787037037
    },
    {
      "number": 4625,
      "title": "[Feature]:  MLA Support",
      "state": "closed",
      "comments": 14,
      "body_length": 875,
      "label_names": [
        "feature request",
        "stale"
      ],
      "created_at": "2024-05-06 15:09:35+00:00",
      "closed_at": "2024-12-19 02:05:08+00:00",
      "resolution_days": 226.45524305555554
    },
    {
      "number": 4608,
      "title": "[Usage]: Cannot run the starter code in tutorial",
      "state": "closed",
      "comments": 14,
      "body_length": 13743,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-05-05 20:12:07+00:00",
      "closed_at": "2024-05-09 22:13:25+00:00",
      "resolution_days": 4.084236111111111
    },
    {
      "number": 4587,
      "title": "[Bug]: vllm 0.4.1 crashing after checking P2P status on single GPU ",
      "state": "closed",
      "comments": 14,
      "body_length": 9675,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-05-03 16:53:25+00:00",
      "closed_at": "2024-11-28 02:05:30+00:00",
      "resolution_days": 208.3833912037037
    },
    {
      "number": 4569,
      "title": "[CI][Contribution Welcomed] Conditional Testing",
      "state": "closed",
      "comments": 12,
      "body_length": 409,
      "label_names": [
        "help wanted",
        "good first issue",
        "stale"
      ],
      "created_at": "2024-05-02 23:15:11+00:00",
      "closed_at": "2024-10-28 03:06:34+00:00",
      "resolution_days": 178.16068287037038
    },
    {
      "number": 4565,
      "title": "[RFC]: Automate Speculative Decoding",
      "state": "closed",
      "comments": 20,
      "body_length": 2927,
      "label_names": [
        "RFC",
        "stale"
      ],
      "created_at": "2024-05-02 18:48:03+00:00",
      "closed_at": "2025-03-20 02:04:35+00:00",
      "resolution_days": 321.3031481481481
    },
    {
      "number": 4532,
      "title": "[RFC]: Refactor FP8 kv-cache",
      "state": "closed",
      "comments": 11,
      "body_length": 6047,
      "label_names": [
        "RFC"
      ],
      "created_at": "2024-05-01 16:58:30+00:00",
      "closed_at": "2024-08-06 15:03:47+00:00",
      "resolution_days": 96.92033564814815
    },
    {
      "number": 4515,
      "title": "[Feature]: FP6",
      "state": "closed",
      "comments": 17,
      "body_length": 409,
      "label_names": [
        "feature request",
        "stale"
      ],
      "created_at": "2024-05-01 06:52:00+00:00",
      "closed_at": "2025-01-25 01:55:07+00:00",
      "resolution_days": 268.7938310185185
    },
    {
      "number": 4514,
      "title": "[Bug]: For RDNA3 (navi31; gfx1100) VLLM_USE_TRITON_FLASH_ATTN=0 currently must be forced",
      "state": "closed",
      "comments": 17,
      "body_length": 6350,
      "label_names": [
        "bug",
        "rocm",
        "stale"
      ],
      "created_at": "2024-05-01 05:44:22+00:00",
      "closed_at": "2024-12-08 10:04:32+00:00",
      "resolution_days": 221.1806712962963
    },
    {
      "number": 4505,
      "title": "v0.4.2 Release Tracker",
      "state": "closed",
      "comments": 12,
      "body_length": 25,
      "label_names": [
        "release"
      ],
      "created_at": "2024-04-30 17:28:48+00:00",
      "closed_at": "2024-05-05 07:20:30+00:00",
      "resolution_days": 4.577569444444444
    },
    {
      "number": 4462,
      "title": "[Usage]: How do you setup vllm to work in k8s/openshift cluster",
      "state": "closed",
      "comments": 12,
      "body_length": 7962,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2024-04-29 23:47:42+00:00",
      "closed_at": "2025-02-21 02:00:34+00:00",
      "resolution_days": 297.0922685185185
    },
    {
      "number": 4430,
      "title": "[Bug]: Engine iteration timed out. This should never happen!",
      "state": "closed",
      "comments": 31,
      "body_length": 28499,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-04-28 09:32:58+00:00",
      "closed_at": "2024-05-23 09:26:27+00:00",
      "resolution_days": 24.995474537037037
    },
    {
      "number": 4410,
      "title": "[Doc]: Offline Inference Distributed Broken for TP",
      "state": "closed",
      "comments": 15,
      "body_length": 1071,
      "label_names": [
        "documentation"
      ],
      "created_at": "2024-04-27 05:46:00+00:00",
      "closed_at": "2024-05-17 17:52:12+00:00",
      "resolution_days": 20.504305555555554
    },
    {
      "number": 4407,
      "title": "[RFC]: environment variable management in vllm",
      "state": "closed",
      "comments": 13,
      "body_length": 3415,
      "label_names": [
        "RFC"
      ],
      "created_at": "2024-04-27 01:05:22+00:00",
      "closed_at": "2024-05-04 18:17:45+00:00",
      "resolution_days": 7.71693287037037
    },
    {
      "number": 4399,
      "title": "[Bug]: TypeError in XFormersMetadata ",
      "state": "closed",
      "comments": 11,
      "body_length": 5458,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-04-26 14:01:02+00:00",
      "closed_at": "2025-04-15 10:18:38+00:00",
      "resolution_days": 353.84555555555556
    },
    {
      "number": 4393,
      "title": "[Model]: Support for InternVL-Chat-V1-5",
      "state": "closed",
      "comments": 14,
      "body_length": 318,
      "label_names": [
        "new-model"
      ],
      "created_at": "2024-04-26 09:42:45+00:00",
      "closed_at": "2024-07-29 11:06:00+00:00",
      "resolution_days": 94.0578125
    },
    {
      "number": 4381,
      "title": "[Bug]: Chunked prefill doesn't seem to work when --kv-cache-dtype fp8",
      "state": "closed",
      "comments": 11,
      "body_length": 260,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-04-26 01:46:04+00:00",
      "closed_at": "2024-08-12 22:47:43+00:00",
      "resolution_days": 108.87614583333334
    },
    {
      "number": 4375,
      "title": "[Bug]: Phi3 still not supported",
      "state": "closed",
      "comments": 16,
      "body_length": 18247,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-04-25 20:20:31+00:00",
      "closed_at": "2024-06-27 14:58:31+00:00",
      "resolution_days": 62.77638888888889
    },
    {
      "number": 4359,
      "title": "[Feature]: GPTQ/AWQ quantization is not fully optimized yet. The speed can be slower than non-quantized models.",
      "state": "closed",
      "comments": 15,
      "body_length": 537,
      "label_names": [
        "feature request"
      ],
      "created_at": "2024-04-25 07:57:30+00:00",
      "closed_at": "2024-07-25 19:51:53+00:00",
      "resolution_days": 91.49609953703704
    },
    {
      "number": 4323,
      "title": "[Bug]: phi-3 (microsoft/Phi-3-mini-128k-instruct) fails with assert \"factor\" in rope_scaling",
      "state": "closed",
      "comments": 16,
      "body_length": 3643,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-04-24 06:31:06+00:00",
      "closed_at": "2024-04-25 08:39:07+00:00",
      "resolution_days": 1.088900462962963
    },
    {
      "number": 4322,
      "title": "[Usage]: Flash Attention not working any more",
      "state": "closed",
      "comments": 18,
      "body_length": 4866,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2024-04-24 06:06:26+00:00",
      "closed_at": "2024-12-26 01:59:53+00:00",
      "resolution_days": 245.82878472222222
    },
    {
      "number": 4313,
      "title": "[Installation]: Compile and Install from source",
      "state": "closed",
      "comments": 13,
      "body_length": 6580,
      "label_names": [
        "installation",
        "stale"
      ],
      "created_at": "2024-04-24 02:05:22+00:00",
      "closed_at": "2024-11-29 02:06:27+00:00",
      "resolution_days": 219.0007523148148
    },
    {
      "number": 4312,
      "title": "[Bug]: NameError: name 'ncclGetVersion' is not defined (or Failed to import NCCL library: Cannot find libnccl.so.2 in the system.)",
      "state": "closed",
      "comments": 32,
      "body_length": 17413,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-04-24 02:05:17+00:00",
      "closed_at": "2024-04-24 16:05:01+00:00",
      "resolution_days": 0.5831481481481482
    },
    {
      "number": 4277,
      "title": "[Bug]: vllm stall on llama3-70b warmup with 0.4.1",
      "state": "closed",
      "comments": 19,
      "body_length": 5192,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-04-22 22:41:13+00:00",
      "closed_at": "2024-06-13 09:02:35+00:00",
      "resolution_days": 51.43150462962963
    },
    {
      "number": 4263,
      "title": "[Bug]: offline test, Process hangs without exiting when using cuda graph",
      "state": "closed",
      "comments": 17,
      "body_length": 7166,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-04-22 09:53:12+00:00",
      "closed_at": "2024-05-09 12:37:47+00:00",
      "resolution_days": 17.11429398148148
    },
    {
      "number": 4257,
      "title": "[Bug]: name 'ncclGetVersion' is not defined",
      "state": "closed",
      "comments": 13,
      "body_length": 9101,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-04-22 08:08:05+00:00",
      "closed_at": "2024-04-23 00:21:49+00:00",
      "resolution_days": 0.6762037037037038
    },
    {
      "number": 4255,
      "title": "[Doc]: How is the support for chunked prefill?",
      "state": "closed",
      "comments": 12,
      "body_length": 166,
      "label_names": [
        "documentation",
        "stale"
      ],
      "created_at": "2024-04-22 07:28:15+00:00",
      "closed_at": "2024-11-29 02:06:36+00:00",
      "resolution_days": 220.77663194444443
    },
    {
      "number": 4229,
      "title": "[Bug]: NameError: name 'vllm_ops' is not defined",
      "state": "closed",
      "comments": 13,
      "body_length": 13359,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-04-21 00:43:20+00:00",
      "closed_at": "2024-05-31 07:07:46+00:00",
      "resolution_days": 40.26696759259259
    },
    {
      "number": 4212,
      "title": "[Speculative decoding] [Performance]: Re-enable bonus tokens",
      "state": "closed",
      "comments": 12,
      "body_length": 1131,
      "label_names": [
        "performance"
      ],
      "created_at": "2024-04-19 18:53:39+00:00",
      "closed_at": "2024-07-10 23:02:49+00:00",
      "resolution_days": 82.1730324074074
    },
    {
      "number": 4201,
      "title": "[Installation]: Failed to build form source code. Python=3.9 CUDA=12.1",
      "state": "closed",
      "comments": 15,
      "body_length": 22941,
      "label_names": [
        "installation",
        "stale"
      ],
      "created_at": "2024-04-19 09:25:43+00:00",
      "closed_at": "2025-01-19 02:02:06+00:00",
      "resolution_days": 274.69193287037035
    },
    {
      "number": 4193,
      "title": "[Bug]: Disk I/O Error when using tools due to shared outlines cache database ",
      "state": "closed",
      "comments": 25,
      "body_length": 5559,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-04-19 07:43:46+00:00",
      "closed_at": "2024-08-24 08:04:16+00:00",
      "resolution_days": 127.01423611111112
    },
    {
      "number": 4180,
      "title": "[Usage]: Llama 3 8B Instruct Inference",
      "state": "closed",
      "comments": 19,
      "body_length": 1958,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-04-18 20:09:05+00:00",
      "closed_at": "2024-04-24 04:43:43+00:00",
      "resolution_days": 5.357384259259259
    },
    {
      "number": 4135,
      "title": "[Bug]: async llm engine failed unexpectedly (using mixtral-8x7b with tp=4)",
      "state": "closed",
      "comments": 11,
      "body_length": 10831,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-04-17 06:18:46+00:00",
      "closed_at": "2024-12-19 15:46:10+00:00",
      "resolution_days": 246.39402777777778
    },
    {
      "number": 4131,
      "title": "[Bug]: Invalid Device Ordinal on ROCm",
      "state": "closed",
      "comments": 13,
      "body_length": 8402,
      "label_names": [
        "bug",
        "rocm"
      ],
      "created_at": "2024-04-16 23:58:58+00:00",
      "closed_at": "2024-09-04 14:08:36+00:00",
      "resolution_days": 140.59002314814813
    },
    {
      "number": 4112,
      "title": "[Bug]: VLLM's output is unstable when handling requests CONCURRENTLY.",
      "state": "closed",
      "comments": 12,
      "body_length": 6409,
      "label_names": [
        "bug",
        "unstale"
      ],
      "created_at": "2024-04-16 09:28:42+00:00",
      "closed_at": "2024-12-19 16:04:23+00:00",
      "resolution_days": 247.27478009259258
    },
    {
      "number": 4108,
      "title": "[Bug]: NCCL watchdog thread terminated with exception: CUDA error: an illegal memory access was encountered",
      "state": "closed",
      "comments": 13,
      "body_length": 58272,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-04-16 07:00:21+00:00",
      "closed_at": "2024-07-23 22:37:20+00:00",
      "resolution_days": 98.65068287037037
    },
    {
      "number": 4100,
      "title": "[Bug]: --engine-use-ray is broken.",
      "state": "closed",
      "comments": 13,
      "body_length": 1163,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-04-15 22:44:55+00:00",
      "closed_at": "2024-04-16 05:24:55+00:00",
      "resolution_days": 0.2777777777777778
    },
    {
      "number": 4093,
      "title": "[Bug]: guided_json bad output for llama2-13b",
      "state": "closed",
      "comments": 20,
      "body_length": 8114,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-04-15 16:23:41+00:00",
      "closed_at": "2024-05-16 02:32:25+00:00",
      "resolution_days": 30.42273148148148
    },
    {
      "number": 4041,
      "title": "[Bug]: \"\"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\"",
      "state": "closed",
      "comments": 13,
      "body_length": 20315,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-04-12 22:45:54+00:00",
      "closed_at": "2024-04-14 22:31:57+00:00",
      "resolution_days": 1.9903125
    },
    {
      "number": 4033,
      "title": "[Feature]: bitsandbytes support",
      "state": "closed",
      "comments": 28,
      "body_length": 1104,
      "label_names": [
        "feature request"
      ],
      "created_at": "2024-04-12 14:40:50+00:00",
      "closed_at": "2024-07-04 13:37:13+00:00",
      "resolution_days": 82.95582175925927
    },
    {
      "number": 4030,
      "title": "[Misc]: vLLM performs consistently poor as compared to HF TGI when tested with the DeepSeek Coder Model",
      "state": "closed",
      "comments": 20,
      "body_length": 3157,
      "label_names": [
        "misc"
      ],
      "created_at": "2024-04-12 06:55:22+00:00",
      "closed_at": "2024-05-08 09:35:29+00:00",
      "resolution_days": 26.11119212962963
    },
    {
      "number": 4027,
      "title": "[Bug]: start api server stuck",
      "state": "closed",
      "comments": 14,
      "body_length": 446,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-04-12 05:57:55+00:00",
      "closed_at": "2024-04-15 04:53:41+00:00",
      "resolution_days": 2.9553935185185187
    },
    {
      "number": 4011,
      "title": "[Installation]: VLLM is impossible to install. ",
      "state": "closed",
      "comments": 18,
      "body_length": 5510,
      "label_names": [
        "installation"
      ],
      "created_at": "2024-04-11 15:49:48+00:00",
      "closed_at": "2024-04-14 03:49:16+00:00",
      "resolution_days": 2.4996296296296294
    },
    {
      "number": 4008,
      "title": "[Bug]: Model architectures ['LlavaForCausalLM'] are not supported for now in vllm 0.4.0.post1",
      "state": "closed",
      "comments": 13,
      "body_length": 1585,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-04-11 13:13:33+00:00",
      "closed_at": "2024-05-31 03:34:19+00:00",
      "resolution_days": 49.59775462962963
    },
    {
      "number": 4000,
      "title": "[Usage]: How to purge zombie requests",
      "state": "closed",
      "comments": 12,
      "body_length": 587,
      "label_names": [
        "bug",
        "usage",
        "stale"
      ],
      "created_at": "2024-04-11 09:14:41+00:00",
      "closed_at": "2024-11-28 02:06:29+00:00",
      "resolution_days": 230.70263888888888
    },
    {
      "number": 3974,
      "title": "[Bug]: LLM is not getting loaded on multiple GPUs but works fine on a single GPU",
      "state": "closed",
      "comments": 20,
      "body_length": 8507,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-04-10 15:05:08+00:00",
      "closed_at": "2024-04-18 22:32:49+00:00",
      "resolution_days": 8.310891203703704
    },
    {
      "number": 3960,
      "title": "[Feature]: Tree attention about Speculative Decoding",
      "state": "closed",
      "comments": 13,
      "body_length": 461,
      "label_names": [
        "feature request",
        "stale"
      ],
      "created_at": "2024-04-10 06:05:51+00:00",
      "closed_at": "2025-01-06 02:03:10+00:00",
      "resolution_days": 270.8314699074074
    },
    {
      "number": 3944,
      "title": "[Bug]: Crash with --enable-prefix-caching enabled",
      "state": "closed",
      "comments": 17,
      "body_length": 14055,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-04-09 15:06:58+00:00",
      "closed_at": "2024-04-10 01:42:20+00:00",
      "resolution_days": 0.44122685185185184
    },
    {
      "number": 3916,
      "title": "[Bug]: bus error (core dumped)",
      "state": "closed",
      "comments": 16,
      "body_length": 5635,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-04-08 11:55:17+00:00",
      "closed_at": "2024-04-08 16:16:43+00:00",
      "resolution_days": 0.18155092592592592
    },
    {
      "number": 3902,
      "title": "[Feature]: Support Ray-free multi-node distributed inference on resource managers like Kubernetes",
      "state": "closed",
      "comments": 13,
      "body_length": 1311,
      "label_names": [
        "feature request",
        "stale"
      ],
      "created_at": "2024-04-07 21:58:25+00:00",
      "closed_at": "2024-11-28 02:06:47+00:00",
      "resolution_days": 234.17247685185185
    },
    {
      "number": 3897,
      "title": "[Usage]: How to determine whether the vllm engine is full with requests or not",
      "state": "closed",
      "comments": 21,
      "body_length": 9673,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2024-04-07 11:02:42+00:00",
      "closed_at": "2025-01-16 01:58:13+00:00",
      "resolution_days": 283.6218865740741
    },
    {
      "number": 3872,
      "title": "[Bug]: Does vLLM support Qwen/Qwen1.5-32B-Chat-AWQ? It works for the first time then stops generating responses.",
      "state": "closed",
      "comments": 16,
      "body_length": 607,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-04-05 18:57:47+00:00",
      "closed_at": "2024-11-28 02:06:54+00:00",
      "resolution_days": 236.29799768518518
    },
    {
      "number": 3861,
      "title": "[Roadmap] vLLM Roadmap Q2 2024",
      "state": "closed",
      "comments": 39,
      "body_length": 6249,
      "label_names": [],
      "created_at": "2024-04-04 22:38:01+00:00",
      "closed_at": "2024-06-25 00:08:31+00:00",
      "resolution_days": 81.06284722222222
    },
    {
      "number": 3852,
      "title": "[Misc]: Can we remove `vllm/entrypoints/api_server.py`?",
      "state": "closed",
      "comments": 12,
      "body_length": 418,
      "label_names": [
        "misc",
        "stale"
      ],
      "created_at": "2024-04-04 12:58:49+00:00",
      "closed_at": "2024-11-28 02:06:58+00:00",
      "resolution_days": 237.54732638888888
    },
    {
      "number": 3851,
      "title": "[Usage]: Setting max_tokens in chat completion request class returns an empty output",
      "state": "closed",
      "comments": 13,
      "body_length": 572,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-04-04 12:36:36+00:00",
      "closed_at": "2024-08-02 16:16:47+00:00",
      "resolution_days": 120.15290509259259
    },
    {
      "number": 3839,
      "title": "[Bug]: Error happen in async_llm_engine when use multiple GPUs",
      "state": "closed",
      "comments": 13,
      "body_length": 26604,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-04-04 01:07:55+00:00",
      "closed_at": "2024-11-28 02:07:02+00:00",
      "resolution_days": 238.04105324074075
    },
    {
      "number": 3793,
      "title": "[Bug]: RuntimeError: No suitable kernel. h_in=16 h_out=3424 dtype=Float out_dtype=BFloat16",
      "state": "closed",
      "comments": 35,
      "body_length": 18837,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-04-02 09:55:23+00:00",
      "closed_at": "2024-08-02 15:27:07+00:00",
      "resolution_days": 122.23037037037037
    },
    {
      "number": 3786,
      "title": "[Feature]: cuda12.2 support ",
      "state": "closed",
      "comments": 17,
      "body_length": 415,
      "label_names": [
        "feature request"
      ],
      "created_at": "2024-04-02 03:31:06+00:00",
      "closed_at": "2024-04-26 00:55:05+00:00",
      "resolution_days": 23.891655092592593
    },
    {
      "number": 3779,
      "title": "[Bug]: vllm-0.4.0 is much slower than vllm-0.3.3",
      "state": "closed",
      "comments": 13,
      "body_length": 7390,
      "label_names": [],
      "created_at": "2024-04-01 17:33:48+00:00",
      "closed_at": "2024-04-02 22:32:22+00:00",
      "resolution_days": 1.207337962962963
    },
    {
      "number": 3764,
      "title": "[Bug]: 2080ti 22G * 4, vllm=0.4.0 startup error",
      "state": "closed",
      "comments": 28,
      "body_length": 11905,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-04-01 00:22:01+00:00",
      "closed_at": "2024-04-03 05:58:53+00:00",
      "resolution_days": 2.233935185185185
    },
    {
      "number": 3750,
      "title": "[Usage]: Is it possible to pin `LLM` to a specific CUDA device?",
      "state": "closed",
      "comments": 14,
      "body_length": 624,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2024-03-30 19:54:46+00:00",
      "closed_at": "2025-02-09 02:02:04+00:00",
      "resolution_days": 315.2550694444444
    },
    {
      "number": 3747,
      "title": "[Bug]: trying to run vllm inference behind the fastapi's server, but it stucks",
      "state": "closed",
      "comments": 14,
      "body_length": 7162,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-03-30 18:25:24+00:00",
      "closed_at": "2024-06-13 09:03:10+00:00",
      "resolution_days": 74.60956018518519
    },
    {
      "number": 3732,
      "title": "[Bug]: Assertion `parentOp->getNumRegions() == 1 && parentOp->getRegion(0).getBlocks().size() == 1' failed",
      "state": "closed",
      "comments": 23,
      "body_length": 22297,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-03-29 15:28:34+00:00",
      "closed_at": "2024-03-30 15:56:39+00:00",
      "resolution_days": 1.0195023148148148
    },
    {
      "number": 3722,
      "title": "[Bug]: RuntimeError: CUDA error: invalid device ordinal  with multi node multi gpus ",
      "state": "closed",
      "comments": 19,
      "body_length": 16953,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-03-29 06:29:19+00:00",
      "closed_at": "2024-09-20 20:04:16+00:00",
      "resolution_days": 175.5659375
    },
    {
      "number": 3713,
      "title": "[Feature]: Integrate with lm-format-enforcer",
      "state": "closed",
      "comments": 11,
      "body_length": 697,
      "label_names": [
        "feature request"
      ],
      "created_at": "2024-03-29 03:28:10+00:00",
      "closed_at": "2024-04-16 05:54:59+00:00",
      "resolution_days": 18.101956018518518
    },
    {
      "number": 3688,
      "title": "[Bug]: Custom all reduce not work.",
      "state": "closed",
      "comments": 21,
      "body_length": 17733,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-03-28 13:45:30+00:00",
      "closed_at": "2025-02-22 01:58:17+00:00",
      "resolution_days": 330.5088773148148
    },
    {
      "number": 3680,
      "title": "Enable mypy type checking",
      "state": "closed",
      "comments": 14,
      "body_length": 168,
      "label_names": [
        "good first issue",
        "misc",
        "unstale"
      ],
      "created_at": "2024-03-28 04:28:16+00:00",
      "closed_at": "2025-02-27 07:25:13+00:00",
      "resolution_days": 336.12288194444443
    },
    {
      "number": 3654,
      "title": "[RFC] Initial Support for CPUs",
      "state": "closed",
      "comments": 11,
      "body_length": 2727,
      "label_names": [
        "RFC",
        "x86-cpu",
        "unstale"
      ],
      "created_at": "2024-03-27 07:45:25+00:00",
      "closed_at": "2025-01-14 16:19:23+00:00",
      "resolution_days": 293.3569212962963
    },
    {
      "number": 3646,
      "title": "[Bug]: No output on WSL (Debian, Windows 11)",
      "state": "closed",
      "comments": 14,
      "body_length": 7132,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-03-26 21:34:16+00:00",
      "closed_at": "2024-03-26 22:08:20+00:00",
      "resolution_days": 0.023657407407407408
    },
    {
      "number": 3620,
      "title": "[RFC] Initial Support for Cloud TPUs",
      "state": "closed",
      "comments": 17,
      "body_length": 6378,
      "label_names": [
        "RFC",
        "tpu",
        "stale"
      ],
      "created_at": "2024-03-25 17:08:43+00:00",
      "closed_at": "2025-03-11 14:04:01+00:00",
      "resolution_days": 350.8717361111111
    },
    {
      "number": 3616,
      "title": "[Feature]: Even Better Observability",
      "state": "closed",
      "comments": 14,
      "body_length": 562,
      "label_names": [
        "good first issue",
        "feature request",
        "stale"
      ],
      "created_at": "2024-03-25 16:03:24+00:00",
      "closed_at": "2024-11-29 02:07:06+00:00",
      "resolution_days": 248.4192361111111
    },
    {
      "number": 3587,
      "title": "[RFC]: Interface and Abstraction for Distributed Inference Environment",
      "state": "closed",
      "comments": 18,
      "body_length": 9819,
      "label_names": [
        "RFC",
        "misc"
      ],
      "created_at": "2024-03-23 23:41:40+00:00",
      "closed_at": "2024-06-14 01:00:32+00:00",
      "resolution_days": 82.05476851851851
    },
    {
      "number": 3567,
      "title": "[Misc]: Throughput/Latency for guided_json with ~100% GPU cache utilization",
      "state": "closed",
      "comments": 67,
      "body_length": 1276,
      "label_names": [
        "structured-output",
        "misc",
        "stale"
      ],
      "created_at": "2024-03-22 12:11:32+00:00",
      "closed_at": "2025-05-17 02:09:48+00:00",
      "resolution_days": 420.5821296296296
    },
    {
      "number": 3563,
      "title": "[Feature]: Offload Model Weights to CPU ",
      "state": "closed",
      "comments": 12,
      "body_length": 3373,
      "label_names": [
        "feature request"
      ],
      "created_at": "2024-03-22 00:09:25+00:00",
      "closed_at": "2024-07-20 02:39:03+00:00",
      "resolution_days": 120.10391203703703
    },
    {
      "number": 3536,
      "title": "[Feature]: Support Guided Decoding in `LLM` entrypoint",
      "state": "closed",
      "comments": 16,
      "body_length": 581,
      "label_names": [
        "feature request"
      ],
      "created_at": "2024-03-20 22:19:01+00:00",
      "closed_at": "2024-08-04 03:12:10+00:00",
      "resolution_days": 136.20357638888888
    },
    {
      "number": 3525,
      "title": "[Usage]: How to inference model with multi-gpus",
      "state": "closed",
      "comments": 17,
      "body_length": 2282,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-03-20 06:54:39+00:00",
      "closed_at": "2024-04-12 08:13:53+00:00",
      "resolution_days": 23.05502314814815
    },
    {
      "number": 3488,
      "title": "[Bug]: DynamicNTKScalingRotaryEmbedding implementation is different from Transformers",
      "state": "closed",
      "comments": 11,
      "body_length": 3434,
      "label_names": [
        "bug",
        "unstale"
      ],
      "created_at": "2024-03-19 03:35:23+00:00",
      "closed_at": "2024-12-19 17:14:31+00:00",
      "resolution_days": 275.5688425925926
    },
    {
      "number": 3464,
      "title": "Openllm with vLLM backend VS  vLLM in handling group of requests at the same time",
      "state": "closed",
      "comments": 15,
      "body_length": 1492,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-03-18 14:55:36+00:00",
      "closed_at": "2024-11-28 02:07:19+00:00",
      "resolution_days": 254.4664699074074
    },
    {
      "number": 3457,
      "title": "[Usage]: When I set tensor_parallel_size as 2 and worker_use_ray will be automatically true, Ray will stop when INFO ray.init() ",
      "state": "closed",
      "comments": 19,
      "body_length": 6900,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-03-18 03:41:47+00:00",
      "closed_at": "2024-03-18 22:58:58+00:00",
      "resolution_days": 0.803599537037037
    },
    {
      "number": 3416,
      "title": "[Bug]: docker \u542f\u52a8vllm \u62a5\u9519",
      "state": "closed",
      "comments": 12,
      "body_length": 3963,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-03-15 00:58:36+00:00",
      "closed_at": "2024-03-15 04:03:50+00:00",
      "resolution_days": 0.12863425925925925
    },
    {
      "number": 3392,
      "title": "AWQ + Marlin Error",
      "state": "closed",
      "comments": 16,
      "body_length": 3512,
      "label_names": [],
      "created_at": "2024-03-14 04:52:22+00:00",
      "closed_at": "2024-07-25 19:53:41+00:00",
      "resolution_days": 133.62591435185186
    },
    {
      "number": 3385,
      "title": "Attention sliding window",
      "state": "closed",
      "comments": 13,
      "body_length": 718,
      "label_names": [
        "stale"
      ],
      "created_at": "2024-03-13 19:53:48+00:00",
      "closed_at": "2025-03-28 02:05:33+00:00",
      "resolution_days": 379.2581597222222
    },
    {
      "number": 3334,
      "title": "TCPStore is not available",
      "state": "closed",
      "comments": 19,
      "body_length": 257,
      "label_names": [
        "stale"
      ],
      "created_at": "2024-03-12 04:15:02+00:00",
      "closed_at": "2024-11-28 02:07:46+00:00",
      "resolution_days": 260.9116203703704
    },
    {
      "number": 3332,
      "title": "Is it possible to use vllm-0.3.3 with CUDA 11.8",
      "state": "closed",
      "comments": 14,
      "body_length": 1039,
      "label_names": [
        "stale"
      ],
      "created_at": "2024-03-12 02:50:00+00:00",
      "closed_at": "2024-11-28 02:07:48+00:00",
      "resolution_days": 260.97069444444446
    },
    {
      "number": 3283,
      "title": "Order of keys for guided JSON",
      "state": "closed",
      "comments": 12,
      "body_length": 1084,
      "label_names": [
        "good first issue"
      ],
      "created_at": "2024-03-08 13:45:25+00:00",
      "closed_at": "2024-06-10 18:35:46+00:00",
      "resolution_days": 94.20163194444444
    },
    {
      "number": 3230,
      "title": "Error in benchmark model with vllm backend",
      "state": "closed",
      "comments": 14,
      "body_length": 3188,
      "label_names": [
        "stale"
      ],
      "created_at": "2024-03-06 12:03:59+00:00",
      "closed_at": "2024-11-30 02:02:06+00:00",
      "resolution_days": 268.582025462963
    },
    {
      "number": 3209,
      "title": "IMPORTANT Bug: Model return empty response (output len = 0), when recieved multiple concurrent request.",
      "state": "closed",
      "comments": 21,
      "body_length": 2146,
      "label_names": [],
      "created_at": "2024-03-05 20:52:08+00:00",
      "closed_at": "2024-03-06 15:52:50+00:00",
      "resolution_days": 0.7921527777777778
    },
    {
      "number": 3155,
      "title": "[v0.4.0] Release Tracker",
      "state": "closed",
      "comments": 11,
      "body_length": 134,
      "label_names": [
        "release"
      ],
      "created_at": "2024-03-02 01:41:58+00:00",
      "closed_at": "2024-04-03 22:42:26+00:00",
      "resolution_days": 32.87532407407407
    },
    {
      "number": 3154,
      "title": "Generation with Prefix-cache are slower than the ones without it ?",
      "state": "closed",
      "comments": 13,
      "body_length": 3354,
      "label_names": [
        "stale"
      ],
      "created_at": "2024-03-02 01:38:22+00:00",
      "closed_at": "2024-11-30 02:02:21+00:00",
      "resolution_days": 273.0166550925926
    },
    {
      "number": 3130,
      "title": "[RFC] Upstream Chunked Prefill",
      "state": "closed",
      "comments": 18,
      "body_length": 4944,
      "label_names": [],
      "created_at": "2024-03-01 01:50:46+00:00",
      "closed_at": "2024-04-26 15:09:56+00:00",
      "resolution_days": 56.554976851851855
    },
    {
      "number": 3061,
      "title": "Building VLLM from source and running inference: No module named 'vllm._C'",
      "state": "closed",
      "comments": 13,
      "body_length": 1975,
      "label_names": [
        "stale"
      ],
      "created_at": "2024-02-27 15:31:26+00:00",
      "closed_at": "2024-11-29 02:07:59+00:00",
      "resolution_days": 275.4420486111111
    },
    {
      "number": 3033,
      "title": "Qwen 14B AWQ deploy: AttributeError: 'ndarray' object has no attribute '_torch_dtype'",
      "state": "closed",
      "comments": 11,
      "body_length": 4274,
      "label_names": [
        "stale"
      ],
      "created_at": "2024-02-26 02:35:02+00:00",
      "closed_at": "2024-11-29 02:08:07+00:00",
      "resolution_days": 276.98130787037036
    },
    {
      "number": 3012,
      "title": "Unable to specify GPU usage in VLLM code",
      "state": "closed",
      "comments": 17,
      "body_length": 608,
      "label_names": [
        "stale"
      ],
      "created_at": "2024-02-23 19:23:00+00:00",
      "closed_at": "2024-11-29 02:08:12+00:00",
      "resolution_days": 279.2813888888889
    },
    {
      "number": 3000,
      "title": "How can I use the Lora Adapter for a model with Vocab size 40960?",
      "state": "closed",
      "comments": 24,
      "body_length": 290,
      "label_names": [
        "stale"
      ],
      "created_at": "2024-02-23 01:32:43+00:00",
      "closed_at": "2024-11-29 02:08:14+00:00",
      "resolution_days": 280.02466435185187
    },
    {
      "number": 2957,
      "title": "I use vllm to accelerate the large model of qwen, mainly qwen7B/qwen14B. Two issues were found during the testing of the large model.",
      "state": "closed",
      "comments": 16,
      "body_length": 477,
      "label_names": [
        "stale"
      ],
      "created_at": "2024-02-21 10:17:08+00:00",
      "closed_at": "2024-12-01 02:15:35+00:00",
      "resolution_days": 283.66559027777777
    },
    {
      "number": 2917,
      "title": "Support for  Smaug-72B-v0.1 on vLLM",
      "state": "closed",
      "comments": 12,
      "body_length": 0,
      "label_names": [],
      "created_at": "2024-02-19 03:25:19+00:00",
      "closed_at": "2024-04-08 19:42:37+00:00",
      "resolution_days": 49.67868055555556
    },
    {
      "number": 2859,
      "title": "[v0.3.1] Release Tracker",
      "state": "closed",
      "comments": 12,
      "body_length": 221,
      "label_names": [
        "release"
      ],
      "created_at": "2024-02-13 23:36:38+00:00",
      "closed_at": "2024-02-16 23:05:19+00:00",
      "resolution_days": 2.978252314814815
    },
    {
      "number": 2850,
      "title": "Missing prometheus metrics in `0.3.0`",
      "state": "closed",
      "comments": 17,
      "body_length": 496,
      "label_names": [],
      "created_at": "2024-02-13 16:28:23+00:00",
      "closed_at": "2024-08-02 16:19:33+00:00",
      "resolution_days": 170.99386574074074
    },
    {
      "number": 2826,
      "title": "vLLM running on a Ray Cluster Hanging on Initializing",
      "state": "closed",
      "comments": 12,
      "body_length": 1456,
      "label_names": [],
      "created_at": "2024-02-09 14:41:23+00:00",
      "closed_at": "2024-02-27 01:33:39+00:00",
      "resolution_days": 17.452962962962964
    },
    {
      "number": 2797,
      "title": "from vllm._C import cuda_utils raise error",
      "state": "closed",
      "comments": 12,
      "body_length": 1165,
      "label_names": [],
      "created_at": "2024-02-07 03:08:33+00:00",
      "closed_at": "2024-03-08 01:50:40+00:00",
      "resolution_days": 29.94591435185185
    },
    {
      "number": 2752,
      "title": "vLLM ignores my requests when I increase the number of concurrent requests",
      "state": "closed",
      "comments": 11,
      "body_length": 2920,
      "label_names": [],
      "created_at": "2024-02-05 05:05:21+00:00",
      "closed_at": "2024-09-20 20:28:45+00:00",
      "resolution_days": 228.64125
    },
    {
      "number": 2747,
      "title": "ImportError: /ramyapra/vllm/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol:",
      "state": "closed",
      "comments": 64,
      "body_length": 1201,
      "label_names": [],
      "created_at": "2024-02-04 15:52:34+00:00",
      "closed_at": "2024-03-25 04:44:55+00:00",
      "resolution_days": 49.53635416666667
    },
    {
      "number": 2733,
      "title": "Better defaults to match Hugging Face",
      "state": "closed",
      "comments": 14,
      "body_length": 9714,
      "label_names": [
        "stale"
      ],
      "created_at": "2024-02-03 07:38:52+00:00",
      "closed_at": "2025-03-21 02:18:37+00:00",
      "resolution_days": 411.77760416666666
    },
    {
      "number": 2731,
      "title": "vLLM getting stuck. Nothing is generate while requests are running and pending.",
      "state": "closed",
      "comments": 29,
      "body_length": 13792,
      "label_names": [],
      "created_at": "2024-02-03 00:56:51+00:00",
      "closed_at": "2024-02-13 23:41:09+00:00",
      "resolution_days": 10.947430555555556
    },
    {
      "number": 2729,
      "title": "Assertion `!(srcMmaLayout && dstMmaLayout) && \"Unexpected mma -> mma layout conversion\"' failed.",
      "state": "closed",
      "comments": 27,
      "body_length": 621,
      "label_names": [
        "stale"
      ],
      "created_at": "2024-02-02 12:02:09+00:00",
      "closed_at": "2025-07-13 02:16:06+00:00",
      "resolution_days": 526.5930208333333
    },
    {
      "number": 2728,
      "title": "Mixtral GPTQ with TP=2 not generating output",
      "state": "closed",
      "comments": 17,
      "body_length": 792,
      "label_names": [],
      "created_at": "2024-02-02 10:16:25+00:00",
      "closed_at": "2024-02-06 07:16:16+00:00",
      "resolution_days": 3.8748958333333334
    },
    {
      "number": 2706,
      "title": "The echo parameters and request logs seem to have some issues in vLLM v0.3.0 version (/v1/completions)",
      "state": "closed",
      "comments": 11,
      "body_length": 4998,
      "label_names": [
        "stale"
      ],
      "created_at": "2024-02-01 08:05:02+00:00",
      "closed_at": "2024-12-27 02:00:38+00:00",
      "resolution_days": 329.74694444444447
    },
    {
      "number": 2699,
      "title": "Seek help, `Qwen-14B-Chat-Int4`ValueError: The input size is not aligned with the quantized weight shape. This can be caused by too large tensor parallel size.",
      "state": "closed",
      "comments": 18,
      "body_length": 5661,
      "label_names": [],
      "created_at": "2024-02-01 02:08:42+00:00",
      "closed_at": "2024-02-21 08:38:34+00:00",
      "resolution_days": 20.27074074074074
    },
    {
      "number": 2681,
      "title": "[Roadmap] vLLM Roadmap Q1 2024",
      "state": "closed",
      "comments": 15,
      "body_length": 1996,
      "label_names": [],
      "created_at": "2024-01-31 06:31:06+00:00",
      "closed_at": "2024-04-04 22:38:54+00:00",
      "resolution_days": 64.67208333333333
    },
    {
      "number": 2653,
      "title": "IndexError when using Beam Search in Chat Completions",
      "state": "closed",
      "comments": 11,
      "body_length": 1129,
      "label_names": [
        "bug",
        "good first issue",
        "stale"
      ],
      "created_at": "2024-01-29 18:41:47+00:00",
      "closed_at": "2025-01-02 02:00:10+00:00",
      "resolution_days": 338.30443287037036
    },
    {
      "number": 2646,
      "title": "[BUG] Compile source code error for ROCM6.0",
      "state": "closed",
      "comments": 12,
      "body_length": 3722,
      "label_names": [],
      "created_at": "2024-01-29 10:46:51+00:00",
      "closed_at": "2024-02-01 17:35:10+00:00",
      "resolution_days": 3.2835532407407406
    },
    {
      "number": 2621,
      "title": "Mixtral AWQ fails to work: asyncio.exceptions.CancelledError: Cancelled by cancel scope 7fd214489990",
      "state": "closed",
      "comments": 12,
      "body_length": 11552,
      "label_names": [],
      "created_at": "2024-01-27 01:26:26+00:00",
      "closed_at": "2024-04-04 15:15:39+00:00",
      "resolution_days": 68.5758449074074
    },
    {
      "number": 2614,
      "title": "[RFC] Automatic Prefix Caching",
      "state": "closed",
      "comments": 17,
      "body_length": 3433,
      "label_names": [
        "feature request",
        "RFC"
      ],
      "created_at": "2024-01-26 08:09:07+00:00",
      "closed_at": "2024-03-02 08:50:03+00:00",
      "resolution_days": 36.02842592592592
    },
    {
      "number": 2547,
      "title": "Allow passing hf config args with openai server",
      "state": "closed",
      "comments": 11,
      "body_length": 887,
      "label_names": [
        "good first issue",
        "feature request",
        "unstale"
      ],
      "created_at": "2024-01-22 09:50:45+00:00",
      "closed_at": "2024-11-09 16:19:28+00:00",
      "resolution_days": 292.2699421296296
    },
    {
      "number": 2484,
      "title": "Aborted request without reason",
      "state": "closed",
      "comments": 50,
      "body_length": 2641,
      "label_names": [],
      "created_at": "2024-01-18 08:48:18+00:00",
      "closed_at": "2024-10-17 22:09:10+00:00",
      "resolution_days": 273.5561574074074
    },
    {
      "number": 2483,
      "title": "Support JSON mode.",
      "state": "closed",
      "comments": 11,
      "body_length": 76,
      "label_names": [],
      "created_at": "2024-01-18 08:39:39+00:00",
      "closed_at": "2024-10-26 16:47:09+00:00",
      "resolution_days": 282.3385416666667
    },
    {
      "number": 2466,
      "title": "vLLM Distributed Inference stuck when using multi -GPU",
      "state": "closed",
      "comments": 11,
      "body_length": 543,
      "label_names": [],
      "created_at": "2024-01-17 12:17:04+00:00",
      "closed_at": "2024-06-13 09:04:58+00:00",
      "resolution_days": 147.86659722222223
    },
    {
      "number": 2464,
      "title": "Truncated response -- repro code",
      "state": "closed",
      "comments": 13,
      "body_length": 1302,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-01-17 07:14:12+00:00",
      "closed_at": "2024-06-03 02:51:04+00:00",
      "resolution_days": 137.81726851851852
    },
    {
      "number": 2460,
      "title": "Run local llm qwen-72b failed",
      "state": "closed",
      "comments": 15,
      "body_length": 506,
      "label_names": [],
      "created_at": "2024-01-17 03:43:00+00:00",
      "closed_at": "2024-06-13 09:03:47+00:00",
      "resolution_days": 148.2227662037037
    },
    {
      "number": 2419,
      "title": "anyone can Qwen-14B-Chat-AWQ work with VLLM/TP ?",
      "state": "closed",
      "comments": 13,
      "body_length": 3799,
      "label_names": [
        "stale"
      ],
      "created_at": "2024-01-11 09:12:43+00:00",
      "closed_at": "2025-07-13 02:16:07+00:00",
      "resolution_days": 548.7106944444445
    },
    {
      "number": 2418,
      "title": "ValueError: The model's max seq len (4096) is larger than the maximum number of tokens that can be stored in KV cache (3664). Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.`",
      "state": "closed",
      "comments": 41,
      "body_length": 2258,
      "label_names": [],
      "created_at": "2024-01-11 08:04:22+00:00",
      "closed_at": "2025-02-02 14:55:41+00:00",
      "resolution_days": 388.2856365740741
    },
    {
      "number": 2370,
      "title": "How to use Splitwise(from microsoft) in vllm?",
      "state": "closed",
      "comments": 12,
      "body_length": 350,
      "label_names": [
        "stale"
      ],
      "created_at": "2024-01-08 03:49:36+00:00",
      "closed_at": "2024-11-30 02:03:07+00:00",
      "resolution_days": 326.9260532407407
    },
    {
      "number": 2364,
      "title": "Compute perplexity/logits for the prompt",
      "state": "closed",
      "comments": 13,
      "body_length": 392,
      "label_names": [
        "stale"
      ],
      "created_at": "2024-01-07 14:26:41+00:00",
      "closed_at": "2025-05-15 02:09:52+00:00",
      "resolution_days": 493.48832175925924
    },
    {
      "number": 2363,
      "title": "OutOfMemoryError",
      "state": "closed",
      "comments": 12,
      "body_length": 636,
      "label_names": [],
      "created_at": "2024-01-06 18:32:58+00:00",
      "closed_at": "2024-04-04 07:47:52+00:00",
      "resolution_days": 88.5520138888889
    },
    {
      "number": 2345,
      "title": "memory leak in v0.2.7",
      "state": "closed",
      "comments": 12,
      "body_length": 842,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-01-04 13:09:40+00:00",
      "closed_at": "2024-08-07 14:21:18+00:00",
      "resolution_days": 216.04974537037037
    },
    {
      "number": 2309,
      "title": "Error installing on windows",
      "state": "closed",
      "comments": 15,
      "body_length": 8361,
      "label_names": [],
      "created_at": "2023-12-31 01:32:15+00:00",
      "closed_at": "2024-04-18 14:00:12+00:00",
      "resolution_days": 109.51940972222222
    },
    {
      "number": 2257,
      "title": "Does the continuous batching technology in the vLLM online service scenario contain the concept of batch size?",
      "state": "closed",
      "comments": 16,
      "body_length": 0,
      "label_names": [],
      "created_at": "2023-12-25 09:27:57+00:00",
      "closed_at": "2024-04-04 07:39:41+00:00",
      "resolution_days": 100.92481481481481
    },
    {
      "number": 2251,
      "title": "Load Mixtral 8x7b AWQ model failed",
      "state": "closed",
      "comments": 30,
      "body_length": 3345,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-12-24 09:27:53+00:00",
      "closed_at": "2024-11-30 02:03:17+00:00",
      "resolution_days": 341.69125
    },
    {
      "number": 2181,
      "title": "Error running Mixtral in tensor-parallel 2",
      "state": "closed",
      "comments": 12,
      "body_length": 3683,
      "label_names": [],
      "created_at": "2023-12-18 19:22:50+00:00",
      "closed_at": "2024-04-04 07:46:56+00:00",
      "resolution_days": 107.51673611111111
    },
    {
      "number": 2140,
      "title": "torch.cuda.OutOfMemoryError: CUDA out of memory",
      "state": "closed",
      "comments": 35,
      "body_length": 2008,
      "label_names": [],
      "created_at": "2023-12-16 13:39:22+00:00",
      "closed_at": "2024-04-03 15:44:55+00:00",
      "resolution_days": 109.0871875
    },
    {
      "number": 2136,
      "title": "KV cache is low, memory profiling does not see the remaining VRAM",
      "state": "closed",
      "comments": 11,
      "body_length": 1595,
      "label_names": [],
      "created_at": "2023-12-15 22:31:17+00:00",
      "closed_at": "2024-04-03 15:33:31+00:00",
      "resolution_days": 109.70988425925925
    },
    {
      "number": 2084,
      "title": "Unable to run any model with tensor_parallel_size>1 on AWS sagemaker notebooks",
      "state": "closed",
      "comments": 11,
      "body_length": 6062,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-12-13 13:16:45+00:00",
      "closed_at": "2024-11-30 02:03:29+00:00",
      "resolution_days": 352.5324537037037
    },
    {
      "number": 2081,
      "title": "Inquiry Regarding vLLM Support for Mac Metal API",
      "state": "closed",
      "comments": 21,
      "body_length": 1728,
      "label_names": [],
      "created_at": "2023-12-13 08:50:01+00:00",
      "closed_at": "2024-09-20 20:29:58+00:00",
      "resolution_days": 282.4860763888889
    },
    {
      "number": 2074,
      "title": "error when inferencing Mixtral AWQ",
      "state": "closed",
      "comments": 30,
      "body_length": 3101,
      "label_names": [
        "quantization"
      ],
      "created_at": "2023-12-13 02:33:06+00:00",
      "closed_at": "2023-12-18 18:56:13+00:00",
      "resolution_days": 5.6827199074074075
    },
    {
      "number": 2069,
      "title": "Mixtral tokens-per-second slower than expected, 10 tps",
      "state": "closed",
      "comments": 18,
      "body_length": 5777,
      "label_names": [
        "performance"
      ],
      "created_at": "2023-12-12 22:25:17+00:00",
      "closed_at": "2023-12-14 08:22:50+00:00",
      "resolution_days": 1.4149652777777777
    },
    {
      "number": 2064,
      "title": "[BUG] Mistral/Mixtral generate nonsense past 4096 tokens in prompt",
      "state": "closed",
      "comments": 15,
      "body_length": 246,
      "label_names": [
        "bug"
      ],
      "created_at": "2023-12-12 19:06:48+00:00",
      "closed_at": "2023-12-13 20:28:15+00:00",
      "resolution_days": 1.0565625
    },
    {
      "number": 2021,
      "title": "ARM aarch-64 server build failed (host OS: Ubuntu22.04.3) ",
      "state": "closed",
      "comments": 54,
      "body_length": 3604,
      "label_names": [],
      "created_at": "2023-12-11 14:37:44+00:00",
      "closed_at": "2024-09-22 19:47:55+00:00",
      "resolution_days": 286.2154050925926
    },
    {
      "number": 2020,
      "title": "Mixtral - KeyError: 'model.layers.10.block_sparse_moe.experts.0.w1.weight'",
      "state": "closed",
      "comments": 11,
      "body_length": 7019,
      "label_names": [],
      "created_at": "2023-12-11 13:54:26+00:00",
      "closed_at": "2023-12-11 20:37:49+00:00",
      "resolution_days": 0.2801273148148148
    },
    {
      "number": 2019,
      "title": "why online seving slower than offline serving??",
      "state": "closed",
      "comments": 13,
      "body_length": 1047,
      "label_names": [],
      "created_at": "2023-12-11 12:50:58+00:00",
      "closed_at": "2024-10-26 16:44:55+00:00",
      "resolution_days": 320.16246527777776
    },
    {
      "number": 2017,
      "title": "TypeError: PaddedGatherOp.forward() takes 6 positional arguments but 7 were given",
      "state": "closed",
      "comments": 16,
      "body_length": 3473,
      "label_names": [],
      "created_at": "2023-12-11 10:33:07+00:00",
      "closed_at": "2023-12-12 09:05:48+00:00",
      "resolution_days": 0.9393634259259259
    },
    {
      "number": 1968,
      "title": "RuntimeError: Inplace update to inference tensor outside InferenceMode is not allowed.You can make a clone to get a normal tensor before doing inplace update.See https://github.com/pytorch/rfcs/pull/17 for more details.",
      "state": "closed",
      "comments": 16,
      "body_length": 6312,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-12-07 18:28:29+00:00",
      "closed_at": "2025-01-23 01:58:55+00:00",
      "resolution_days": 412.3128009259259
    },
    {
      "number": 1958,
      "title": "Repeated answer: When I use vllm with opt-13b, the generated text is not end until the max length, with the repeated answer",
      "state": "closed",
      "comments": 11,
      "body_length": 987,
      "label_names": [],
      "created_at": "2023-12-07 08:02:58+00:00",
      "closed_at": "2024-03-25 10:10:52+00:00",
      "resolution_days": 109.08881944444444
    },
    {
      "number": 1946,
      "title": "Streaming support in VLLM",
      "state": "closed",
      "comments": 15,
      "body_length": 1002,
      "label_names": [],
      "created_at": "2023-12-06 12:29:14+00:00",
      "closed_at": "2024-04-04 08:07:49+00:00",
      "resolution_days": 119.81846064814815
    },
    {
      "number": 1936,
      "title": "Can't load `vllm-openai/v0.2.3` image",
      "state": "closed",
      "comments": 14,
      "body_length": 454,
      "label_names": [],
      "created_at": "2023-12-05 13:22:48+00:00",
      "closed_at": "2023-12-08 17:53:48+00:00",
      "resolution_days": 3.1881944444444446
    },
    {
      "number": 1908,
      "title": "Is there a way to terminate vllm.LLM and release the GPU memory",
      "state": "closed",
      "comments": 44,
      "body_length": 415,
      "label_names": [],
      "created_at": "2023-12-04 00:12:27+00:00",
      "closed_at": "2025-02-08 21:10:10+00:00",
      "resolution_days": 432.87341435185186
    },
    {
      "number": 1880,
      "title": "[Performance] Use optimized kernels for MQA/GQA",
      "state": "closed",
      "comments": 34,
      "body_length": 1214,
      "label_names": [
        "help wanted",
        "performance",
        "stale"
      ],
      "created_at": "2023-12-01 07:06:39+00:00",
      "closed_at": "2024-12-01 02:15:48+00:00",
      "resolution_days": 365.79802083333334
    },
    {
      "number": 1870,
      "title": "Add latency metrics",
      "state": "closed",
      "comments": 11,
      "body_length": 621,
      "label_names": [
        "help wanted",
        "good first issue"
      ],
      "created_at": "2023-11-30 22:02:04+00:00",
      "closed_at": "2024-04-29 01:26:18+00:00",
      "resolution_days": 150.1418287037037
    },
    {
      "number": 1861,
      "title": "01-ai/Yi-34B-Chat never stops",
      "state": "closed",
      "comments": 12,
      "body_length": 2657,
      "label_names": [],
      "created_at": "2023-11-30 12:15:58+00:00",
      "closed_at": "2024-01-16 22:30:05+00:00",
      "resolution_days": 47.42646990740741
    },
    {
      "number": 1846,
      "title": "Model Loading Stuck (in ray ?)",
      "state": "closed",
      "comments": 21,
      "body_length": 568,
      "label_names": [],
      "created_at": "2023-11-30 02:08:20+00:00",
      "closed_at": "2023-11-30 08:23:44+00:00",
      "resolution_days": 0.26069444444444445
    },
    {
      "number": 1814,
      "title": "ModuleNotFoundError: No module named \"vllm._C\"",
      "state": "closed",
      "comments": 38,
      "body_length": 98,
      "label_names": [],
      "created_at": "2023-11-28 14:13:20+00:00",
      "closed_at": "2024-05-31 20:37:32+00:00",
      "resolution_days": 185.26680555555555
    },
    {
      "number": 1808,
      "title": "Need help with supporting \"core42/jais-13b-chat\" model",
      "state": "closed",
      "comments": 17,
      "body_length": 3405,
      "label_names": [],
      "created_at": "2023-11-28 11:20:11+00:00",
      "closed_at": "2024-03-25 12:41:17+00:00",
      "resolution_days": 118.05631944444444
    },
    {
      "number": 1801,
      "title": "Nvidia drivers 545.29.02 broken --tensor-parallel-size",
      "state": "closed",
      "comments": 16,
      "body_length": 2657,
      "label_names": [],
      "created_at": "2023-11-27 17:54:00+00:00",
      "closed_at": "2024-03-25 11:30:01+00:00",
      "resolution_days": 118.73334490740741
    },
    {
      "number": 1792,
      "title": "CUDA OOM on Multi GPU but works on single GPU, multiple models",
      "state": "closed",
      "comments": 11,
      "body_length": 2617,
      "label_names": [],
      "created_at": "2023-11-26 19:09:37+00:00",
      "closed_at": "2023-12-02 04:42:09+00:00",
      "resolution_days": 5.397592592592592
    },
    {
      "number": 1742,
      "title": "Lookahead decoding",
      "state": "closed",
      "comments": 23,
      "body_length": 381,
      "label_names": [
        "stale"
      ],
      "created_at": "2023-11-21 23:45:39+00:00",
      "closed_at": "2025-02-20 02:17:01+00:00",
      "resolution_days": 456.10511574074076
    },
    {
      "number": 1728,
      "title": "How to use logits_processors",
      "state": "closed",
      "comments": 13,
      "body_length": 89,
      "label_names": [],
      "created_at": "2023-11-20 15:22:39+00:00",
      "closed_at": "2024-03-25 09:50:46+00:00",
      "resolution_days": 125.76952546296296
    },
    {
      "number": 1726,
      "title": "NCCL error",
      "state": "closed",
      "comments": 24,
      "body_length": 357,
      "label_names": [],
      "created_at": "2023-11-20 13:40:48+00:00",
      "closed_at": "2024-05-31 20:20:50+00:00",
      "resolution_days": 193.27780092592593
    },
    {
      "number": 1719,
      "title": "/usr/bin/python: Error while finding module specification for 'vllm.entrypoints.api_server' (ImportError: cannot import name 'cuda_utils' from partially initialized module 'vllm' (most likely due to a circular import) (/workspace/vllm/vllm/__init__.py))",
      "state": "closed",
      "comments": 18,
      "body_length": 525,
      "label_names": [],
      "created_at": "2023-11-19 14:53:04+00:00",
      "closed_at": "2024-03-25 09:56:58+00:00",
      "resolution_days": 126.794375
    },
    {
      "number": 1718,
      "title": "ImportError: libcudart.so.12",
      "state": "closed",
      "comments": 11,
      "body_length": 245,
      "label_names": [
        "installation"
      ],
      "created_at": "2023-11-19 08:40:27+00:00",
      "closed_at": "2024-06-27 15:00:19+00:00",
      "resolution_days": 221.2637962962963
    },
    {
      "number": 1707,
      "title": "API causes slowdown in batch request handling",
      "state": "closed",
      "comments": 42,
      "body_length": 413,
      "label_names": [
        "bug",
        "unstale"
      ],
      "created_at": "2023-11-17 16:57:04+00:00",
      "closed_at": "2025-02-02 14:49:08+00:00",
      "resolution_days": 442.9111574074074
    },
    {
      "number": 1654,
      "title": "usage of vllm for extracting embeddings",
      "state": "closed",
      "comments": 12,
      "body_length": 973,
      "label_names": [],
      "created_at": "2023-11-14 04:13:55+00:00",
      "closed_at": "2024-05-31 20:23:30+00:00",
      "resolution_days": 199.67332175925927
    },
    {
      "number": 1643,
      "title": "baichuan-13b-chat\u7528vllm\u6765\u751f\u6210\uff0c\u5f88\u591a\u6d4b\u8bd5\u6570\u636e\uff08\u6709\u957f\u6709\u77ed\uff0c\u6ca1\u6709\u8d85\u51fa\u957f\u5ea6\u9650\u5236\uff09\u53ea\u80fd\u751f\u6210\u4e00\u4e2a\u53e5\u53f7\uff0c\u800c\u4e14\u6709\u4e9b\u793a\u4f8b\u5728\u5220\u6389\u4e00\u4e9b\u5b57\u8bcd\u6216\u53e5\u5b50\u4e4b\u540e\uff0c\u5c31\u53ef\u4ee5\u6b63\u5e38\u751f\u6210\u4e86\uff0c\u8bf7\u95ee\u6709\u53ef\u80fd\u662f\u4ec0\u4e48\u539f\u56e0\uff1f",
      "state": "closed",
      "comments": 16,
      "body_length": 916,
      "label_names": [],
      "created_at": "2023-11-13 12:27:39+00:00",
      "closed_at": "2024-05-31 20:11:18+00:00",
      "resolution_days": 200.32197916666667
    },
    {
      "number": 1636,
      "title": "(Async) Batch request, OpenAI API server",
      "state": "closed",
      "comments": 11,
      "body_length": 495,
      "label_names": [],
      "created_at": "2023-11-13 00:31:44+00:00",
      "closed_at": "2023-12-01 08:51:57+00:00",
      "resolution_days": 18.347372685185185
    },
    {
      "number": 1635,
      "title": "Prompt caching",
      "state": "closed",
      "comments": 13,
      "body_length": 285,
      "label_names": [],
      "created_at": "2023-11-13 00:28:09+00:00",
      "closed_at": "2024-05-31 20:05:52+00:00",
      "resolution_days": 200.8178587962963
    },
    {
      "number": 1625,
      "title": "KeyError: 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_A.weight'",
      "state": "closed",
      "comments": 22,
      "body_length": 1609,
      "label_names": [],
      "created_at": "2023-11-11 07:00:25+00:00",
      "closed_at": "2023-11-15 19:37:38+00:00",
      "resolution_days": 4.525844907407407
    },
    {
      "number": 1610,
      "title": "Can S-Lora be integrated into vLLM?",
      "state": "closed",
      "comments": 19,
      "body_length": 384,
      "label_names": [],
      "created_at": "2023-11-09 15:29:10+00:00",
      "closed_at": "2024-02-28 01:00:11+00:00",
      "resolution_days": 110.39653935185186
    },
    {
      "number": 1574,
      "title": "Support for sparsity?",
      "state": "closed",
      "comments": 15,
      "body_length": 82,
      "label_names": [
        "feature request",
        "stale"
      ],
      "created_at": "2023-11-06 19:07:48+00:00",
      "closed_at": "2024-12-01 02:15:57+00:00",
      "resolution_days": 390.2973263888889
    },
    {
      "number": 1566,
      "title": "Tensor parallelism on ray cluster",
      "state": "closed",
      "comments": 19,
      "body_length": 365,
      "label_names": [],
      "created_at": "2023-11-05 07:43:16+00:00",
      "closed_at": "2024-05-31 20:25:45+00:00",
      "resolution_days": 208.5295023148148
    },
    {
      "number": 1559,
      "title": "Yarn-Mistral-7b-128k  set max_seq_len, CUDA OUT OF MEMORY....",
      "state": "closed",
      "comments": 12,
      "body_length": 952,
      "label_names": [],
      "created_at": "2023-11-04 06:28:06+00:00",
      "closed_at": "2023-11-04 19:05:43+00:00",
      "resolution_days": 0.5261226851851852
    },
    {
      "number": 1556,
      "title": "ValueError: Double free!",
      "state": "closed",
      "comments": 23,
      "body_length": 420,
      "label_names": [],
      "created_at": "2023-11-03 23:49:59+00:00",
      "closed_at": "2024-03-13 00:30:09+00:00",
      "resolution_days": 130.0278935185185
    },
    {
      "number": 1548,
      "title": " The detected CUDA version (11.8) mismatches the version that was used to compile       PyTorch (12.1). Please make sure to use the same CUDA versions.",
      "state": "closed",
      "comments": 11,
      "body_length": 592,
      "label_names": [],
      "created_at": "2023-11-02 08:54:18+00:00",
      "closed_at": "2024-03-25 11:21:01+00:00",
      "resolution_days": 144.1018865740741
    },
    {
      "number": 1479,
      "title": "Running out of memory with TheBloke/CodeLlama-7B-AWQ",
      "state": "closed",
      "comments": 20,
      "body_length": 3582,
      "label_names": [],
      "created_at": "2023-10-26 04:52:07+00:00",
      "closed_at": "2024-03-25 11:13:45+00:00",
      "resolution_days": 151.26502314814815
    },
    {
      "number": 1453,
      "title": "The detected CUDA version (11.8) mismatches the version that was used to compile       PyTorch (12.1). Please make sure to use the same CUDA versions",
      "state": "closed",
      "comments": 26,
      "body_length": 172,
      "label_names": [],
      "created_at": "2023-10-24 03:18:08+00:00",
      "closed_at": "2023-10-28 11:55:38+00:00",
      "resolution_days": 4.359375
    },
    {
      "number": 1441,
      "title": "Does vllm support the Mac/Metal/MPS? ",
      "state": "closed",
      "comments": 110,
      "body_length": 143,
      "label_names": [],
      "created_at": "2023-10-21 00:52:39+00:00",
      "closed_at": "2023-10-22 08:01:00+00:00",
      "resolution_days": 1.2974652777777778
    },
    {
      "number": 1403,
      "title": "[BUG] vLLM Baichuan model is not working in latest vLLM release",
      "state": "closed",
      "comments": 14,
      "body_length": 548,
      "label_names": [
        "bug"
      ],
      "created_at": "2023-10-17 20:41:39+00:00",
      "closed_at": "2023-12-01 02:35:51+00:00",
      "resolution_days": 44.24597222222222
    },
    {
      "number": 1400,
      "title": "Using VLLM with a Tesla T4 on SageMaker Studio (ml.g4dn.xlarge instance)",
      "state": "closed",
      "comments": 11,
      "body_length": 787,
      "label_names": [],
      "created_at": "2023-10-17 16:56:52+00:00",
      "closed_at": "2024-05-31 20:02:30+00:00",
      "resolution_days": 227.12891203703703
    },
    {
      "number": 1391,
      "title": " Could not build wheels for vllm, which is required to install pyproject.toml-based projects",
      "state": "closed",
      "comments": 28,
      "body_length": 238,
      "label_names": [
        "installation",
        "stale"
      ],
      "created_at": "2023-10-17 09:44:22+00:00",
      "closed_at": "2025-02-02 14:55:08+00:00",
      "resolution_days": 474.2158101851852
    },
    {
      "number": 1369,
      "title": "ImportError: libcudart.so.11.0: cannot open shared object file: No such file or directory",
      "state": "closed",
      "comments": 22,
      "body_length": 706,
      "label_names": [
        "installation"
      ],
      "created_at": "2023-10-16 08:51:56+00:00",
      "closed_at": "2024-04-04 08:05:58+00:00",
      "resolution_days": 170.9680787037037
    },
    {
      "number": 1363,
      "title": "How to deploy vllm model across multiple nodes in kubernetes?",
      "state": "closed",
      "comments": 13,
      "body_length": 402,
      "label_names": [],
      "created_at": "2023-10-16 02:24:40+00:00",
      "closed_at": "2024-03-28 13:37:33+00:00",
      "resolution_days": 164.4672800925926
    },
    {
      "number": 1351,
      "title": "[Error] 400 Bad Request",
      "state": "closed",
      "comments": 13,
      "body_length": 653,
      "label_names": [],
      "created_at": "2023-10-14 05:09:22+00:00",
      "closed_at": "2024-03-25 10:13:42+00:00",
      "resolution_days": 163.2113425925926
    },
    {
      "number": 1345,
      "title": "Quantization for V100",
      "state": "closed",
      "comments": 12,
      "body_length": 7558,
      "label_names": [
        "quantization",
        "stale"
      ],
      "created_at": "2023-10-13 16:44:17+00:00",
      "closed_at": "2024-12-01 02:16:03+00:00",
      "resolution_days": 414.3970601851852
    },
    {
      "number": 1312,
      "title": "vllm cutoff the output",
      "state": "closed",
      "comments": 16,
      "body_length": 561,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2023-10-10 11:14:36+00:00",
      "closed_at": "2024-12-01 02:16:04+00:00",
      "resolution_days": 417.6260185185185
    },
    {
      "number": 1304,
      "title": "Could you support Attention Sink?",
      "state": "closed",
      "comments": 11,
      "body_length": 267,
      "label_names": [
        "feature request",
        "stale"
      ],
      "created_at": "2023-10-10 01:40:30+00:00",
      "closed_at": "2024-12-01 02:16:06+00:00",
      "resolution_days": 418.02472222222224
    },
    {
      "number": 1293,
      "title": "Docker image ",
      "state": "closed",
      "comments": 12,
      "body_length": 47,
      "label_names": [],
      "created_at": "2023-10-08 04:27:57+00:00",
      "closed_at": "2024-03-05 13:06:01+00:00",
      "resolution_days": 149.3597685185185
    },
    {
      "number": 1237,
      "title": "Data parallel inference",
      "state": "closed",
      "comments": 29,
      "body_length": 745,
      "label_names": [
        "feature request"
      ],
      "created_at": "2023-09-30 23:24:38+00:00",
      "closed_at": "2024-09-13 17:00:25+00:00",
      "resolution_days": 348.73318287037034
    },
    {
      "number": 1236,
      "title": "AWQ broken when trying to add Mistral",
      "state": "closed",
      "comments": 14,
      "body_length": 3984,
      "label_names": [
        "bug"
      ],
      "created_at": "2023-09-30 20:01:16+00:00",
      "closed_at": "2023-10-11 19:56:34+00:00",
      "resolution_days": 10.996736111111112
    },
    {
      "number": 1234,
      "title": "Running out of memory loading 7B AWQ quantized models with 12GB vram",
      "state": "closed",
      "comments": 25,
      "body_length": 698,
      "label_names": [
        "performance",
        "stale"
      ],
      "created_at": "2023-09-30 18:09:45+00:00",
      "closed_at": "2024-12-01 02:16:11+00:00",
      "resolution_days": 427.33780092592593
    },
    {
      "number": 1208,
      "title": "3 gpu's not supported?",
      "state": "closed",
      "comments": 16,
      "body_length": 89,
      "label_names": [],
      "created_at": "2023-09-28 04:16:12+00:00",
      "closed_at": "2024-03-13 11:24:26+00:00",
      "resolution_days": 167.29738425925925
    },
    {
      "number": 1206,
      "title": "GPU KV cache usage: 100.0%\u4ee5\u540e\u5c31\u5361\u4f4f",
      "state": "closed",
      "comments": 21,
      "body_length": 63,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2023-09-28 01:13:13+00:00",
      "closed_at": "2024-12-01 02:16:12+00:00",
      "resolution_days": 430.04373842592594
    },
    {
      "number": 1200,
      "title": "Key Error when handle multiple requests simultaneously",
      "state": "closed",
      "comments": 15,
      "body_length": 1342,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2023-09-27 18:47:31+00:00",
      "closed_at": "2024-12-01 02:16:16+00:00",
      "resolution_days": 430.31163194444446
    },
    {
      "number": 1191,
      "title": "JSON formatting issue",
      "state": "closed",
      "comments": 16,
      "body_length": 121,
      "label_names": [],
      "created_at": "2023-09-27 07:28:04+00:00",
      "closed_at": "2024-03-28 12:06:50+00:00",
      "resolution_days": 183.19358796296297
    },
    {
      "number": 1171,
      "title": "[Discussion] Will vLLM consider using Speculative Sampling to accelerating LLM decoding?",
      "state": "closed",
      "comments": 25,
      "body_length": 638,
      "label_names": [],
      "created_at": "2023-09-25 07:15:57+00:00",
      "closed_at": "2024-05-18 11:07:20+00:00",
      "resolution_days": 236.16068287037038
    },
    {
      "number": 1167,
      "title": "Phi 1.5 support",
      "state": "closed",
      "comments": 15,
      "body_length": 404,
      "label_names": [
        "new-model"
      ],
      "created_at": "2023-09-24 18:08:58+00:00",
      "closed_at": "2023-11-16 22:28:40+00:00",
      "resolution_days": 53.180347222222224
    },
    {
      "number": 1137,
      "title": "test benchmark,baichuan2-13B slow?why",
      "state": "closed",
      "comments": 20,
      "body_length": 118,
      "label_names": [],
      "created_at": "2023-09-22 03:15:34+00:00",
      "closed_at": "2024-03-13 12:59:02+00:00",
      "resolution_days": 173.4051851851852
    },
    {
      "number": 1131,
      "title": "vLLM to add a locally trained model",
      "state": "closed",
      "comments": 14,
      "body_length": 163,
      "label_names": [],
      "created_at": "2023-09-21 17:04:08+00:00",
      "closed_at": "2024-03-13 11:23:54+00:00",
      "resolution_days": 173.76372685185186
    },
    {
      "number": 1100,
      "title": "gptq  Qwen-7B-Chat-Int4 load_error",
      "state": "closed",
      "comments": 16,
      "body_length": 112,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2023-09-19 09:55:14+00:00",
      "closed_at": "2024-12-01 02:16:21+00:00",
      "resolution_days": 438.6813310185185
    },
    {
      "number": 1070,
      "title": "Can't build vLLM from Docker due to AWQ's minimum architecture requirements - TORCH_CUDA_ARCH_LIST does not help",
      "state": "closed",
      "comments": 14,
      "body_length": 10216,
      "label_names": [],
      "created_at": "2023-09-17 09:12:52+00:00",
      "closed_at": "2023-09-26 17:21:09+00:00",
      "resolution_days": 9.339085648148147
    },
    {
      "number": 1069,
      "title": "Inconsistent results between HuggingFace Transformers and vllm",
      "state": "closed",
      "comments": 19,
      "body_length": 7598,
      "label_names": [],
      "created_at": "2023-09-17 08:53:00+00:00",
      "closed_at": "2024-04-04 08:04:41+00:00",
      "resolution_days": 199.96644675925927
    },
    {
      "number": 1058,
      "title": "vllm hangs when reinitializing ray",
      "state": "closed",
      "comments": 21,
      "body_length": 4486,
      "label_names": [
        "bug"
      ],
      "created_at": "2023-09-15 19:03:58+00:00",
      "closed_at": "2024-06-13 09:04:29+00:00",
      "resolution_days": 271.58369212962964
    },
    {
      "number": 1026,
      "title": "Long prompt will block all running generation.",
      "state": "closed",
      "comments": 19,
      "body_length": 344,
      "label_names": [],
      "created_at": "2023-09-13 04:52:48+00:00",
      "closed_at": "2024-02-22 11:07:36+00:00",
      "resolution_days": 162.2602777777778
    },
    {
      "number": 1002,
      "title": "GGUF support",
      "state": "closed",
      "comments": 49,
      "body_length": 505,
      "label_names": [
        "feature request"
      ],
      "created_at": "2023-09-10 00:49:07+00:00",
      "closed_at": "2024-08-05 23:54:24+00:00",
      "resolution_days": 330.9620023148148
    },
    {
      "number": 967,
      "title": "Falcon 180B support?",
      "state": "closed",
      "comments": 19,
      "body_length": 98,
      "label_names": [],
      "created_at": "2023-09-06 18:26:49+00:00",
      "closed_at": "2024-03-08 12:25:03+00:00",
      "resolution_days": 183.74877314814816
    },
    {
      "number": 966,
      "title": "Non-deterministic outputs for llama2",
      "state": "closed",
      "comments": 15,
      "body_length": 368,
      "label_names": [
        "bug"
      ],
      "created_at": "2023-09-06 17:34:29+00:00",
      "closed_at": "2024-02-10 06:37:48+00:00",
      "resolution_days": 156.54396990740742
    },
    {
      "number": 963,
      "title": "Support for compute capability <7.0",
      "state": "closed",
      "comments": 24,
      "body_length": 234,
      "label_names": [],
      "created_at": "2023-09-06 12:48:59+00:00",
      "closed_at": "2023-09-11 15:18:39+00:00",
      "resolution_days": 5.103935185185185
    },
    {
      "number": 962,
      "title": "can model  Qwen/Qwen-VL-Chat work well?",
      "state": "closed",
      "comments": 11,
      "body_length": 2442,
      "label_names": [
        "new-model"
      ],
      "created_at": "2023-09-06 10:18:59+00:00",
      "closed_at": "2024-09-05 12:48:12+00:00",
      "resolution_days": 365.1036226851852
    },
    {
      "number": 912,
      "title": "I want to close kv cache. if i set gpu_memory_utilization is 0. Does it means that i close the kv cache?",
      "state": "closed",
      "comments": 14,
      "body_length": 0,
      "label_names": [],
      "created_at": "2023-08-30 11:14:06+00:00",
      "closed_at": "2024-05-31 19:48:43+00:00",
      "resolution_days": 275.35737268518517
    },
    {
      "number": 905,
      "title": "vLLM doesn't support context length exceeding about 13k",
      "state": "closed",
      "comments": 12,
      "body_length": 684,
      "label_names": [
        "bug"
      ],
      "created_at": "2023-08-29 13:18:15+00:00",
      "closed_at": "2023-09-27 05:27:14+00:00",
      "resolution_days": 28.672905092592593
    },
    {
      "number": 904,
      "title": "stuck at llm_engine.py:196",
      "state": "closed",
      "comments": 11,
      "body_length": 849,
      "label_names": [
        "bug"
      ],
      "created_at": "2023-08-29 12:53:20+00:00",
      "closed_at": "2023-09-20 20:35:13+00:00",
      "resolution_days": 22.320752314814815
    },
    {
      "number": 901,
      "title": "Qwen-7b-Chat Test",
      "state": "closed",
      "comments": 15,
      "body_length": 1258,
      "label_names": [],
      "created_at": "2023-08-29 08:51:19+00:00",
      "closed_at": "2023-09-19 06:11:14+00:00",
      "resolution_days": 20.88883101851852
    },
    {
      "number": 900,
      "title": "Any plan to support Code Llama?",
      "state": "closed",
      "comments": 15,
      "body_length": 246,
      "label_names": [],
      "created_at": "2023-08-29 06:42:09+00:00",
      "closed_at": "2023-09-11 14:54:58+00:00",
      "resolution_days": 13.342233796296297
    },
    {
      "number": 878,
      "title": "SIGABRT - Fatal Python error: Aborted when running vllm on llama2-7b with --tensor-parallel-size 2",
      "state": "closed",
      "comments": 16,
      "body_length": 17385,
      "label_names": [],
      "created_at": "2023-08-25 20:05:06+00:00",
      "closed_at": "2024-05-31 19:35:11+00:00",
      "resolution_days": 279.97922453703706
    },
    {
      "number": 787,
      "title": "Aborted due to the lack of CPU swap space, but there is enough RAM and swap",
      "state": "closed",
      "comments": 14,
      "body_length": 2288,
      "label_names": [],
      "created_at": "2023-08-17 23:55:22+00:00",
      "closed_at": "2023-10-15 20:41:06+00:00",
      "resolution_days": 58.865092592592596
    },
    {
      "number": 767,
      "title": "CUDA error: an illegal memory acces with Falcon 40B",
      "state": "closed",
      "comments": 15,
      "body_length": 453,
      "label_names": [
        "bug"
      ],
      "created_at": "2023-08-16 00:11:11+00:00",
      "closed_at": "2023-09-10 08:39:04+00:00",
      "resolution_days": 25.35269675925926
    },
    {
      "number": 761,
      "title": "WizardCoder 15B not given proper output",
      "state": "closed",
      "comments": 13,
      "body_length": 2943,
      "label_names": [],
      "created_at": "2023-08-14 14:55:43+00:00",
      "closed_at": "2023-08-25 08:29:06+00:00",
      "resolution_days": 10.731516203703704
    },
    {
      "number": 747,
      "title": "Maybe Wrong implementation of AttentionWithRoPE for GPTJ and GPT-NeoX?",
      "state": "closed",
      "comments": 11,
      "body_length": 1035,
      "label_names": [
        "bug"
      ],
      "created_at": "2023-08-12 14:58:49+00:00",
      "closed_at": "2023-09-04 16:59:22+00:00",
      "resolution_days": 23.083715277777777
    },
    {
      "number": 744,
      "title": "Add Support for Quantized Model in VLLM - $500 Reward",
      "state": "closed",
      "comments": 35,
      "body_length": 313,
      "label_names": [],
      "created_at": "2023-08-12 08:52:21+00:00",
      "closed_at": "2024-03-08 11:21:43+00:00",
      "resolution_days": 209.10372685185186
    },
    {
      "number": 721,
      "title": "Issue with raylet error",
      "state": "closed",
      "comments": 18,
      "body_length": 557,
      "label_names": [],
      "created_at": "2023-08-09 22:54:10+00:00",
      "closed_at": "2024-05-18 10:54:44+00:00",
      "resolution_days": 282.50039351851854
    },
    {
      "number": 712,
      "title": "Potential degredation in sampling/too repetitive",
      "state": "closed",
      "comments": 26,
      "body_length": 1425,
      "label_names": [],
      "created_at": "2023-08-09 05:16:26+00:00",
      "closed_at": "2024-03-13 12:58:53+00:00",
      "resolution_days": 217.32114583333333
    },
    {
      "number": 704,
      "title": "why not support llama 7b\uff1f",
      "state": "closed",
      "comments": 12,
      "body_length": 127,
      "label_names": [],
      "created_at": "2023-08-08 12:36:53+00:00",
      "closed_at": "2023-09-11 14:58:27+00:00",
      "resolution_days": 34.098310185185184
    },
    {
      "number": 694,
      "title": "Memory leak while using tensor_parallel_size>1",
      "state": "closed",
      "comments": 13,
      "body_length": 101,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2023-08-08 02:01:16+00:00",
      "closed_at": "2025-05-16 08:07:55+00:00",
      "resolution_days": 647.2546180555555
    },
    {
      "number": 645,
      "title": "The IPv6 network addresses of (__internal_head__, 18566) cannot be retrieved",
      "state": "closed",
      "comments": 13,
      "body_length": 1175,
      "label_names": [],
      "created_at": "2023-08-02 08:32:45+00:00",
      "closed_at": "2023-08-04 07:30:44+00:00",
      "resolution_days": 1.9569328703703703
    },
    {
      "number": 641,
      "title": "RuntimeError: probability tensor contains either `inf`, `nan` or element < 0",
      "state": "closed",
      "comments": 19,
      "body_length": 10600,
      "label_names": [
        "bug"
      ],
      "created_at": "2023-08-02 02:09:23+00:00",
      "closed_at": "2023-09-04 00:20:08+00:00",
      "resolution_days": 32.92413194444445
    },
    {
      "number": 629,
      "title": "RuntimeError: CUDA error: no kernel image is available for execution on the device",
      "state": "closed",
      "comments": 33,
      "body_length": 768,
      "label_names": [],
      "created_at": "2023-07-31 19:25:24+00:00",
      "closed_at": "2024-03-25 10:04:32+00:00",
      "resolution_days": 237.61050925925926
    },
    {
      "number": 621,
      "title": "Installing with ROCM",
      "state": "closed",
      "comments": 44,
      "body_length": 369,
      "label_names": [
        "feature request"
      ],
      "created_at": "2023-07-30 22:31:29+00:00",
      "closed_at": "2024-07-04 13:42:10+00:00",
      "resolution_days": 339.63241898148146
    },
    {
      "number": 615,
      "title": "Llama2 answers is noise",
      "state": "closed",
      "comments": 17,
      "body_length": 1093,
      "label_names": [
        "bug"
      ],
      "created_at": "2023-07-28 21:54:16+00:00",
      "closed_at": "2024-03-25 09:43:43+00:00",
      "resolution_days": 240.49267361111112
    },
    {
      "number": 608,
      "title": "When using the same prompt and greedy sampling params, the output is not same before and after the two times",
      "state": "closed",
      "comments": 23,
      "body_length": 0,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2023-07-28 04:02:58+00:00",
      "closed_at": "2024-12-29 02:05:49+00:00",
      "resolution_days": 519.9186458333334
    },
    {
      "number": 586,
      "title": "OOM killer - actor is dead",
      "state": "closed",
      "comments": 11,
      "body_length": 905,
      "label_names": [],
      "created_at": "2023-07-26 09:50:03+00:00",
      "closed_at": "2023-08-07 22:19:05+00:00",
      "resolution_days": 12.520162037037037
    },
    {
      "number": 581,
      "title": "\u591agpus\u5982\u4f55\u4f7f\u7528\uff1f",
      "state": "closed",
      "comments": 32,
      "body_length": 0,
      "label_names": [
        "usage"
      ],
      "created_at": "2023-07-26 06:08:38+00:00",
      "closed_at": "2024-10-26 16:38:36+00:00",
      "resolution_days": 458.43747685185184
    },
    {
      "number": 572,
      "title": "ModuleNotFoundError: No module named 'transformers_modules' with API serving using baichuan-7b",
      "state": "closed",
      "comments": 14,
      "body_length": 1429,
      "label_names": [],
      "created_at": "2023-07-25 10:40:48+00:00",
      "closed_at": "2023-08-07 21:47:30+00:00",
      "resolution_days": 13.46298611111111
    },
    {
      "number": 570,
      "title": "[Llama-2-13b-chat-hf] IPv6 Network Address Retrieval Error on 4 V100s 16GB",
      "state": "closed",
      "comments": 14,
      "body_length": 1159,
      "label_names": [],
      "created_at": "2023-07-25 08:14:00+00:00",
      "closed_at": "2023-08-07 21:43:42+00:00",
      "resolution_days": 13.562291666666667
    },
    {
      "number": 551,
      "title": "Support custom stop function?",
      "state": "closed",
      "comments": 12,
      "body_length": 153,
      "label_names": [
        "feature request"
      ],
      "created_at": "2023-07-23 14:37:20+00:00",
      "closed_at": "2024-03-13 11:16:01+00:00",
      "resolution_days": 233.86019675925925
    },
    {
      "number": 546,
      "title": "vLLM stops all processing when CPU KV cache is used, has to be shut down and restarted.",
      "state": "closed",
      "comments": 17,
      "body_length": 1973,
      "label_names": [
        "bug"
      ],
      "created_at": "2023-07-21 17:26:44+00:00",
      "closed_at": "2025-02-02 14:50:46+00:00",
      "resolution_days": 561.8916898148149
    },
    {
      "number": 541,
      "title": "NVIDIA Triton support",
      "state": "closed",
      "comments": 28,
      "body_length": 2331,
      "label_names": [
        "feature request"
      ],
      "created_at": "2023-07-21 06:06:13+00:00",
      "closed_at": "2023-09-26 03:20:19+00:00",
      "resolution_days": 66.88479166666667
    },
    {
      "number": 525,
      "title": "LlaMA 2: Input prompt (2664 tokens) is too long and exceeds limit of 2048/2560",
      "state": "closed",
      "comments": 13,
      "body_length": 265,
      "label_names": [
        "bug"
      ],
      "created_at": "2023-07-20 05:00:28+00:00",
      "closed_at": "2023-08-02 20:35:12+00:00",
      "resolution_days": 13.64912037037037
    },
    {
      "number": 501,
      "title": "Support LLaMA-2",
      "state": "closed",
      "comments": 13,
      "body_length": 0,
      "label_names": [],
      "created_at": "2023-07-18 18:30:11+00:00",
      "closed_at": "2023-07-20 18:38:29+00:00",
      "resolution_days": 2.0057638888888887
    },
    {
      "number": 490,
      "title": "Baichuan model can not be run",
      "state": "closed",
      "comments": 12,
      "body_length": 287,
      "label_names": [
        "bug"
      ],
      "created_at": "2023-07-18 06:16:58+00:00",
      "closed_at": "2024-03-08 10:20:50+00:00",
      "resolution_days": 234.16935185185184
    },
    {
      "number": 485,
      "title": "Flash Attention V2",
      "state": "closed",
      "comments": 14,
      "body_length": 207,
      "label_names": [
        "feature request"
      ],
      "created_at": "2023-07-17 20:48:41+00:00",
      "closed_at": "2023-08-25 08:49:53+00:00",
      "resolution_days": 38.50083333333333
    },
    {
      "number": 464,
      "title": "RoPE scaling support?",
      "state": "closed",
      "comments": 11,
      "body_length": 166,
      "label_names": [
        "feature request"
      ],
      "created_at": "2023-07-14 15:51:59+00:00",
      "closed_at": "2023-09-27 10:37:05+00:00",
      "resolution_days": 74.78131944444445
    },
    {
      "number": 451,
      "title": "install error",
      "state": "closed",
      "comments": 11,
      "body_length": 1684,
      "label_names": [
        "installation"
      ],
      "created_at": "2023-07-13 02:02:39+00:00",
      "closed_at": "2023-08-07 08:48:31+00:00",
      "resolution_days": 25.28185185185185
    },
    {
      "number": 450,
      "title": "Inference with LLaMA 65B generates nothing but \\n",
      "state": "closed",
      "comments": 24,
      "body_length": 1570,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2023-07-13 01:28:55+00:00",
      "closed_at": "2024-11-30 02:03:32+00:00",
      "resolution_days": 506.02403935185185
    },
    {
      "number": 437,
      "title": "ubuntu install vllm errors",
      "state": "closed",
      "comments": 12,
      "body_length": 20872,
      "label_names": [
        "installation"
      ],
      "created_at": "2023-07-12 03:47:15+00:00",
      "closed_at": "2024-04-04 07:56:15+00:00",
      "resolution_days": 267.17291666666665
    },
    {
      "number": 421,
      "title": "+34% higher throughput?",
      "state": "closed",
      "comments": 46,
      "body_length": 17843,
      "label_names": [
        "feature request"
      ],
      "created_at": "2023-07-10 22:21:20+00:00",
      "closed_at": "2024-05-18 11:07:34+00:00",
      "resolution_days": 312.5321064814815
    },
    {
      "number": 416,
      "title": "[Feature Request] Support input embedding in `LLM.generate()`",
      "state": "closed",
      "comments": 17,
      "body_length": 1202,
      "label_names": [
        "feature request"
      ],
      "created_at": "2023-07-10 08:28:50+00:00",
      "closed_at": "2025-05-02 08:06:40+00:00",
      "resolution_days": 661.9846064814815
    },
    {
      "number": 406,
      "title": "Cuda failure 'peer access is not supported between these two devices'",
      "state": "closed",
      "comments": 15,
      "body_length": 2404,
      "label_names": [
        "bug"
      ],
      "created_at": "2023-07-08 23:46:38+00:00",
      "closed_at": "2023-07-20 06:10:33+00:00",
      "resolution_days": 11.266608796296296
    },
    {
      "number": 392,
      "title": "Loading quantized models",
      "state": "closed",
      "comments": 31,
      "body_length": 791,
      "label_names": [],
      "created_at": "2023-07-07 13:08:28+00:00",
      "closed_at": "2024-01-31 03:21:39+00:00",
      "resolution_days": 207.59248842592592
    },
    {
      "number": 387,
      "title": "pipeline parallel support in the future\uff1f",
      "state": "closed",
      "comments": 14,
      "body_length": 134,
      "label_names": [
        "feature request"
      ],
      "created_at": "2023-07-07 08:40:19+00:00",
      "closed_at": "2024-08-28 18:58:00+00:00",
      "resolution_days": 418.42894675925925
    },
    {
      "number": 367,
      "title": "Tensor Parallelism vs Data Parallelism",
      "state": "closed",
      "comments": 14,
      "body_length": 673,
      "label_names": [],
      "created_at": "2023-07-05 07:01:22+00:00",
      "closed_at": "2024-03-08 11:14:53+00:00",
      "resolution_days": 247.17605324074074
    },
    {
      "number": 354,
      "title": "Loading Models that require execution of third party code (trust_remote_code=True)",
      "state": "closed",
      "comments": 15,
      "body_length": 603,
      "label_names": [
        "bug"
      ],
      "created_at": "2023-07-04 08:05:46+00:00",
      "closed_at": "2024-03-08 10:22:14+00:00",
      "resolution_days": 248.09476851851852
    },
    {
      "number": 340,
      "title": "Decode error while inferencing a batch of prompts",
      "state": "closed",
      "comments": 22,
      "body_length": 2094,
      "label_names": [
        "bug"
      ],
      "created_at": "2023-07-03 10:42:46+00:00",
      "closed_at": "2024-03-29 15:19:01+00:00",
      "resolution_days": 270.19184027777777
    },
    {
      "number": 322,
      "title": "ray OOM in tensor parallel",
      "state": "closed",
      "comments": 27,
      "body_length": 6889,
      "label_names": [
        "bug"
      ],
      "created_at": "2023-06-30 09:27:50+00:00",
      "closed_at": "2024-03-20 12:35:08+00:00",
      "resolution_days": 264.1300694444444
    },
    {
      "number": 307,
      "title": "[Question] Usage with Multimodal LLM",
      "state": "closed",
      "comments": 13,
      "body_length": 430,
      "label_names": [
        "new-model",
        "feature request"
      ],
      "created_at": "2023-06-29 08:13:05+00:00",
      "closed_at": "2024-04-03 17:13:53+00:00",
      "resolution_days": 279.37555555555554
    },
    {
      "number": 299,
      "title": "Support Multiple Models",
      "state": "closed",
      "comments": 89,
      "body_length": 181,
      "label_names": [
        "feature request"
      ],
      "created_at": "2023-06-28 19:14:50+00:00",
      "closed_at": "2024-09-04 04:24:59+00:00",
      "resolution_days": 433.3820486111111
    },
    {
      "number": 296,
      "title": "Feature request\uff1asupport ExLlama",
      "state": "closed",
      "comments": 15,
      "body_length": 205,
      "label_names": [
        "feature request",
        "stale"
      ],
      "created_at": "2023-06-28 17:21:31+00:00",
      "closed_at": "2024-11-30 02:03:33+00:00",
      "resolution_days": 520.3625231481482
    },
    {
      "number": 295,
      "title": "8bit support",
      "state": "closed",
      "comments": 12,
      "body_length": 382,
      "label_names": [],
      "created_at": "2023-06-28 16:08:06+00:00",
      "closed_at": "2024-03-20 12:30:21+00:00",
      "resolution_days": 265.8487847222222
    },
    {
      "number": 288,
      "title": "Support for Constrained decoding",
      "state": "closed",
      "comments": 32,
      "body_length": 392,
      "label_names": [
        "good first issue",
        "feature request"
      ],
      "created_at": "2023-06-28 09:23:14+00:00",
      "closed_at": "2024-03-19 22:46:11+00:00",
      "resolution_days": 265.5576041666667
    },
    {
      "number": 286,
      "title": "Long context will cause the vLLM stop",
      "state": "closed",
      "comments": 12,
      "body_length": 243,
      "label_names": [
        "bug"
      ],
      "created_at": "2023-06-28 07:07:46+00:00",
      "closed_at": "2023-11-05 01:30:47+00:00",
      "resolution_days": 129.7659837962963
    },
    {
      "number": 247,
      "title": "when to support chatglm2-6b? ",
      "state": "closed",
      "comments": 24,
      "body_length": 75,
      "label_names": [
        "new-model"
      ],
      "created_at": "2023-06-26 02:28:58+00:00",
      "closed_at": "2024-03-06 16:44:27+00:00",
      "resolution_days": 254.59408564814814
    },
    {
      "number": 244,
      "title": "[Roadmap] vLLM Development Roadmap: H2 2023",
      "state": "closed",
      "comments": 16,
      "body_length": 3778,
      "label_names": [],
      "created_at": "2023-06-25 17:39:15+00:00",
      "closed_at": "2024-01-31 06:32:57+00:00",
      "resolution_days": 219.53729166666668
    },
    {
      "number": 214,
      "title": "`8-bit quantization` support",
      "state": "closed",
      "comments": 14,
      "body_length": 217,
      "label_names": [
        "feature request"
      ],
      "created_at": "2023-06-22 23:02:03+00:00",
      "closed_at": "2024-04-18 13:52:08+00:00",
      "resolution_days": 300.6181134259259
    },
    {
      "number": 208,
      "title": "Remove Ray for the dependency",
      "state": "closed",
      "comments": 12,
      "body_length": 241,
      "label_names": [
        "feature request"
      ],
      "created_at": "2023-06-22 16:07:39+00:00",
      "closed_at": "2024-05-18 10:56:34+00:00",
      "resolution_days": 330.7839699074074
    },
    {
      "number": 194,
      "title": "Ubuntu pip installation issue",
      "state": "closed",
      "comments": 15,
      "body_length": 6982,
      "label_names": [
        "installation"
      ],
      "created_at": "2023-06-21 16:29:21+00:00",
      "closed_at": "2024-03-06 11:14:37+00:00",
      "resolution_days": 258.78143518518516
    },
    {
      "number": 188,
      "title": "CUDA error: out of memory",
      "state": "closed",
      "comments": 54,
      "body_length": 2667,
      "label_names": [
        "bug"
      ],
      "created_at": "2023-06-21 13:50:20+00:00",
      "closed_at": "2023-06-27 14:50:09+00:00",
      "resolution_days": 6.041539351851852
    },
    {
      "number": 187,
      "title": "Adding support for encoder-decoder models, like T5 or BART",
      "state": "closed",
      "comments": 33,
      "body_length": 128,
      "label_names": [
        "new-model"
      ],
      "created_at": "2023-06-21 13:24:04+00:00",
      "closed_at": "2024-09-12 10:12:05+00:00",
      "resolution_days": 448.8666782407407
    },
    {
      "number": 185,
      "title": "Can I directly obtain the logits here?",
      "state": "closed",
      "comments": 12,
      "body_length": 543,
      "label_names": [],
      "created_at": "2023-06-21 11:40:56+00:00",
      "closed_at": "2023-06-22 09:10:04+00:00",
      "resolution_days": 0.8952314814814815
    },
    {
      "number": 182,
      "title": "Would it be possible to support LoRA fine-tuned models?",
      "state": "closed",
      "comments": 47,
      "body_length": 193,
      "label_names": [
        "feature request"
      ],
      "created_at": "2023-06-21 07:53:13+00:00",
      "closed_at": "2024-03-06 08:54:02+00:00",
      "resolution_days": 259.0422337962963
    },
    {
      "number": 180,
      "title": "Whisper support",
      "state": "closed",
      "comments": 40,
      "body_length": 110,
      "label_names": [
        "new-model"
      ],
      "created_at": "2023-06-21 07:06:07+00:00",
      "closed_at": "2025-01-03 08:39:21+00:00",
      "resolution_days": 562.0647453703704
    },
    {
      "number": 176,
      "title": "Any plan to support cpu only mode?",
      "state": "closed",
      "comments": 15,
      "body_length": 129,
      "label_names": [
        "feature request"
      ],
      "created_at": "2023-06-21 03:59:42+00:00",
      "closed_at": "2024-04-18 11:32:50+00:00",
      "resolution_days": 302.31467592592594
    },
    {
      "number": 174,
      "title": "GPTQ / Quantization support?",
      "state": "closed",
      "comments": 19,
      "body_length": 36,
      "label_names": [
        "feature request"
      ],
      "created_at": "2023-06-21 02:40:47+00:00",
      "closed_at": "2024-03-06 09:01:49+00:00",
      "resolution_days": 259.2646064814815
    },
    {
      "number": 173,
      "title": "ValueError: The precision of the fractional quantity of resource",
      "state": "closed",
      "comments": 11,
      "body_length": 1746,
      "label_names": [
        "bug"
      ],
      "created_at": "2023-06-21 01:15:10+00:00",
      "closed_at": "2023-06-21 16:21:42+00:00",
      "resolution_days": 0.629537037037037
    },
    {
      "number": 129,
      "title": "Build failure due to CUDA version mismatch",
      "state": "closed",
      "comments": 25,
      "body_length": 348,
      "label_names": [
        "installation"
      ],
      "created_at": "2023-05-26 04:35:42+00:00",
      "closed_at": "2024-04-20 12:10:09+00:00",
      "resolution_days": 330.3155902777778
    },
    {
      "number": 125,
      "title": "Implement custom kernels for top-k and top-p sampling",
      "state": "closed",
      "comments": 20,
      "body_length": 220,
      "label_names": [
        "help wanted",
        "performance",
        "stale"
      ],
      "created_at": "2023-05-25 01:11:46+00:00",
      "closed_at": "2024-11-30 02:03:36+00:00",
      "resolution_days": 555.0359953703704
    },
    {
      "number": 61,
      "title": "Support BLOOM",
      "state": "closed",
      "comments": 13,
      "body_length": 174,
      "label_names": [
        "new-model"
      ],
      "created_at": "2023-05-03 20:31:38+00:00",
      "closed_at": "2023-07-03 20:12:37+00:00",
      "resolution_days": 60.98679398148148
    },
    {
      "number": 20862,
      "title": "[Usage]: vLLM Server Launch Freezes at Using NCCL on B200",
      "state": "open",
      "comments": 20,
      "body_length": 22084,
      "label_names": [
        "usage"
      ],
      "created_at": "2025-07-12 20:31:16+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 20468,
      "title": "[Feature]: Support EPLB for More MoE Models, e.g. Qwen 3, Llama 4",
      "state": "open",
      "comments": 20,
      "body_length": 2136,
      "label_names": [
        "good first issue",
        "feature request"
      ],
      "created_at": "2025-07-04 05:06:31+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 20341,
      "title": "[Bug]: No output / Repeated outputs when using Gemma 3  on vLLM",
      "state": "open",
      "comments": 16,
      "body_length": 2111,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-07-01 22:35:38+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 20336,
      "title": "[Roadmap] vLLM Roadmap Q3 2025",
      "state": "open",
      "comments": 12,
      "body_length": 5540,
      "label_names": [],
      "created_at": "2025-07-01 21:05:19+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 20226,
      "title": "[Bug]: RuntimeError: NCCL error: unhandled cuda error",
      "state": "open",
      "comments": 11,
      "body_length": 29427,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-06-29 17:07:52+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 20098,
      "title": "[RFC]: Lazy CUDA Graph capture",
      "state": "open",
      "comments": 11,
      "body_length": 1097,
      "label_names": [
        "RFC",
        "startup-ux"
      ],
      "created_at": "2025-06-25 21:27:51+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 20060,
      "title": "[Bug]: Sampling discrepancy between ollama and vLLM for gemma-3-27b-it et al.",
      "state": "open",
      "comments": 36,
      "body_length": 10853,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-06-25 06:26:20+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 20025,
      "title": "[Bug]: Mistrall Small 3.2 doesn't work with images",
      "state": "open",
      "comments": 16,
      "body_length": 5086,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-06-24 15:11:18+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 20015,
      "title": "[Usage]: Why does the Prefix cache hit rate reach 60% for random data during benchmark?",
      "state": "open",
      "comments": 14,
      "body_length": 14834,
      "label_names": [
        "usage"
      ],
      "created_at": "2025-06-24 09:53:42+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 20001,
      "title": "[Performance]: For LoRA models, performance drops in versions other than 0.8.2.",
      "state": "open",
      "comments": 15,
      "body_length": 22300,
      "label_names": [
        "performance"
      ],
      "created_at": "2025-06-24 02:31:12+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 19854,
      "title": "[RFC]: KV cache offloading",
      "state": "open",
      "comments": 21,
      "body_length": 3435,
      "label_names": [
        "RFC"
      ],
      "created_at": "2025-06-19 10:52:43+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 19491,
      "title": "[Bug]: vLLM outputs are not reproducible",
      "state": "open",
      "comments": 16,
      "body_length": 13212,
      "label_names": [
        "bug",
        "torch.compile"
      ],
      "created_at": "2025-06-11 14:40:27+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 19483,
      "title": "[Bug]: Docker vLLM 0.9.1 CUDA error: an illegal memory access, sampled_token_ids.tolist()",
      "state": "open",
      "comments": 21,
      "body_length": 37137,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-06-11 09:17:05+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 19403,
      "title": "[Bug]: Issue of Unstable Output for Identical Queries",
      "state": "open",
      "comments": 23,
      "body_length": 9389,
      "label_names": [
        "bug",
        "torch.compile"
      ],
      "created_at": "2025-06-10 07:07:59+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 19166,
      "title": "[Bug]: CUDA Illegal Access Error when running 2x Blackwell for tensor or pipeline parallel.",
      "state": "open",
      "comments": 12,
      "body_length": 9864,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-06-04 23:03:22+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 19097,
      "title": "[RFC]: Response format extensions for structured outputs",
      "state": "open",
      "comments": 11,
      "body_length": 3200,
      "label_names": [
        "structured-output",
        "RFC",
        "v1"
      ],
      "created_at": "2025-06-03 17:05:05+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 19070,
      "title": "[Bug]: ValueError: Attempted to assign 119 = 119 multimodal tokens to 120 placeholders",
      "state": "open",
      "comments": 39,
      "body_length": 6747,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-06-03 09:21:37+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 19056,
      "title": "[Bug]: Hermes tool parser stream output error in Qwen3 case",
      "state": "open",
      "comments": 11,
      "body_length": 37476,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-06-03 06:26:37+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 19020,
      "title": "[Feature]: online bf16->fp8 (and maybe even fp4?) quantization in `load_weights(...)` (so not only from disk checkpoint, but importantly from VRAM or RAM) for weights reloading in GRPO rollout workloads, and speed gains for generating long reasoning rollouts and peak VRAM reduction",
      "state": "open",
      "comments": 11,
      "body_length": 2153,
      "label_names": [
        "feature request"
      ],
      "created_at": "2025-06-02 13:01:17+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 19001,
      "title": "[Usage]: How to use DeepSeek-R1-0528-Qwen3-8B with function call",
      "state": "open",
      "comments": 46,
      "body_length": 13582,
      "label_names": [
        "usage"
      ],
      "created_at": "2025-06-01 13:18:43+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 18851,
      "title": "[Bug]: Strange error `AssertionError: failed to get the hash of the compiled graph` when running `Qwen/Qwen3-8B` via `LLM` class",
      "state": "open",
      "comments": 20,
      "body_length": 14087,
      "label_names": [
        "bug",
        "torch.compile"
      ],
      "created_at": "2025-05-28 19:04:20+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 18814,
      "title": "[Bug] TP=2 fails on dual RTX 5090: TorchInductor compile error or CUDA illegal memory access (TP=1 works)",
      "state": "open",
      "comments": 20,
      "body_length": 56237,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-05-28 07:04:26+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 18793,
      "title": "[New Model]: ByteDance-Seed/BAGEL-7B-MoT",
      "state": "open",
      "comments": 17,
      "body_length": 560,
      "label_names": [
        "new-model"
      ],
      "created_at": "2025-05-28 03:01:37+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 18679,
      "title": "[Usage]:",
      "state": "open",
      "comments": 23,
      "body_length": 814,
      "label_names": [
        "usage"
      ],
      "created_at": "2025-05-25 10:22:20+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 18619,
      "title": "[Bug][PERF]: Qwen2.5 performance degradation 0.8.4 -> 0.8.5",
      "state": "open",
      "comments": 34,
      "body_length": 15033,
      "label_names": [
        "bug",
        "qwen"
      ],
      "created_at": "2025-05-23 14:48:43+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 18571,
      "title": "[RFC]: Deprecating vLLM V0",
      "state": "open",
      "comments": 48,
      "body_length": 3524,
      "label_names": [
        "RFC"
      ],
      "created_at": "2025-05-22 21:45:16+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 18476,
      "title": "[New Model]: Gemma 3n support",
      "state": "open",
      "comments": 20,
      "body_length": 522,
      "label_names": [],
      "created_at": "2025-05-21 09:38:16+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 18392,
      "title": "[Performance]: Low GPU Utilization (70%) for ViT+Qwen2 VLM Model.",
      "state": "open",
      "comments": 11,
      "body_length": 11347,
      "label_names": [
        "performance"
      ],
      "created_at": "2025-05-20 07:21:19+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 18385,
      "title": "[Bug]: Audio transcription does not support webm",
      "state": "open",
      "comments": 11,
      "body_length": 8460,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-05-20 05:43:10+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 18378,
      "title": "[Bug]: Kimi_VL will generate confusing/repetitive character answers when n > 1",
      "state": "open",
      "comments": 11,
      "body_length": 18444,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-05-20 03:50:17+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 18324,
      "title": "[Bug]: Clarification regarding bug inside vllm-flash-attn vision module",
      "state": "open",
      "comments": 11,
      "body_length": 1418,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-05-18 15:42:38+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 18176,
      "title": "[RFC]: Returning Last Token, Last Layer Hidden States Through The OpenAI API",
      "state": "open",
      "comments": 19,
      "body_length": 2587,
      "label_names": [
        "RFC"
      ],
      "created_at": "2025-05-15 01:01:45+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 18141,
      "title": "[Bug]: When running phi-4-reasoning-plus with vLLM, the model gets stuck repeating reasoning phrases",
      "state": "open",
      "comments": 18,
      "body_length": 767,
      "label_names": [
        "bug",
        "usage"
      ],
      "created_at": "2025-05-14 13:02:04+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 18136,
      "title": "[Performance]: SGLANG is 4 times faster than vLLM for Qwen/Qwen3-32B-AWQ",
      "state": "open",
      "comments": 17,
      "body_length": 9775,
      "label_names": [
        "performance"
      ],
      "created_at": "2025-05-14 10:28:13+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 18135,
      "title": "[Performance]: lmcache cannot work\uff01",
      "state": "open",
      "comments": 19,
      "body_length": 45905,
      "label_names": [
        "usage"
      ],
      "created_at": "2025-05-14 09:58:40+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 18108,
      "title": "[Usage]: Deploy Qwen3-32B in vLLM 0.8.5. In the function_call mode, the results of streaming calls and non-streaming calls are inconsistent.",
      "state": "open",
      "comments": 11,
      "body_length": 4671,
      "label_names": [
        "usage"
      ],
      "created_at": "2025-05-14 01:44:33+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 18041,
      "title": "[New Model]:  support Qwen3-235B-A22B-GPTQ-Int4",
      "state": "open",
      "comments": 19,
      "body_length": 320,
      "label_names": [],
      "created_at": "2025-05-13 02:05:12+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 17972,
      "title": "[Bug]: vLLM server hangs and timeouts after initial requests",
      "state": "open",
      "comments": 12,
      "body_length": 57984,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-05-12 01:41:55+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 17887,
      "title": "[Feature]: Add Support for thinking_budget for Qwen3 Models",
      "state": "open",
      "comments": 12,
      "body_length": 761,
      "label_names": [
        "feature request"
      ],
      "created_at": "2025-05-09 07:04:03+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 17854,
      "title": "[Usage]: Will vllm by default use all my gpus?",
      "state": "open",
      "comments": 18,
      "body_length": 1783,
      "label_names": [
        "usage"
      ],
      "created_at": "2025-05-08 11:31:17+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 17835,
      "title": "[Bug]: torch.OutOfMemoryError: CUDA out of memory",
      "state": "open",
      "comments": 14,
      "body_length": 14789,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-05-08 07:03:59+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 17821,
      "title": "Tool calls not triggered properly with vLLM 0.8.5 and Qwen2.5-Coder-32B-Instruct-GPTQ-Int4",
      "state": "open",
      "comments": 23,
      "body_length": 10934,
      "label_names": [
        "bug",
        "tool-calling"
      ],
      "created_at": "2025-05-08 00:29:00+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 17813,
      "title": "[Bug]: 0.8.5 post1 cuda error",
      "state": "open",
      "comments": 15,
      "body_length": 37983,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-05-07 17:35:46+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 17741,
      "title": "[Bug]: Huge performance drop from 1c2bc7e to 2c4f59a",
      "state": "open",
      "comments": 12,
      "body_length": 7222,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-05-06 20:01:04+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 17689,
      "title": "[Bug]: gemma3 shows degraded accuracy in vLLM v0.8.4",
      "state": "open",
      "comments": 26,
      "body_length": 7260,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-05-06 03:07:01+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 17661,
      "title": "[Bug]: Can't serve can we serve Q4_K_M-GGUF  Model",
      "state": "open",
      "comments": 19,
      "body_length": 731,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-05-05 15:03:44+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 17652,
      "title": "[Bug]: Degradation of Qwen/Qwen3-30B-A3B performance depending on batch size",
      "state": "open",
      "comments": 13,
      "body_length": 8127,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-05-05 12:53:49+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 17650,
      "title": "[Bug]: the throughput of qwen3moe is low for prompts above 2000 tokens",
      "state": "open",
      "comments": 13,
      "body_length": 10656,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-05-05 12:19:07+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 17618,
      "title": "[Bug]: Engine Core initialization failed. See root cause above",
      "state": "open",
      "comments": 36,
      "body_length": 2363,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-05-03 23:53:51+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 17611,
      "title": "[Bug]: Qwen2.5-vl-7B stuck after loading weight and use a lot of shared GPU memory",
      "state": "open",
      "comments": 12,
      "body_length": 9368,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-05-03 11:06:13+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 17604,
      "title": "[Bug]: `size_k must divisible by BLOCK_SIZE_K` error when using tensor parallelism with AWQ-quantized MoE models",
      "state": "open",
      "comments": 20,
      "body_length": 15073,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-05-03 00:46:47+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 17581,
      "title": "[Bug]: Qwen3 FP8 on 0.8.5: type fp8e4nv not supported in this architecture.",
      "state": "open",
      "comments": 13,
      "body_length": 57871,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-05-02 10:33:25+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 17569,
      "title": "[Bug]: ValueError: The output_size of gate's and up's weight = 192 is not divisible by weight quantization block_n = 128.",
      "state": "open",
      "comments": 17,
      "body_length": 32201,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-05-02 00:17:38+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 17462,
      "title": "[Bug]: The cuda graph load process seems have memory leak in 0.8.4/0.85 V1 engine",
      "state": "open",
      "comments": 14,
      "body_length": 28464,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-30 09:06:45+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 17419,
      "title": "[RFC]: Kernel Library Restructure / Packaging Split (addressing long build times)",
      "state": "open",
      "comments": 29,
      "body_length": 5754,
      "label_names": [
        "RFC"
      ],
      "created_at": "2025-04-29 22:40:31+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 17392,
      "title": "[Bug]: Failed to run model Qwen3-30B-A3B on DGX V100x4",
      "state": "open",
      "comments": 20,
      "body_length": 24006,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-29 13:40:53+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 17349,
      "title": "[Bug]: Qwen3's answer was wrongly placed in `reasoning_content`",
      "state": "open",
      "comments": 11,
      "body_length": 1416,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-29 03:57:54+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 17346,
      "title": "[Bug]: [v0.8.5] Qwen3 returned reasoning content, but --enable-reasoning was not enabled.",
      "state": "open",
      "comments": 12,
      "body_length": 10673,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-29 03:21:27+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 17327,
      "title": "[Usage] Qwen3 Usage Guide",
      "state": "open",
      "comments": 88,
      "body_length": 1387,
      "label_names": [
        "usage"
      ],
      "created_at": "2025-04-28 22:05:06+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 17234,
      "title": "[New Model]: moonshotai/Kimi-Audio-7B-Instruct",
      "state": "open",
      "comments": 15,
      "body_length": 514,
      "label_names": [
        "new-model"
      ],
      "created_at": "2025-04-26 22:37:48+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 17171,
      "title": "[Bug]: Qwen2VL-2b / Qwen2.5-7b has AssertionError and Cuda error when qps goes higher",
      "state": "open",
      "comments": 19,
      "body_length": 22288,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-25 08:23:07+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 16993,
      "title": "[Bug]: vLLM sleep experiences segmentation fault when used in TRL",
      "state": "open",
      "comments": 15,
      "body_length": 12915,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-22 15:50:59+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 16908,
      "title": "[Bug]: Kimi-VL-A3B-Thinking Error",
      "state": "open",
      "comments": 12,
      "body_length": 3571,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-21 07:40:07+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 16856,
      "title": "[Feature]: Support Gemma 3 QAT series",
      "state": "open",
      "comments": 13,
      "body_length": 543,
      "label_names": [
        "feature request"
      ],
      "created_at": "2025-04-18 16:35:24+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 16763,
      "title": "[Bug]: qwen2.5-vl inference truncated",
      "state": "open",
      "comments": 13,
      "body_length": 7652,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2025-04-17 07:18:42+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 16757,
      "title": "[Usage]:  Wrong context length for Qwen2.5-7B-Instruct?",
      "state": "open",
      "comments": 11,
      "body_length": 1392,
      "label_names": [
        "usage"
      ],
      "created_at": "2025-04-17 05:01:12+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 16718,
      "title": "[Bug]: Vllm serve\u2018s results is not equal to offline inference.",
      "state": "open",
      "comments": 38,
      "body_length": 4285,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-16 11:38:51+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 16692,
      "title": "[Bug]: Problems with vllm serve DeepSeek-R1 with 2 nodes and TP = 16\uff08include vllm v0.8.4 v0.7.3 v0.7.2 V0 V1 engine\uff09",
      "state": "open",
      "comments": 11,
      "body_length": 49412,
      "label_names": [
        "bug",
        "ray"
      ],
      "created_at": "2025-04-16 02:43:26+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 16663,
      "title": "[Bug]: cpu memory not released when wake up the vLLM instance",
      "state": "open",
      "comments": 15,
      "body_length": 9761,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-15 13:21:53+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 16660,
      "title": "[Bug]: 100% CPU usage when idle",
      "state": "open",
      "comments": 13,
      "body_length": 4954,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-15 11:59:11+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 16632,
      "title": "[Usage]: [swift with vllm and only using vllm serve]  leads to different result\uff0810% diff\uff09",
      "state": "open",
      "comments": 11,
      "body_length": 8943,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2025-04-15 03:59:34+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 16626,
      "title": "[Bug]: Multi-modal inference too slow",
      "state": "open",
      "comments": 25,
      "body_length": 10155,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-15 02:58:23+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 16521,
      "title": "[Bug]: Add TPU support for gemma-3-4b-it and gemma-3-27b-it",
      "state": "open",
      "comments": 15,
      "body_length": 27325,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-12 01:08:58+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 16429,
      "title": "[Bug]: Cannot load  Qwen2.5-VL",
      "state": "open",
      "comments": 26,
      "body_length": 29747,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-10 20:30:01+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 16406,
      "title": "[New Model]: Multimodal Embedding Model GME.",
      "state": "open",
      "comments": 46,
      "body_length": 551,
      "label_names": [],
      "created_at": "2025-04-10 10:54:05+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 16380,
      "title": "[Bug]: vllm\u90e8\u7f72Gemma-3-27b\u95ee\u9898",
      "state": "open",
      "comments": 21,
      "body_length": 47237,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-04-10 02:35:43+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 16348,
      "title": "[Bug]: Missing metrics  in V1",
      "state": "open",
      "comments": 13,
      "body_length": 10708,
      "label_names": [
        "bug",
        "v1"
      ],
      "created_at": "2025-04-09 14:38:15+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 16342,
      "title": "[Performance]:",
      "state": "open",
      "comments": 15,
      "body_length": 1736,
      "label_names": [
        "performance"
      ],
      "created_at": "2025-04-09 12:38:47+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 16294,
      "title": "[Feature]: Integrate Triton MoE Kernel",
      "state": "open",
      "comments": 24,
      "body_length": 646,
      "label_names": [
        "good first issue",
        "feature request"
      ],
      "created_at": "2025-04-08 23:50:43+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 16259,
      "title": "[Bug]: vLLM still runs after Ray workers crash",
      "state": "open",
      "comments": 12,
      "body_length": 55089,
      "label_names": [
        "bug",
        "ray"
      ],
      "created_at": "2025-04-08 11:23:38+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 16214,
      "title": "[Usage]: Llama4 tool parser",
      "state": "open",
      "comments": 17,
      "body_length": 492,
      "label_names": [
        "usage"
      ],
      "created_at": "2025-04-07 21:36:28+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 16166,
      "title": "[Feature]: torch compile for llama4 unsloth",
      "state": "open",
      "comments": 12,
      "body_length": 578,
      "label_names": [
        "feature request",
        "stale"
      ],
      "created_at": "2025-04-07 07:33:57+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 16152,
      "title": "[Bug]: Stuck When Launching Llama-4-Maverick-17B-128E-Instruct-FP8",
      "state": "open",
      "comments": 15,
      "body_length": 12261,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2025-04-07 03:44:58+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 16146,
      "title": "[Bug]: ValueError: BertForMaskedLM has no vLLM implementation and the Transformers implementation is not compatible with vLLM.",
      "state": "open",
      "comments": 14,
      "body_length": 15362,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2025-04-07 02:37:00+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 16144,
      "title": "[RFC]: Offload KV cache to CPU in V1",
      "state": "open",
      "comments": 16,
      "body_length": 4604,
      "label_names": [
        "RFC"
      ],
      "created_at": "2025-04-07 02:17:20+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 16109,
      "title": "[Installation]: how to run swiftkv with vllm",
      "state": "open",
      "comments": 12,
      "body_length": 723,
      "label_names": [
        "installation"
      ],
      "created_at": "2025-04-05 23:44:51+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 16068,
      "title": "[Bug]: v0.8.1 V1 with pipeline-parallel-size 4, weird responses",
      "state": "open",
      "comments": 11,
      "body_length": 6431,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2025-04-04 16:17:25+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 15901,
      "title": "[SpecDecode] Support EAGLE in V1",
      "state": "open",
      "comments": 21,
      "body_length": 985,
      "label_names": [
        "speculative-decoding",
        "v1"
      ],
      "created_at": "2025-04-01 19:45:13+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 15871,
      "title": "[Bug]: CPU offload not working for DeepSeek-V2-Lite-Chat",
      "state": "open",
      "comments": 11,
      "body_length": 25432,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2025-04-01 09:35:22+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 15836,
      "title": "[Bug]: Gemma-3 (27B) can't load save_pretrained() checkpoint: AssertionError: expected size 5376==2560, stride 1==1 at dim=0",
      "state": "open",
      "comments": 17,
      "body_length": 16296,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-03-31 21:43:29+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 15814,
      "title": "[Bug]: Command-A model returns responses with unintended",
      "state": "open",
      "comments": 14,
      "body_length": 1147,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2025-03-31 11:59:43+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 15773,
      "title": "[Doc]: vllm==0.8.2 V1 does not yet support Pooling models.",
      "state": "open",
      "comments": 23,
      "body_length": 652,
      "label_names": [
        "documentation"
      ],
      "created_at": "2025-03-30 13:49:36+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 15764,
      "title": "[Bug]: qwen2-vl 7b, on vllm 0.8.1 & 0.8.2, sometimes (not deterministically but depends on data) I got: ValueError: Attempted to assign 702 = 702 multimodal tokens to 703 placeholders",
      "state": "open",
      "comments": 23,
      "body_length": 3889,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-03-30 08:14:47+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 15754,
      "title": "[Bug]: TypeError: Unknown image model type: qwen2_5_omni for branch: qwen2_omni_public_v1",
      "state": "open",
      "comments": 44,
      "body_length": 5105,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-03-29 14:14:45+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 15697,
      "title": "[Feature]: Composite model loading using `AutoWeightsLoader` for all models",
      "state": "open",
      "comments": 15,
      "body_length": 1401,
      "label_names": [
        "good first issue",
        "feature request"
      ],
      "created_at": "2025-03-28 10:39:19+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 15647,
      "title": "[Performance]: Update Cascade Attention Heuristics for FA3",
      "state": "open",
      "comments": 11,
      "body_length": 1257,
      "label_names": [
        "help wanted",
        "good first issue",
        "feature request"
      ],
      "created_at": "2025-03-27 22:02:43+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 15601,
      "title": "[Bug]: Qwen2-VL-2B quantization model has no improvement in reasoning speed compared to the original model",
      "state": "open",
      "comments": 11,
      "body_length": 618,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2025-03-27 06:43:06+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 15600,
      "title": "[V1] [Performance Benchmark] Benchmark the performance of Speculative Decoding",
      "state": "open",
      "comments": 15,
      "body_length": 292,
      "label_names": [
        "stale"
      ],
      "created_at": "2025-03-27 06:23:12+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 15569,
      "title": "[Bug]: Vllm 0.8.2 + Ray 2.44 (Ray serve deployment) fallbacks to V0 Engine",
      "state": "open",
      "comments": 13,
      "body_length": 7290,
      "label_names": [
        "bug",
        "ray"
      ],
      "created_at": "2025-03-26 19:29:21+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 15440,
      "title": "[Usage]: `Phi-4-multimodal-instruct` activate LoRA module but get mangled text output",
      "state": "open",
      "comments": 19,
      "body_length": 11454,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2025-03-25 05:38:48+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 15435,
      "title": "[Installation]: Fail to build vLLM from source on CUDA 12.6",
      "state": "open",
      "comments": 11,
      "body_length": 20067,
      "label_names": [
        "installation"
      ],
      "created_at": "2025-03-25 03:51:53+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 15393,
      "title": "[Bug]: Batch embedding inference is inconsistent with hf",
      "state": "open",
      "comments": 11,
      "body_length": 8900,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-03-24 12:09:09+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 15340,
      "title": "[Bug][V0][Trition MLA][GGUF]: Deepseek R1 GGUF starts producing gibberish towards the end of a longer generation",
      "state": "open",
      "comments": 12,
      "body_length": 64998,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2025-03-22 19:12:30+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 15332,
      "title": "[Bug]: RuntimeError: The size of tensor a (1059) must match the size of tensor b (376) at non-singleton dimension, DeepSeek R1 H20x16 pp2, v1 engine",
      "state": "open",
      "comments": 24,
      "body_length": 27409,
      "label_names": [
        "bug",
        "ray"
      ],
      "created_at": "2025-03-22 13:35:07+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 15228,
      "title": "[Bug]: Out of Memory error for Qwen2.5 in 0.8.0 and 0.8.1. Worked fine in the previous versions",
      "state": "open",
      "comments": 23,
      "body_length": 41387,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-03-20 15:16:17+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 15131,
      "title": "[Usage]: relationship between embedding size and vocab_size",
      "state": "open",
      "comments": 13,
      "body_length": 1508,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2025-03-19 14:02:44+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 15110,
      "title": "[Bug]: Internal Server Error when using Qwen2-VL-7B with vLLM Docker Container",
      "state": "open",
      "comments": 12,
      "body_length": 4950,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2025-03-19 08:27:37+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 15102,
      "title": "[Bug]: 0.8.0(V1) RayChannelTimeoutError when inferencing DeepSeekV3 on 16 H20 with large batch size",
      "state": "open",
      "comments": 36,
      "body_length": 15427,
      "label_names": [
        "bug",
        "ray"
      ],
      "created_at": "2025-03-19 07:13:52+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 15090,
      "title": "[Bug]: Capture CudaGraph with LoRA",
      "state": "open",
      "comments": 11,
      "body_length": 1104,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2025-03-19 05:37:08+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 15010,
      "title": "[Installation]: undefined symbol: _ZNK3c1011StorageImpl27throw_data_ptr_access_errorEv",
      "state": "open",
      "comments": 13,
      "body_length": 1768,
      "label_names": [
        "installation"
      ],
      "created_at": "2025-03-18 08:22:45+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 14764,
      "title": "[RFC]: Schema for checking input shapes for multi-modal models",
      "state": "open",
      "comments": 29,
      "body_length": 2654,
      "label_names": [
        "good first issue",
        "feature request",
        "RFC",
        "multi-modality"
      ],
      "created_at": "2025-03-13 14:57:56+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 14753,
      "title": "[Feature]: Support Gemma3 GGUF",
      "state": "open",
      "comments": 19,
      "body_length": 3260,
      "label_names": [
        "feature request"
      ],
      "created_at": "2025-03-13 10:40:49+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 14747,
      "title": "[Installation]: Cannot compile vLLM from source on XPU",
      "state": "open",
      "comments": 11,
      "body_length": 20906,
      "label_names": [
        "installation",
        "stale"
      ],
      "created_at": "2025-03-13 09:47:58+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 14745,
      "title": "[Feature]: Support tool calls for DeepSeek.",
      "state": "open",
      "comments": 43,
      "body_length": 846,
      "label_names": [
        "feature request",
        "tool-calling"
      ],
      "created_at": "2025-03-13 09:31:41+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 14729,
      "title": "[V1] [Performance] Optimize Cascade Kernel",
      "state": "open",
      "comments": 11,
      "body_length": 685,
      "label_names": [
        "feature request"
      ],
      "created_at": "2025-03-13 05:28:44+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 14723,
      "title": "[Feature]: gemma3 raise error",
      "state": "open",
      "comments": 23,
      "body_length": 822,
      "label_names": [
        "feature request"
      ],
      "created_at": "2025-03-13 03:34:34+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 14721,
      "title": "[Feature]: Support openai responses API interface",
      "state": "open",
      "comments": 23,
      "body_length": 886,
      "label_names": [
        "good first issue",
        "feature request"
      ],
      "created_at": "2025-03-13 02:55:40+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 14452,
      "title": "[Doc]: Steps to run vLLM on your RTX5080 or 5090!",
      "state": "open",
      "comments": 119,
      "body_length": 3303,
      "label_names": [
        "documentation"
      ],
      "created_at": "2025-03-07 18:12:24+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 14449,
      "title": "[Bug]: `vllm serve Qwen/QwQ-32B-AWQ  --tensor-parallel-size 2` hangs with both RTX A6000 GPUs at max utilization",
      "state": "open",
      "comments": 12,
      "body_length": 6807,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2025-03-07 17:20:36+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 14435,
      "title": "[Usage]: VLLM Inference - 2x slower with LoRA rank=256 vs none.",
      "state": "open",
      "comments": 14,
      "body_length": 5044,
      "label_names": [
        "usage",
        "stale"
      ],
      "created_at": "2025-03-07 11:58:26+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 14429,
      "title": "[Feature]: support tool and reasoning together",
      "state": "open",
      "comments": 17,
      "body_length": 1462,
      "label_names": [
        "feature request"
      ],
      "created_at": "2025-03-07 10:19:33+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 14426,
      "title": "[Usage]: LLM.beam_search is much slower in vLLM 0.7.3 compared to 0.5.4",
      "state": "open",
      "comments": 13,
      "body_length": 9884,
      "label_names": [
        "usage"
      ],
      "created_at": "2025-03-07 09:50:36+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 14286,
      "title": "[Bug][V1]: Loading Llama3.1-8B-INT8 gets OOM when using VLLM_USE_v1=1 but safe using v0",
      "state": "open",
      "comments": 12,
      "body_length": 64574,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2025-03-05 12:50:10+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 13981,
      "title": "[RFC]: Drop support for prompt adapter",
      "state": "open",
      "comments": 11,
      "body_length": 569,
      "label_names": [
        "RFC",
        "keep-open"
      ],
      "created_at": "2025-02-27 17:58:49+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 13941,
      "title": "[Bug]: wake up OOM (72B model in 8*A800(40G))",
      "state": "open",
      "comments": 14,
      "body_length": 11030,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-02-27 03:37:20+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 13663,
      "title": "[New Model]: Google SigLip 2",
      "state": "open",
      "comments": 12,
      "body_length": 796,
      "label_names": [
        "help wanted",
        "good first issue",
        "new-model"
      ],
      "created_at": "2025-02-21 09:40:40+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 13651,
      "title": "[Bug]: \u542f\u52a8qwen2.5-vl\u7cfb\u5217\u7684\u65f6\u5019\u4e3a\u5565\u8001\u662f\u5361\u7740",
      "state": "open",
      "comments": 28,
      "body_length": 464,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-02-21 06:29:52+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 13563,
      "title": "[Bug]: structured output with xgrammar using vllm serve with llama-8b fails results in os error OSError: OSError: (...)/.cache/torch_extensions/py312_cu124/xgrammar/xgrammar.so: cannot open shared object file: No such file or directory",
      "state": "open",
      "comments": 14,
      "body_length": 7810,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2025-02-19 18:13:57+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 13521,
      "title": "[Bug]: Can't serve on ray cluster although passing VLLM_HOST_IP",
      "state": "open",
      "comments": 26,
      "body_length": 2305,
      "label_names": [
        "bug",
        "ray"
      ],
      "created_at": "2025-02-19 07:52:16+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 13471,
      "title": "[Bug]: deepseek r1 + vllm (v0.7.2) torch.compile error",
      "state": "open",
      "comments": 11,
      "body_length": 9956,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2025-02-18 08:57:03+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 13446,
      "title": "[Usage]: ValueError: The checkpoint you are trying to load has model type `qwen2_5_vl` but Transformers does not recognize this architecture",
      "state": "open",
      "comments": 20,
      "body_length": 1697,
      "label_names": [
        "usage",
        "unstale"
      ],
      "created_at": "2025-02-18 02:39:11+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 13216,
      "title": "ValueError: Model architectures ['Qwen2ForCausalLM'] failed to be inspected. Please check the logs for more details.",
      "state": "open",
      "comments": 24,
      "body_length": 615,
      "label_names": [
        "usage"
      ],
      "created_at": "2025-02-13 09:42:39+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 13175,
      "title": "[Bug]: Qwen/Qwen2.5-1.5B-Instruct generates out of vocabulary tokens",
      "state": "open",
      "comments": 28,
      "body_length": 2473,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-02-12 20:26:27+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 13136,
      "title": "[Bug]: deepseek-r1 mutlti-node crash",
      "state": "open",
      "comments": 18,
      "body_length": 11715,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2025-02-12 07:49:45+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 13052,
      "title": "[Bug]: see connection to gpu node timeout issue when initializing ray vllm multi-node serving",
      "state": "open",
      "comments": 16,
      "body_length": 38021,
      "label_names": [
        "bug",
        "ray",
        "unstale"
      ],
      "created_at": "2025-02-10 19:38:11+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 13047,
      "title": "[Bug]: `undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE` when running `0.7.3.dev57+g2ae88905.precompiled` on A100",
      "state": "open",
      "comments": 12,
      "body_length": 23061,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-02-10 17:16:08+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 13035,
      "title": "[Bug]: Llama-3.1-405B-Instruct-FP8 only generates exclamation marks",
      "state": "open",
      "comments": 19,
      "body_length": 5600,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-02-10 12:43:09+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 12961,
      "title": "[Bug]:There is no module or parameter named 'base_model' in Qwen2ForCausalLM",
      "state": "open",
      "comments": 15,
      "body_length": 38634,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-02-08 15:08:56+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 12791,
      "title": "[Bug]: Reward model usage",
      "state": "open",
      "comments": 12,
      "body_length": 1123,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-02-05 22:20:25+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 12699,
      "title": "[Bug]: different logprobs for qwn2-vl when running on transformers and on vllm",
      "state": "open",
      "comments": 29,
      "body_length": 9020,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2025-02-03 16:18:25+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 12568,
      "title": "[V1] Feedback Thread",
      "state": "open",
      "comments": 92,
      "body_length": 300,
      "label_names": [
        "v1"
      ],
      "created_at": "2025-01-30 02:46:45+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 12511,
      "title": "[Feature]: Support torch.distributed as the runtime for multi-node inference",
      "state": "open",
      "comments": 21,
      "body_length": 725,
      "label_names": [
        "feature request",
        "stale"
      ],
      "created_at": "2025-01-28 14:34:51+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 12254,
      "title": "[RFC]: Sparse KV cache management framework",
      "state": "open",
      "comments": 18,
      "body_length": 3178,
      "label_names": [
        "RFC",
        "unstale"
      ],
      "created_at": "2025-01-21 08:46:39+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 12249,
      "title": "[RFC]: Hidden states processor",
      "state": "open",
      "comments": 19,
      "body_length": 4739,
      "label_names": [
        "RFC",
        "unstale"
      ],
      "created_at": "2025-01-21 07:09:32+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 12225,
      "title": "[Usage]: Guided choice not working as expected",
      "state": "open",
      "comments": 11,
      "body_length": 1723,
      "label_names": [
        "usage"
      ],
      "created_at": "2025-01-20 16:02:03+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 12080,
      "title": "[RFC]: BatchLLM for better shared prefix utilizing in offline scenarios",
      "state": "open",
      "comments": 13,
      "body_length": 12666,
      "label_names": [
        "RFC",
        "stale"
      ],
      "created_at": "2025-01-15 11:40:31+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 12052,
      "title": "[Bug]: PaliGemma2 not working with OpenAI Docker serve",
      "state": "open",
      "comments": 19,
      "body_length": 1301,
      "label_names": [
        "bug",
        "unstale"
      ],
      "created_at": "2025-01-14 19:22:48+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 12032,
      "title": "[Bug]: Profiling on vLLM server hangs when --num-scheduler-steps > 1 ",
      "state": "open",
      "comments": 11,
      "body_length": 18803,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2025-01-14 10:38:48+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 11905,
      "title": "[Feature]: Support Multiple Tasks Per Model",
      "state": "open",
      "comments": 15,
      "body_length": 814,
      "label_names": [
        "feature request"
      ],
      "created_at": "2025-01-09 18:22:13+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 11873,
      "title": "[Bug]: Engine is gracefully shutting down",
      "state": "open",
      "comments": 11,
      "body_length": 26430,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2025-01-09 02:34:09+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 11786,
      "title": "[Bug]: Multiple tool calls for llama3.2-11b-vision-instruct",
      "state": "open",
      "comments": 12,
      "body_length": 4030,
      "label_names": [
        "bug",
        "tool-calling"
      ],
      "created_at": "2025-01-07 04:20:45+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 11671,
      "title": "[Bug]: The output size is not aligned with the quantized weight shape. This can be caused by too large tensor parallel size.",
      "state": "open",
      "comments": 15,
      "body_length": 8997,
      "label_names": [
        "bug"
      ],
      "created_at": "2025-01-01 15:12:23+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 11655,
      "title": "[Feature]: Support Inflight quantization: load as 8bit quantization.",
      "state": "open",
      "comments": 20,
      "body_length": 618,
      "label_names": [
        "feature request",
        "unstale"
      ],
      "created_at": "2024-12-31 08:42:16+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 11539,
      "title": "[Model] DeepSeek-V3 Enhancements ",
      "state": "open",
      "comments": 50,
      "body_length": 1025,
      "label_names": [
        "performance",
        "new-model",
        "unstale"
      ],
      "created_at": "2024-12-27 00:39:28+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 11522,
      "title": "[RFC]: Refactor tool parsers to eliminate coding errors and allow more efficient implementations.",
      "state": "open",
      "comments": 11,
      "body_length": 2548,
      "label_names": [
        "RFC",
        "tool-calling"
      ],
      "created_at": "2024-12-26 13:47:17+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 11490,
      "title": "[Usage]: how to use prefill-decode disaggregation ??",
      "state": "open",
      "comments": 13,
      "body_length": 1686,
      "label_names": [
        "usage"
      ],
      "created_at": "2024-12-25 10:19:08+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 11397,
      "title": "[Feature]: obtain logits",
      "state": "open",
      "comments": 28,
      "body_length": 433,
      "label_names": [
        "feature request",
        "unstale"
      ],
      "created_at": "2024-12-21 08:07:46+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 11345,
      "title": "[Performance]: 1P1D Disaggregation performance",
      "state": "open",
      "comments": 11,
      "body_length": 1154,
      "label_names": [
        "performance"
      ],
      "created_at": "2024-12-19 18:37:57+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 11286,
      "title": "[Performance]: decoding speed on long context",
      "state": "open",
      "comments": 53,
      "body_length": 1785,
      "label_names": [
        "performance",
        "stale"
      ],
      "created_at": "2024-12-18 06:59:09+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 11255,
      "title": "[Feature]: LoRA support for qwen2-vl Models",
      "state": "open",
      "comments": 16,
      "body_length": 1340,
      "label_names": [
        "feature request"
      ],
      "created_at": "2024-12-17 06:41:58+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 11175,
      "title": "[Bug]: Nonsensical Sentences Generated When Inferencing INT8 Quantized Qwen2.5-72B Model",
      "state": "open",
      "comments": 15,
      "body_length": 41313,
      "label_names": [
        "bug",
        "unstale"
      ],
      "created_at": "2024-12-13 14:42:50+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 11173,
      "title": "[Installation]: XPU dependencies are missing",
      "state": "open",
      "comments": 11,
      "body_length": 36792,
      "label_names": [
        "installation",
        "unstale"
      ],
      "created_at": "2024-12-13 12:03:16+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 11037,
      "title": "[Installation]: no version of pip install vllm works - Failed to initialize NumPy: No Module named 'numpy'",
      "state": "open",
      "comments": 18,
      "body_length": 3434,
      "label_names": [
        "installation"
      ],
      "created_at": "2024-12-09 22:11:26+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 10971,
      "title": "[Bug]: Vllm CPU mode only takes 1 single core for multi-core cpu",
      "state": "open",
      "comments": 20,
      "body_length": 6101,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-12-06 23:48:15+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 10913,
      "title": "[Bug]: Speculative decoding inconsistency for Qwen-Coder-32B",
      "state": "open",
      "comments": 23,
      "body_length": 2243,
      "label_names": [
        "bug",
        "unstale"
      ],
      "created_at": "2024-12-05 03:49:40+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 10818,
      "title": "[RFC]: Disaggregated prefilling and KV cache transfer roadmap",
      "state": "open",
      "comments": 22,
      "body_length": 3589,
      "label_names": [
        "RFC",
        "stale"
      ],
      "created_at": "2024-12-02 03:01:11+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 10774,
      "title": "[Feature]: Unblock LLM while handling long sequences / Handling multiple prefills at the same time",
      "state": "open",
      "comments": 11,
      "body_length": 2834,
      "label_names": [
        "feature request",
        "unstale"
      ],
      "created_at": "2024-11-29 10:51:54+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 10710,
      "title": "[Bug]: Unsloth bitsandbytes quantized model cannot be run due to: `KeyError: 'layers.42.mlp.down_proj.weight.absmax`",
      "state": "open",
      "comments": 15,
      "body_length": 18456,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-11-27 13:12:17+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 10478,
      "title": "[Bug]: vLLM CPU mode broken Unable to get JIT kernel for brgemm",
      "state": "open",
      "comments": 14,
      "body_length": 35221,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-11-20 06:39:21+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 10459,
      "title": "[Installation]: VLLM on ARM machine with GH200",
      "state": "open",
      "comments": 28,
      "body_length": 3774,
      "label_names": [
        "installation"
      ],
      "created_at": "2024-11-19 16:57:34+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 10300,
      "title": "[Bug]: undefined symbol: __nvJitLinkComplete_12_4, version libnvJitLink.so.12",
      "state": "open",
      "comments": 23,
      "body_length": 1563,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-11-13 16:27:59+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 10293,
      "title": "[Bug]: Can't use yarn rope config for long context in Qwen2 model",
      "state": "open",
      "comments": 31,
      "body_length": 11135,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-11-13 11:01:37+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 10267,
      "title": "[Feature]: Support for NVIDIA Unified memory ",
      "state": "open",
      "comments": 17,
      "body_length": 435,
      "label_names": [
        "feature request"
      ],
      "created_at": "2024-11-12 20:27:17+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 10081,
      "title": "[Bug]:Structured outputs inference often took a very long time,and eventually causing a timeout and vLLM engine crushing.",
      "state": "open",
      "comments": 12,
      "body_length": 7042,
      "label_names": [
        "bug",
        "structured-output",
        "unstale"
      ],
      "created_at": "2024-11-06 14:46:54+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 9807,
      "title": "[Feature]: Integrate Writing in the Margins inference pattern ($5,000 Bounty)",
      "state": "open",
      "comments": 21,
      "body_length": 2463,
      "label_names": [
        "feature request",
        "stale"
      ],
      "created_at": "2024-10-29 17:47:14+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 9565,
      "title": "[Performance]: vllm Eagle performance is worse than expected",
      "state": "open",
      "comments": 22,
      "body_length": 1848,
      "label_names": [
        "performance"
      ],
      "created_at": "2024-10-21 22:16:51+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 9496,
      "title": "[Bug]: [Performance] 100% performance drop using multiple lora vs no lora(qwen-chat model)",
      "state": "open",
      "comments": 14,
      "body_length": 804,
      "label_names": [
        "bug",
        "unstale"
      ],
      "created_at": "2024-10-18 08:14:35+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 9469,
      "title": "[Bug]: I want to integrate vllm into LLaMA-Factory, a transformers-based LLM training framework. However, I encountered two bugs: RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method & RuntimeError: NCCL error: invalid usage (run with NCCL_DEBUG=WARN for details)",
      "state": "open",
      "comments": 11,
      "body_length": 42637,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-10-17 15:17:07+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 9451,
      "title": "[Feature]: Consider parallel_tool_calls parameter at the API level",
      "state": "open",
      "comments": 19,
      "body_length": 5452,
      "label_names": [
        "feature request",
        "tool-calling"
      ],
      "created_at": "2024-10-17 07:41:26+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 9423,
      "title": "[Bug]: Speculative decoding breaks guided decoding.",
      "state": "open",
      "comments": 13,
      "body_length": 6415,
      "label_names": [
        "bug",
        "structured-output"
      ],
      "created_at": "2024-10-16 13:51:18+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 9421,
      "title": "[Bug]: When using the latest 0.6.3, No module named 'vllm._version' appears",
      "state": "open",
      "comments": 16,
      "body_length": 801,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-10-16 13:21:41+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 9306,
      "title": "[Bug]: Failed to pickle inputs of failed execution: CUDA error: an illegal memory access was encountered",
      "state": "open",
      "comments": 11,
      "body_length": 35636,
      "label_names": [
        "bug",
        "unstale"
      ],
      "created_at": "2024-10-12 04:38:17+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 9206,
      "title": "[Feature]: Simple Data Parallelism in vLLM",
      "state": "open",
      "comments": 17,
      "body_length": 1564,
      "label_names": [
        "feature request",
        "unstale"
      ],
      "created_at": "2024-10-09 22:04:57+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 9190,
      "title": "[Performance]: phi 3.5 vision model consuming high CPU RAM and the process getting killed",
      "state": "open",
      "comments": 37,
      "body_length": 1488,
      "label_names": [
        "performance",
        "unstale"
      ],
      "created_at": "2024-10-09 12:19:09+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 9176,
      "title": "[Bug]: Extreme low throughput when using pipeline parallelism when Batch Size(running req) is small",
      "state": "open",
      "comments": 11,
      "body_length": 11475,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-10-09 02:46:05+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 9006,
      "title": "[Roadmap] vLLM Roadmap Q4 2024",
      "state": "open",
      "comments": 29,
      "body_length": 3046,
      "label_names": [
        "stale"
      ],
      "created_at": "2024-10-01 17:39:50+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 8933,
      "title": "[Bug]: Vllm0.6.2 UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown",
      "state": "open",
      "comments": 25,
      "body_length": 1906,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-09-29 02:04:54+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 8893,
      "title": "[Bug]: RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method",
      "state": "open",
      "comments": 31,
      "body_length": 167,
      "label_names": [
        "usage",
        "unstale"
      ],
      "created_at": "2024-09-27 06:47:56+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 8881,
      "title": "[Bug]: assert len(self._async_stopped) == 0",
      "state": "open",
      "comments": 11,
      "body_length": 24178,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-09-27 03:09:24+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 8826,
      "title": "Llama3.2 Vision Model: Guides and Issues",
      "state": "open",
      "comments": 54,
      "body_length": 856,
      "label_names": [
        "stale"
      ],
      "created_at": "2024-09-25 22:50:46+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 8803,
      "title": "[Misc]: Strange `leaked shared_memory` warnings reported by multiprocessing when using vLLM",
      "state": "open",
      "comments": 23,
      "body_length": 2130,
      "label_names": [
        "misc"
      ],
      "created_at": "2024-09-25 12:29:58+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 8793,
      "title": "[Doc]: Is Qwen2.5's long context YARN handled?",
      "state": "open",
      "comments": 19,
      "body_length": 1745,
      "label_names": [
        "documentation",
        "unstale"
      ],
      "created_at": "2024-09-25 06:06:53+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 8779,
      "title": "vLLM's V1 Engine Architecture",
      "state": "open",
      "comments": 14,
      "body_length": 5117,
      "label_names": [
        "RFC",
        "keep-open"
      ],
      "created_at": "2024-09-24 18:25:22+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 8629,
      "title": "[Bug]: memory leak",
      "state": "open",
      "comments": 15,
      "body_length": 8547,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-09-19 09:59:58+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 8581,
      "title": "[Feature]: DRY Sampling",
      "state": "open",
      "comments": 21,
      "body_length": 1010,
      "label_names": [
        "feature request",
        "unstale"
      ],
      "created_at": "2024-09-18 23:05:42+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 8439,
      "title": "[Usage]:  why speculate decoding is slower than normal decoding\uff1f",
      "state": "open",
      "comments": 14,
      "body_length": 1347,
      "label_names": [
        "usage",
        "unstale"
      ],
      "created_at": "2024-09-13 03:43:26+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 8313,
      "title": "[Performance]: guided generation is very slow in offline mode",
      "state": "open",
      "comments": 21,
      "body_length": 1306,
      "label_names": [
        "performance",
        "structured-output",
        "unstale"
      ],
      "created_at": "2024-09-10 02:21:33+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 8177,
      "title": "[Bug]: watchdog thread terminated with exception: CUDA error: an illegal memory access was encountered",
      "state": "open",
      "comments": 16,
      "body_length": 23498,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-09-05 05:27:29+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 8087,
      "title": "[Bug]: when tensor-parallel-size>1\uff0cStuck",
      "state": "open",
      "comments": 15,
      "body_length": 11545,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-09-02 13:29:03+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 8024,
      "title": "[Bug]: deploy on V100, mma -> mma layout conversion is only supported on Ampere",
      "state": "open",
      "comments": 12,
      "body_length": 1720,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-08-30 06:52:13+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 7977,
      "title": "[Bug]: When enabling LoRA, greedy search got different answers.",
      "state": "open",
      "comments": 24,
      "body_length": 9257,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-08-29 01:00:12+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 7939,
      "title": "[Bug]: Is vllm compatible with torchrun?",
      "state": "open",
      "comments": 14,
      "body_length": 9897,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-08-28 06:19:47+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 7519,
      "title": "[Feature]: Context Parallelism",
      "state": "open",
      "comments": 12,
      "body_length": 4107,
      "label_names": [
        "feature request",
        "unstale"
      ],
      "created_at": "2024-08-14 14:56:56+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 7430,
      "title": "[Bug]: Does not work on MacOS",
      "state": "open",
      "comments": 11,
      "body_length": 4717,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-08-12 17:46:48+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 7382,
      "title": "[Bug]: LLaMa 3.1 8B/70B/405B all behave poorly and differently using completions API as compared to good chat API",
      "state": "open",
      "comments": 22,
      "body_length": 7508,
      "label_names": [
        "bug",
        "unstale"
      ],
      "created_at": "2024-08-10 01:47:36+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 7366,
      "title": "[RFC]: Encoder/decoder models & feature compatibility",
      "state": "open",
      "comments": 17,
      "body_length": 28756,
      "label_names": [
        "RFC",
        "unstale"
      ],
      "created_at": "2024-08-09 15:03:54+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 6918,
      "title": "[Bug]:  error: Segmentation fault(SIGSEGV received at time)",
      "state": "open",
      "comments": 13,
      "body_length": 11057,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-07-29 21:52:17+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 6801,
      "title": "[RFC]: Performance Roadmap ",
      "state": "open",
      "comments": 37,
      "body_length": 379,
      "label_names": [
        "RFC",
        "keep-open"
      ],
      "created_at": "2024-07-25 21:32:34+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 6614,
      "title": "[Bug]: No available block found in 60 second in shm",
      "state": "open",
      "comments": 29,
      "body_length": 6786,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-07-21 07:39:35+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 6165,
      "title": "[Feature]: Return hidden states (in progress?)",
      "state": "open",
      "comments": 19,
      "body_length": 1221,
      "label_names": [
        "feature request",
        "unstale"
      ],
      "created_at": "2024-07-06 01:26:10+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 5907,
      "title": "[Bug]: TRACKING ISSUE: CUDA OOM with Logprobs",
      "state": "open",
      "comments": 20,
      "body_length": 995,
      "label_names": [
        "bug",
        "unstale"
      ],
      "created_at": "2024-06-27 14:18:52+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 5901,
      "title": "[Bug]: TRACKING ISSUE: `AsyncEngineDeadError`",
      "state": "open",
      "comments": 23,
      "body_length": 1229,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-06-27 11:49:38+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 5898,
      "title": "[Bug]: Inconsistent Responses with VLLM When Batch Size > 1 even temperature = 0",
      "state": "open",
      "comments": 36,
      "body_length": 1223,
      "label_names": [
        "bug",
        "unstale"
      ],
      "created_at": "2024-06-27 09:34:57+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 5751,
      "title": "[RFC]: Support sparse KV cache framework",
      "state": "open",
      "comments": 17,
      "body_length": 4784,
      "label_names": [
        "RFC",
        "keep-open"
      ],
      "created_at": "2024-06-21 20:21:39+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 5547,
      "title": "[Bug]: RuntimeError: CUDA error: no kernel image is available for execution on the device",
      "state": "open",
      "comments": 22,
      "body_length": 4939,
      "label_names": [
        "bug",
        "unstale"
      ],
      "created_at": "2024-06-14 16:13:53+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 5543,
      "title": "[Bug]: prefix-caching: inconsistent completions",
      "state": "open",
      "comments": 22,
      "body_length": 3630,
      "label_names": [
        "bug",
        "keep-open"
      ],
      "created_at": "2024-06-14 14:18:02+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 5404,
      "title": "[Bug]:  topk=1 and temperature=0 cause different output in vllm",
      "state": "open",
      "comments": 27,
      "body_length": 792,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-06-11 03:00:51+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 5344,
      "title": "[Bug]: with `--enable-prefix-caching` , `/completions` crashes server with `echo=True` above certain prompt length",
      "state": "open",
      "comments": 16,
      "body_length": 8764,
      "label_names": [
        "bug",
        "unstale"
      ],
      "created_at": "2024-06-07 13:13:22+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 5060,
      "title": "[Bug]: vllm.engine.async_llm_engine.AsyncEngineDeadError: Background loop has errored already.",
      "state": "open",
      "comments": 45,
      "body_length": 10939,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-05-26 22:44:41+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 4873,
      "title": "[RFC]: Add control panel support for vLLM",
      "state": "open",
      "comments": 12,
      "body_length": 2565,
      "label_names": [
        "RFC",
        "keep-open"
      ],
      "created_at": "2024-05-17 02:20:50+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 4763,
      "title": "[Feature]: Support W4A8KV4 Quantization(QServe/QoQ)",
      "state": "open",
      "comments": 13,
      "body_length": 1169,
      "label_names": [
        "feature request",
        "unstale"
      ],
      "created_at": "2024-05-11 12:30:46+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 4392,
      "title": "[Bug]: Running llama2-7b on H20, Floating point exception (core dumped) appears on float16",
      "state": "open",
      "comments": 25,
      "body_length": 6333,
      "label_names": [
        "bug"
      ],
      "created_at": "2024-04-26 09:40:05+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 4241,
      "title": "[Bug]: Ray memory leak",
      "state": "open",
      "comments": 14,
      "body_length": 6330,
      "label_names": [
        "bug",
        "ray",
        "stale"
      ],
      "created_at": "2024-04-21 14:06:32+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 4194,
      "title": "[RFC]: Multi-modality Support on vLLM",
      "state": "open",
      "comments": 98,
      "body_length": 15303,
      "label_names": [
        "RFC",
        "multi-modality"
      ],
      "created_at": "2024-04-19 07:51:48+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 4186,
      "title": "[Bug]: lora base_model.model.lm_head.base_layer.weight is not supported ",
      "state": "open",
      "comments": 11,
      "body_length": 7406,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-04-19 02:36:50+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 4070,
      "title": "[Bug]: sending request using response_format json twice breaks vLLM",
      "state": "open",
      "comments": 16,
      "body_length": 7691,
      "label_names": [
        "bug",
        "structured-output",
        "unstale"
      ],
      "created_at": "2024-04-14 18:16:46+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 3998,
      "title": "[Bug]: Qwen1.5-14B-Chat\u4f7f\u7528vllm==0.3.3\u7248\u672c\u5728Tesla V100-PCIE-32GB\u663e\u5361\u4e0a\u90e8\u7f72\u7ed3\u679c\u5168\u90e8\u662f\u611f\u53f9\u53f7\uff0c\u65e0\u7ed3\u679c",
      "state": "open",
      "comments": 31,
      "body_length": 412,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-04-11 07:56:08+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 3900,
      "title": "[Bug]: AttributeError: 'MergedColumnParallelLinear' object has no attribute 'weight'",
      "state": "open",
      "comments": 14,
      "body_length": 9156,
      "label_names": [
        "bug",
        "stale"
      ],
      "created_at": "2024-04-07 16:20:39+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 3490,
      "title": "[Feature]: Compute and log the serving FLOPs",
      "state": "open",
      "comments": 12,
      "body_length": 230,
      "label_names": [
        "good first issue",
        "feature request"
      ],
      "created_at": "2024-03-19 07:29:03+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 3281,
      "title": "unload the model",
      "state": "open",
      "comments": 14,
      "body_length": 333,
      "label_names": [
        "unstale"
      ],
      "created_at": "2024-03-08 13:40:20+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 3203,
      "title": "ExLlamaV2: exl2 support",
      "state": "open",
      "comments": 38,
      "body_length": 220,
      "label_names": [
        "feature request",
        "stale"
      ],
      "created_at": "2024-03-05 14:54:03+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 3192,
      "title": "[Feature request] Output attention scores in vLLM",
      "state": "open",
      "comments": 12,
      "body_length": 90,
      "label_names": [
        "feature request",
        "stale"
      ],
      "created_at": "2024-03-05 08:13:46+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 3090,
      "title": "Loading models from an S3 location instead of local path",
      "state": "open",
      "comments": 15,
      "body_length": 831,
      "label_names": [
        "unstale"
      ],
      "created_at": "2024-02-28 18:20:13+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 2847,
      "title": "Please add lora support for higher ranks and alpha values",
      "state": "open",
      "comments": 16,
      "body_length": 58,
      "label_names": [
        "stale"
      ],
      "created_at": "2024-02-13 07:26:31+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 2248,
      "title": "Recent vLLMs ask for too much memory: ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.",
      "state": "open",
      "comments": 53,
      "body_length": 5784,
      "label_names": [
        "bug",
        "unstale"
      ],
      "created_at": "2023-12-24 02:47:42+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    },
    {
      "number": 1185,
      "title": "Generate nothing from VLLM output",
      "state": "open",
      "comments": 35,
      "body_length": 185,
      "label_names": [
        "bug"
      ],
      "created_at": "2023-09-26 18:31:25+00:00",
      "closed_at": "NaT",
      "resolution_days": "NaT"
    }
  ]
}