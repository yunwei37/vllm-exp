{
  "issue_type": "performance",
  "extracted_from": "/root/yunwei37/vllm-exp/bug-study/analysis_llm/sample_results/vllm/label_based/performance/issues.json",
  "issue": {
    "number": 15018,
    "title": "[Performance]: only 0.4 tokens/s when running 2 or more request",
    "body": "### Proposal to improve performance\n\n_No response_\n\n### Report of performance regression\n\n_No response_\n\n### Misc discussion on performance\n\nI was tring to run DeepSeek-R1-Distill-Qwen-32B-Q4_K_M.gguf with 7900 XTX\uff0824G) and it works,I have got 19.3 tokens/s when it run with one request.However\uff0cthe throughput was only 0.4 tokens/s   when it running two or more requsets.The GPU KV cache usage  is enought,is there any parameters i have to set?\nINFO 03-18 08:46:00 [__init__.py:256] Automatically detected platform rocm.\nINFO 03-18 08:46:01 [api_server.py:912] vLLM API server version 0.7.4.dev442+gfd8e055f\nINFO 03-18 08:46:01 [api_server.py:913] args: Namespace(subparser='serve', model_tag='/app/model/DeepSeek-R1-Distill-Qwen-32B-GGUF/DeepSeek-R1-Distill-Qwen-32B-Q4_K_M.gguf', config='', host='0.0.0.0', port=8199, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/app/model/DeepSeek-R1-Distill-Qwen-32B-GGUF/DeepSeek-R1-Distill-Qwen-32B-Q4_K_M.gguf', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=4096, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.96, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, use_tqdm_on_load=True, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['Deepseek-R1:32B'], qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function ServeSubcommand.cmd at 0x7a3251b5b600>)\nINFO 03-18 08:46:01 [api_server.py:209] Started engine process with PID 660\nINFO 03-18 08:46:02 [__init__.py:256] Automatically detected platform rocm.\nINFO 03-18 08:46:10 [config.py:2573] Downcasting torch.float32 to torch.float16.\nINFO 03-18 08:46:12 [config.py:2573] Downcasting torch.float32 to torch.float16.\nINFO 03-18 08:46:16 [config.py:581] This model supports multiple tasks: {'embed', 'generate', 'reward', 'classify', 'score'}. Defaulting to 'generate'.\nWARNING 03-18 08:46:16 [config.py:660] gguf quantization is not fully optimized yet. The speed can be slower than non-quantized models.\nINFO 03-18 08:46:16 [config.py:1526] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\nINFO 03-18 08:46:17 [config.py:581] This model supports multiple tasks: {'reward', 'generate', 'embed', 'score', 'classify'}. Defaulting to 'generate'.\nWARNING 03-18 08:46:17 [config.py:660] gguf quantization is not fully optimized yet. The speed can be slower than non-quantized models.\nINFO 03-18 08:46:17 [config.py:1526] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\nINFO 03-18 08:46:19 [llm_engine.py:235] Initializing a V0 LLM engine (v0.7.4.dev442+gfd8e055f) with config: model='/app/model/DeepSeek-R1-Distill-Qwen-32B-GGUF/DeepSeek-R1-Distill-Qwen-32B-Q4_K_M.gguf', speculative_config=None, tokenizer='/app/model/DeepSeek-R1-Distill-Qwen-32B-GGUF/DeepSeek-R1-Distill-Qwen-32B-Q4_K_M.gguf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.GGUF, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=gguf, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Deepseek-R1:32B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=True, \nINFO 03-18 08:46:36 [rocm.py:130] None is not supported in AMD GPUs.\nINFO 03-18 08:46:36 [rocm.py:131] Using ROCmFlashAttention backend.\nINFO 03-18 08:46:36 [parallel_state.py:948] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\nINFO 03-18 08:46:36 [model_runner.py:1110] Starting to load model /app/model/DeepSeek-R1-Distill-Qwen-32B-GGUF/DeepSeek-R1-Distill-Qwen-32B-Q4_K_M.gguf...\nSliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\nWARNING 03-18 08:46:40 [rocm.py:206] Model architecture 'Qwen2ForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n/usr/local/lib/python3.12/dist-packages/torch/nested/__init__.py:228: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /app/pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n  return _nested.nested_tensor(\nINFO 03-18 08:46:53 [model_runner.py:1146] Model loading took 18.9082 GB and 16.624474 seconds\nINFO 03-18 08:50:01 [worker.py:267] Memory profiling takes 187.38 seconds\nINFO 03-18 08:50:01 [worker.py:267] the current vLLM instance can use total_gpu_memory (23.98GiB) x gpu_memory_utilization (0.96) = 23.02GiB\nINFO 03-18 08:50:01 [worker.py:267] model weights take 18.91GiB; non_torch_memory takes 0.25GiB; PyTorch activation peak memory takes 1.42GiB; the rest of the memory reserved for KV Cache is 2.44GiB.\nINFO 03-18 08:50:01 [executor_base.py:111] # rocm blocks: 625, # CPU blocks: 1024\nINFO 03-18 08:50:01 [executor_base.py:116] Maximum concurrency for 4096 tokens per request: 2.44x\nINFO 03-18 08:50:01 [model_runner.py:1442] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nCapturing CUDA graph shapes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 35/35 [09:52<00:00, 16.93s/it]\nINFO 03-18 08:59:54 [model_runner.py:1570] Graph capturing finished in 593 secs, took 0.15 GiB\nINFO 03-18 08:59:54 [llm_engine.py:441] init engine (profile, create kv cache, warmup model) took 780.78 seconds\nINFO 03-18 09:00:11 [api_server.py:958] Starting vLLM API server on http://0.0.0.0:8199\nINFO 03-18 09:00:11 [launcher.py:26] Available routes are:\nINFO 03-18 09:00:11 [launcher.py:34] Route: /openapi.json, Methods: HEAD, GET\nINFO 03-18 09:00:11 [launcher.py:34] Route: /docs, Methods: HEAD, GET\nINFO 03-18 09:00:11 [launcher.py:34] Route: /docs/oauth2-redirect, Methods: HEAD, GET\nINFO 03-18 09:00:11 [launcher.py:34] Route: /redoc, Methods: HEAD, GET\nINFO 03-18 09:00:11 [launcher.py:34] Route: /health, Methods: GET\nINFO 03-18 09:00:11 [launcher.py:34] Route: /ping, Methods: POST, GET\nINFO 03-18 09:00:11 [launcher.py:34] Route: /tokenize, Methods: POST\nINFO 03-18 09:00:11 [launcher.py:34] Route: /detokenize, Methods: POST\nINFO 03-18 09:00:11 [launcher.py:34] Route: /v1/models, Methods: GET\nINFO 03-18 09:00:11 [launcher.py:34] Route: /version, Methods: GET\nINFO 03-18 09:00:11 [launcher.py:34] Route: /v1/chat/completions, Methods: POST\nINFO 03-18 09:00:11 [launcher.py:34] Route: /v1/completions, Methods: POST\nINFO 03-18 09:00:11 [launcher.py:34] Route: /v1/embeddings, Methods: POST\nINFO 03-18 09:00:11 [launcher.py:34] Route: /pooling, Methods: POST\nINFO 03-18 09:00:11 [launcher.py:34] Route: /score, Methods: POST\nINFO 03-18 09:00:11 [launcher.py:34] Route: /v1/score, Methods: POST\nINFO 03-18 09:00:11 [launcher.py:34] Route: /v1/audio/transcriptions, Methods: POST\nINFO 03-18 09:00:11 [launcher.py:34] Route: /rerank, Methods: POST\nINFO 03-18 09:00:11 [launcher.py:34] Route: /v1/rerank, Methods: POST\nINFO 03-18 09:00:11 [launcher.py:34] Route: /v2/rerank, Methods: POST\nINFO 03-18 09:00:11 [launcher.py:34] Route: /invocations, Methods: POST\nINFO:     Started server process [621]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nERROR 03-18 09:02:03 [serving_chat.py:133] Error with model object='error' message='The model `QwQ-32B-Q4_K_M` does not exist.' type='NotFoundError' param=None code=404\nINFO:     192.168.1.168:12475 - \"POST /v1/chat/completions HTTP/1.1\" 404 Not Found\nERROR 03-18 09:02:34 [serving_chat.py:133] Error with model object='error' message='The model `DeepSeek-R1-Distill-Qwen-32B-Q4_K_M` does not exist.' type='NotFoundError' param=None code=404\nINFO:     192.168.1.168:12509 - \"POST /v1/chat/completions HTTP/1.1\" 404 Not Found\nINFO 03-18 09:03:16 [chat_utils.py:346] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\nINFO 03-18 09:03:16 [logger.py:39] Received request chatcmpl-6e1744b21b174b9489bd4cea3b8b4b13: prompt: '<\uff5cUser\uff5c>\u4ec0\u4e48\u662f\u6279\u91cf\u5f52\u4e00\u5316\uff08Batch Normalization\uff09\uff1f<\uff5cAssistant\uff5c>', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.9, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=2048, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\nINFO:     192.168.1.168:12564 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nINFO 03-18 09:03:16 [engine.py:289] Added request chatcmpl-6e1744b21b174b9489bd4cea3b8b4b13.\nINFO 03-18 09:03:21 [metrics.py:481] Avg prompt throughput: 3.1 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\nINFO 03-18 09:03:26 [metrics.py:481] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.\nINFO 03-18 09:03:31 [metrics.py:481] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.2 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nINFO 03-18 09:03:41 [metrics.py:481] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nINFO 03-18 09:03:43 [logger.py:39] Received request chatcmpl-ccb9a0c7cb1448489054051f6f39c4cf: prompt: '<\uff5cUser\uff5c>\u4ec0\u4e48\u662f\u63a8\u8350\u7cfb\u7edf\uff1f<\uff5cAssistant\uff5c>', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.9, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=2048, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\nINFO:     192.168.1.168:12595 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nINFO 03-18 09:03:43 [engine.py:289] Added request chatcmpl-ccb9a0c7cb1448489054051f6f39c4cf.\nINFO 03-18 09:03:43 [logger.py:39] Received request chatcmpl-e7b6a1e58d994b3caef5e397b8728e28: prompt: '<\uff5cUser\uff5c>\u4ec0\u4e48\u662fAutoML\uff1f<\uff5cAssistant\uff5c>', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.9, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=2048, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\nINFO:     192.168.1.168:12596 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nINFO 03-18 09:03:49 [engine.py:289] Added request chatcmpl-e7b6a1e58d994b3caef5e397b8728e28.\nINFO 03-18 09:03:49 [metrics.py:481] Avg prompt throughput: 1.9 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\nINFO 03-18 09:03:58 [metrics.py:481] Avg prompt throughput: 1.5 tokens/s, Avg generation throughput: 0.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\nINFO 03-18 09:04:08 [metrics.py:481] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.\nINFO 03-18 09:04:17 [metrics.py:481] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.\nINFO 03-18 09:04:27 [metrics.py:481] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.\nINFO 03-18 09:04:37 [metrics.py:481] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.\nINFO 03-18 09:04:46 [metrics.py:481] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.\nINFO 03-18 09:04:56 [metrics.py:481] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.\nINFO 03-18 09:05:05 [metrics.py:481] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.\nINFO 03-18 09:05:15 [metrics.py:481] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.\n\n\n### Your current environment (if you think it is necessary)\n\n```text\nPyTorch version: 2.6.0a0+git8d4926e\nIs debug build: False\nCUDA used to build PyTorch: N/A\nROCM used to build PyTorch: 6.3.42133-1b9c17779\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: 18.0.0git (https://github.com/RadeonOpenCompute/llvm-project roc-6.3.1 24491 1e0fda770a2079fbd71e4b70974d74f62fd3af10)\nCMake version: version 3.31.6\nLibc version: glibc-2.35\n\nPython version: 3.12.9 (main, Feb  5 2025, 08:49:00) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-6.11.0-19-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: Radeon RX 7900 XTX (gfx1100)\nNvidia driver version: Could not collect\ncuDNN version: Could not collect\nHIP runtime version: 6.3.42133\nMIOpen runtime version: 3.3.0\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        48 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               32\nOn-line CPU(s) list:                  0-31\nVendor ID:                            AuthenticAMD\nModel name:                           AMD Ryzen 9 9950X 16-Core Processor\nCPU family:                           26\nModel:                                68\nThread(s) per core:                   2\nCore(s) per socket:                   16\nSocket(s):                            1\nStepping:                             0\nFrequency boost:                      enabled\nCPU max MHz:                          5752.0000\nCPU min MHz:                          600.0000\nBogoMIPS:                             8583.31\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good amd_lbr_v2 nopl xtopology nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba perfmon_v2 ibrs ibpb stibp ibrs_enhanced vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local user_shstk avx_vnni avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif x2avic v_spec_ctrl vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid bus_lock_detect movdiri movdir64b overflow_recov succor smca fsrm avx512_vp2intersect flush_l1d amd_lbr_pmc_freeze\nVirtualization:                       AMD-V\nL1d cache:                            768 KiB (16 instances)\nL1i cache:                            512 KiB (16 instances)\nL2 cache:                             16 MiB (16 instances)\nL3 cache:                             64 MiB (2 instances)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-31\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] pyzmq==26.3.0\n[pip3] torch==2.6.0a0+git8d4926e\n[pip3] torchvision==0.19.1a0+6194369\n[pip3] transformers==4.49.0\n[pip3] triton==3.2.0+gite5be006a\n[conda] Could not collect\nROCM Version: 6.3.42133-1b9c17779\nNeuron SDK Version: N/A\nvLLM Version: 0.7.4.dev442+gfd8e055f\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n============================ ROCm System Management Interface ============================\n================================ Weight between two GPUs =================================\n       GPU0         GPU1         \nGPU0   0            40           \nGPU1   40           0            \n\n================================= Hops between two GPUs ==================================\n       GPU0         GPU1         \nGPU0   0            2            \nGPU1   2            0            \n\n=============================== Link Type between two GPUs ===============================\n       GPU0         GPU1         \nGPU0   0            PCIE         \nGPU1   PCIE         0            \n\n======================================= Numa Nodes =======================================\nGPU[0]\t\t: (Topology) Numa Node: 0\nGPU[0]\t\t: (Topology) Numa Affinity: -1\nGPU[1]\t\t: (Topology) Numa Node: 0\nGPU[1]\t\t: (Topology) Numa Affinity: -1\n================================== End of ROCm SMI Log ===================================\n\nPYTORCH_ROCM_ARCH=gfx1100;gfx1101;gfx1200;gfx1201\nLD_LIBRARY_PATH=/usr/local/lib/python3.12/dist-packages/cv2/../../lib64:/opt/rocm/lib:/usr/local/lib:\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n```\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "performance",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-18T09:17:19+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15018/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15018"
  }
}