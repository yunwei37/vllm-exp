{
  "issue_type": "performance",
  "extracted_from": "/root/yunwei37/vllm-exp/bug-study/analysis_llm/sample_results/vllm/label_based/performance/issues.json",
  "issue": {
    "number": 4702,
    "title": "[Performance]: why hf is better than vllm when using benchmark throughput",
    "body": "When I run benchmark on H800,  the results are confusing. Why hf is better than vllm? Is anything wrong when I run the script?\r\n\r\n```\r\npython benchmark_throughput.py --input-len 128 --model /home/jiekong/.cache/modelscope/hub/AI-ModelScope/opt-125 --output-len 128 --max-num-batched-tokens 2048 --trust-remote-code\r\n```\r\nThroughput: 59.50 requests/s, 15231.62 tokens/s\r\n\r\n![image](https://github.com/vllm-project/vllm/assets/12995855/92d2d824-da47-43f2-aa59-78ff44ad0cd9)\r\n\r\n```\r\npython benchmark_throughput.py --input-len 128 --model /home/jiekong/.cache/modelscope/hub/AI-ModelScope/opt-125 --output-len 128 --backend hf --hf-max-batch-size 256\r\n```\r\nThroughput: 108.34 requests/s, 27736.31 tokens/s\r\n\r\n![image](https://github.com/vllm-project/vllm/assets/12995855/ce316880-4b7d-408d-9189-25a15731691e)\r\n",
    "labels": [
      "performance",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-05-09T06:32:31+00:00",
    "closed_at": "2024-11-21T03:01:34+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4702/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4702"
  }
}