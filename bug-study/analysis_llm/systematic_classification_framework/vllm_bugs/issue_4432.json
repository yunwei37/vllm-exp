{
  "issue_type": "bug",
  "extracted_from": "/root/yunwei37/vllm-exp/bug-study/analysis_llm/sample_results/vllm/label_based/bug/issues.json",
  "issue": {
    "number": 4432,
    "title": "[Bug]:  all_reduce assert result == 0, File \"torch/cuda/graphs.py\", line 88, in capture_end    super().capture_end(), RuntimeError: CUDA error: operation failed due to a previous error during capture",
    "body": "### Your current environment\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.1.2+cu118\r\nIs debug build: False\r\nCUDA used to build PyTorch: 11.8\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Centos 7 (Final) (x86_64)\r\nGCC version: (GCC) 7.3.0\r\nClang version: Could not collect\r\nCMake version: version 3.26.1\r\nLibc version: glibc-2.17\r\n\r\nPython version: 3.8.12 (default, Nov 11 2021, 20:11:20)  [GCC 7.3.0] (64-bit runtime)\r\nPython platform: Linux-4.14.105-1-tlinux3-0013-x86_64-with-glibc2.2.5\r\nIs CUDA available: True\r\nCUDA runtime version: 11.8.89\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: Tesla V100-SXM2-32GB\r\nGPU 1: Tesla V100-SXM2-32GB\r\nGPU 2: Tesla V100-SXM2-32GB\r\nGPU 3: Tesla V100-SXM2-32GB\r\nGPU 4: Tesla V100-SXM2-32GB\r\nGPU 5: Tesla V100-SXM2-32GB\r\nGPU 6: Tesla V100-SXM2-32GB\r\nGPU 7: Tesla V100-SXM2-32GB\r\n\r\nNvidia driver version: 450.156.00\r\ncuDNN version: Probably one of the following:\r\n/usr/lib64/libcudnn.so.8.0.5\r\n/usr/lib64/libcudnn_adv_infer.so.8.0.5\r\n/usr/lib64/libcudnn_adv_train.so.8.0.5\r\n/usr/lib64/libcudnn_cnn_infer.so.8.0.5\r\n/usr/lib64/libcudnn_cnn_train.so.8.0.5\r\n/usr/lib64/libcudnn_ops_infer.so.8.0.5\r\n/usr/lib64/libcudnn_ops_train.so.8.0.5\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:          x86_64\r\nCPU op-mode(s):        32-bit, 64-bit\r\nByte Order:            Little Endian\r\nCPU(s):                96\r\nOn-line CPU(s) list:   0-95\r\nThread(s) per core:    2\r\nCore(s) per socket:    24\r\nSocket(s):             2\r\nNUMA node(s):          2\r\nVendor ID:             GenuineIntel\r\nCPU family:            6\r\nModel:                 85\r\nModel name:            Intel(R) Xeon(R) Platinum 8255C CPU @ 2.50GHz\r\nStepping:              7\r\nCPU MHz:               3099.587\r\nCPU max MHz:           2501.0000\r\nCPU min MHz:           1000.0000\r\nBogoMIPS:              5000.00\r\nVirtualization:        VT-x\r\nL1d cache:             32K\r\nL1i cache:             32K\r\nL2 cache:              1024K\r\nL3 cache:              36608K\r\nNUMA node0 CPU(s):     0-23,48-71\r\nNUMA node1 CPU(s):     24-47,72-95\r\nFlags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts pku ospke avx512_vnni flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.23.5\r\n[pip3] nvidia-nccl-cu11==2.19.3\r\n[pip3] pytorchvideo==0.1.5\r\n[pip3] torch==2.1.2+cu118\r\n[pip3] torchaudio==0.9.0\r\n[pip3] torchdata==0.6.0\r\n[pip3] torchvision==0.15.2\r\n[pip3] triton==2.1.0\r\n[conda] Could not collectROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.0.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    mlx5_0  mlx5_1  mlx5_2  mlx5_3  mlx5_4  mlx5_5  mlx5_6  mlx5_7  mlx5_8  mlx5_9  mlx5_10 mlx5_11 mlx5_12 mlx5_13 mlx5_14 mlx5_15 mlx5_16   mlx5_17 CPU Affinity    NUMA Affinity\r\nGPU0     X      NV1     NV2     NV1     SYS     SYS     SYS     NV2     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE      NODE    NODE    0-23,48-71      0\r\nGPU1    NV1      X      NV1     NV2     SYS     SYS     NV2     SYS     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE      NODE    NODE    0-23,48-71      0\r\nGPU2    NV2     NV1      X      NV2     SYS     NV1     SYS     SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     0-23,48-71      0\r\nGPU3    NV1     NV2     NV2      X      NV1     SYS     SYS     SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     0-23,48-71      0\r\nGPU4    SYS     SYS     SYS     NV1      X      NV2     NV2     NV1     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS       SYS     SYS     24-47,72-95     1\r\nGPU5    SYS     SYS     NV1     SYS     NV2      X      NV1     NV2     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS       SYS     SYS     24-47,72-95     1\r\nGPU6    SYS     NV2     SYS     SYS     NV2     NV1      X      NV1     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS       SYS     SYS     24-47,72-95     1\r\nGPU7    NV2     SYS     SYS     SYS     NV1     NV2     NV1      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS       SYS     SYS     24-47,72-95     1\r\nmlx5_0  NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX\r\nmlx5_1  NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX\r\nmlx5_2  NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX\r\nmlx5_3  NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX\r\nmlx5_4  NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX\r\nmlx5_5  NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX\r\nmlx5_6  NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX\r\nmlx5_7  NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX\r\nmlx5_8  NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX\r\nmlx5_9  NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX\r\nmlx5_10 NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX       PIX     PIX\r\nmlx5_11 NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX       PIX     PIX\r\nmlx5_12 NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX       PIX     PIX\r\nmlx5_13 NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX       PIX     PIX\r\nmlx5_14 NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX       PIX     PIX\r\nmlx5_15 NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X        PIX     PIX\r\nmlx5_16 NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX        X      PIX\r\nmlx5_17 NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\n## when i run starcoder2, error come out:\r\n\r\n2024-04-28 20:49:41,941 INFO worker.py:1752 -- Started a local Ray instance.\r\nINFO 04-28 20:49:43 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='/apdcephfs_cq10/share_1567347/share_info/llm_models/starcoder2-15b', tokenizer='/apdcephfs_cq10/share_1567347/share_info/llm_models/starcoder2-15b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\r\n/usr/local/python/lib/python3.8/site-packages/vllm/executor/ray_gpu_executor.py:87: UserWarning: Failed to get the IP address, using 0.0.0.0 by default.The value can be set by the environment variable HOST_IP.\r\n  driver_ip = get_ip()\r\n(RayWorkerVllm pid=62031) /usr/local/python/lib/python3.8/site-packages/vllm/engine/ray_utils.py:48: UserWarning: Failed to get the IP address, using 0.0.0.0 by default.The value can be set by the environment variable HOST_IP.\r\n(RayWorkerVllm pid=62031)   return get_ip()\r\nINFO 04-28 20:49:50 selector.py:40] Cannot use FlashAttention backend for Volta and Turing GPUs.\r\nINFO 04-28 20:49:50 selector.py:25] Using XFormers backend.\r\n(RayWorkerVllm pid=62111) INFO 04-28 20:49:52 selector.py:40] Cannot use FlashAttention backend for Volta and Turing GPUs.\r\n(RayWorkerVllm pid=62111) INFO 04-28 20:49:52 selector.py:25] Using XFormers backend.\r\nINFO 04-28 20:49:52 pynccl_utils.py:45] vLLM is using nccl==2.10.3\r\n(RayWorkerVllm pid=62111) INFO 04-28 20:49:52 pynccl_utils.py:45] vLLM is using nccl==2.10.3\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO Bootstrap : Using eth1:9.91.2.209<0>\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO NET/Socket : Using [0]eth1:9.91.2.209<0>\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO Using network Socket\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO Bootstrap : Using eth1:9.91.2.209<0>\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO cudaDriverVersion 11080\r\nNCCL version 2.18.6+cuda11.8\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO NET/Socket : Using [0]eth1:9.91.2.209<0>\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO Using network Socket\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO comm 0x53f905b0 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1a000 commId 0x605ed9e94f174b - Init START\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to LOC\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO Channel 00/02 :    0   1\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO Channel 01/02 :    0   1\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO P2P Chunksize set to 131072\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO Channel 00 : 0[0] -> 1[1] via SHM/direct/direct\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO Channel 01 : 0[0] -> 1[1] via SHM/direct/direct\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO Connected all rings\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO Connected all trees\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO comm 0x53f905b0 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1a000 commId 0x605ed9e94f174b - Init COMPLETE\r\nNCCL version 2.10.3+cuda11.0\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to LOC\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO Channel 00/02 :    0   1\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO Channel 01/02 :    0   1\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO Channel 00 : 0[1a000] -> 1[1b000] via direct shared memory\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO Channel 01 : 0[1a000] -> 1[1b000] via direct shared memory\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO Connected all rings\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO Connected all trees\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/512\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO comm 0x5483be20 rank 0 nranks 2 cudaDev 0 busId 1a000 - Init COMPLETE\r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] NCCL INFO Bootstrap : Using eth1:9.91.2.209<0>\r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] NCCL INFO NET/Socket : Using [0]eth1:9.91.2.209<0>\r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] NCCL INFO Using network Socket\r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] NCCL INFO NCCL_P2P_LEVEL set by environment to LOC\r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0\r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] NCCL INFO Channel 00 : 1[1b000] -> 0[1a000] via direct shared memory\r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] NCCL INFO Channel 01 : 1[1b000] -> 0[1a000] via direct shared memory\r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] NCCL INFO Connected all rings\r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] NCCL INFO Connected all trees\r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/512\r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO Launch mode Parallel\r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] NCCL INFO comm 0xbd04190 rank 1 nranks 2 cudaDev 1 busId 1b000 - Init COMPLETE\r\nINFO 04-28 20:50:02 model_runner.py:104] Loading model weights took 14.8672 GB\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62379 [0] NCCL INFO Using network Socket\r\n(RayWorkerVllm pid=62111) INFO 04-28 20:50:12 model_runner.py:104] Loading model weights took 14.8672 GB\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62379 [0] NCCL INFO comm 0x98440b0 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1a000 commId 0x59afa392e79b7504 - Init START\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62379 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62379 [0] NCCL INFO Channel 00/02 :    0   1\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62379 [0] NCCL INFO Channel 01/02 :    0   1\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62379 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62379 [0] NCCL INFO P2P Chunksize set to 131072\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62379 [0] NCCL INFO Channel 00 : 0[0] -> 1[1] via SHM/direct/direct\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62379 [0] NCCL INFO Channel 01 : 0[0] -> 1[1] via SHM/direct/direct\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62379 [0] NCCL INFO Connected all rings\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62379 [0] NCCL INFO Connected all trees\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62379 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62379 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62379 [0] NCCL INFO comm 0x98440b0 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1a000 commId 0x59afa392e79b7504 - Init COMPLETE\r\nINFO 04-28 20:50:20 ray_gpu_executor.py:240] # GPU blocks: 15176, # CPU blocks: 6553\r\nINFO 04-28 20:50:23 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\nINFO 04-28 20:50:23 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n(RayWorkerVllm pid=62111) INFO 04-28 20:50:23 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\n(RayWorkerVllm pid=62111) INFO 04-28 20:50:23 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] enqueue.cc:267 NCCL WARN Cuda failure 'dependency created on uncaptured work in another stream'\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO enqueue.cc:1045 -> 1\r\nTraceback (most recent call last):\r\n  File \"/usr/local/python/lib/python3.8/site-packages/vllm/worker/model_runner.py\", line 921, in capture\r\n    hidden_states = self.model(\r\n  File \"/usr/local/python/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/python/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/python/lib/python3.8/site-packages/vllm/model_executor/models/starcoder2.py\", line 260, in forward\r\n    hidden_states = self.model(input_ids, positions, kv_caches,\r\n  File \"/usr/local/python/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/python/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/python/lib/python3.8/site-packages/vllm/model_executor/models/starcoder2.py\", line 219, in forward\r\n    hidden_states = self.embed_tokens(input_ids)\r\n  File \"/usr/local/python/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/python/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/python/lib/python3.8/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py\", line 107, in forward\r\n    output = tensor_model_parallel_all_reduce(output_parallel)\r\n  File \"/usr/local/python/lib/python3.8/site-packages/vllm/model_executor/parallel_utils/communication_op.py\", line 35, in tensor_model_parallel_all_reduce\r\n    pynccl_utils.all_reduce(input_)\r\n  File \"/usr/local/python/lib/python3.8/site-packages/vllm/model_executor/parallel_utils/pynccl_utils.py\", line 55, in all_reduce\r\n    comm.all_reduce(input_, op)\r\n  File \"/usr/local/python/lib/python3.8/site-packages/vllm/model_executor/parallel_utils/pynccl.py\", line 258, in all_reduce\r\n    assert result == 0\r\nAssertionError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"lua_test_file_gen_vllm.py\", line 221, in <module>\r\n    main()\r\n  File \"lua_test_file_gen_vllm.py\", line 105, in main\r\n    llm = LLM(model=args.model, tensor_parallel_size=args.num_gpus, dtype=\"float16\", gpu_memory_utilization=0.9)\r\n  File \"/usr/local/python/lib/python3.8/site-packages/vllm/entrypoints/llm.py\", line 112, in __init__\r\n    self.llm_engine = LLMEngine.from_engine_args(\r\n  File \"/usr/local/python/lib/python3.8/site-packages/vllm/engine/llm_engine.py\", line 196, in from_engine_args\r\n    engine = cls(\r\n  File \"/usr/local/python/lib/python3.8/site-packages/vllm/engine/llm_engine.py\", line 110, in __init__\r\n    self.model_executor = executor_class(model_config, cache_config,\r\n  File \"/usr/local/python/lib/python3.8/site-packages/vllm/executor/ray_gpu_executor.py\", line 65, in __init__\r\n    self._init_cache()\r\n  File \"/usr/local/python/lib/python3.8/site-packages/vllm/executor/ray_gpu_executor.py\", line 253, in _init_cache\r\n    self._run_workers(\"warm_up_model\")\r\n  File \"/usr/local/python/lib/python3.8/site-packages/vllm/executor/ray_gpu_executor.py\", line 324, in _run_workers\r\n    driver_worker_output = getattr(self.driver_worker,\r\n  File \"/usr/local/python/lib/python3.8/site-packages/vllm/worker/worker.py\", line 167, in warm_up_model\r\n    self.model_runner.capture_model(self.gpu_cache)\r\n  File \"/usr/local/python/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/python/lib/python3.8/site-packages/vllm/worker/model_runner.py\", line 854, in capture_model\r\n    graph_runner.capture(\r\n  File \"/usr/local/python/lib/python3.8/site-packages/vllm/worker/model_runner.py\", line 921, in capture\r\n    hidden_states = self.model(\r\n  File \"/usr/local/python/lib/python3.8/site-packages/torch/cuda/graphs.py\", line 197, in __exit__\r\n    self.cuda_graph.capture_end()\r\n  File \"/usr/local/python/lib/python3.8/site-packages/torch/cuda/graphs.py\", line 88, in capture_end\r\n    super().capture_end()\r\nRuntimeError: CUDA error: operation failed due to a previous error during capture\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\n(RayWorkerVllm pid=62111) \r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] enqueue.cc:267 NCCL WARN Cuda failure 'dependency created on uncaptured work in another stream'\r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] NCCL INFO enqueue.cc:1045 -> 1\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44] Error executing method warm_up_model. This might cause deadlock in distributed execution.\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44] Traceback (most recent call last):\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/vllm/worker/model_runner.py\", line 921, in capture\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     hidden_states = self.model(\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     return self._call_impl(*args, **kwargs)\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     return forward_call(*args, **kwargs)\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/vllm/model_executor/models/starcoder2.py\", line 260, in forward\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     hidden_states = self.model(input_ids, positions, kv_caches,\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     return self._call_impl(*args, **kwargs)\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     return forward_call(*args, **kwargs)\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/vllm/model_executor/models/starcoder2.py\", line 219, in forward\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     hidden_states = self.embed_tokens(input_ids)\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     return self._call_impl(*args, **kwargs)\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     return forward_call(*args, **kwargs)\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py\", line 107, in forward\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     output = tensor_model_parallel_all_reduce(output_parallel)\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/vllm/model_executor/parallel_utils/communication_op.py\", line 35, in tensor_model_parallel_all_reduce\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     pynccl_utils.all_reduce(input_)\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/vllm/model_executor/parallel_utils/pynccl_utils.py\", line 55, in all_reduce\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     comm.all_reduce(input_, op)\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/vllm/model_executor/parallel_utils/pynccl.py\", line 258, in all_reduce\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     assert result == 0\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44] AssertionError\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44] \r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44] During handling of the above exception, another exception occurred:\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44] \r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44] Traceback (most recent call last):\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/vllm/engine/ray_utils.py\", line 37, in execute_method\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     return executor(*args, **kwargs)\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/vllm/worker/worker.py\", line 167, in warm_up_model\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     self.model_runner.capture_model(self.gpu_cache)\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     return func(*args, **kwargs)\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/vllm/worker/model_runner.py\", line 854, in capture_model\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     graph_runner.capture(\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/vllm/worker/model_runner.py\", line 921, in capture\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     hidden_states = self.model(\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/torch/cuda/graphs.py\", line 197, in __exit__\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     self.cuda_graph.capture_end()\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/torch/cuda/graphs.py\", line 88, in capture_end\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     super().capture_end()\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44] RuntimeError: CUDA error: operation failed due to a previous error during capture\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44] For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44] \r\n(RayWorkerVllm pid=62111) /usr/local/python/lib/python3.8/site-packages/vllm/engine/ray_utils.py:48: UserWarning: Failed to get the IP address, using 0.0.0.0 by default.The value can be set by the environment variable HOST_IP.\r\n(RayWorkerVllm pid=62111)   return get_ip()",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-04-28T13:01:49+00:00",
    "closed_at": "2024-11-28T02:05:53+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4432/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4432"
  }
}