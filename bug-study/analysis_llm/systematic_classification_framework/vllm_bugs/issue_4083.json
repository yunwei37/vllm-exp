{
  "issue_type": "bug",
  "extracted_from": "/root/yunwei37/vllm-exp/bug-study/analysis_llm/sample_results/vllm/label_based/bug/issues.json",
  "issue": {
    "number": 4083,
    "title": "[Bug]: vllm_C is missing. ",
    "body": "### Your current environment\n\nPrevious fix from https://github.com/vllm-project/vllm/pull/3913 did not seem to work. Same issue still encountered. \r\n\r\n```text\r\nCollecting environment information...\r\nINFO 04-15 07:13:37 pynccl.py:58] Loading nccl from library /home/me/.config/vllm/nccl/cu12/libnccl.so.2.18.1\r\nPyTorch version: 2.1.2+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Debian GNU/Linux 11 (bullseye) (x86_64)\r\nGCC version: (Debian 10.2.1-6) 10.2.1 20210110\r\nClang version: 11.0.1-2\r\nCMake version: version 3.29.2\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.9.2 (default, Feb 28 2021, 17:03:44)  [GCC 10.2.1 20210110] (64-bit runtime)\r\nPython platform: Linux-5.16.0-0.bpo.4-amd64-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A100-SXM4-80GB\r\nGPU 1: NVIDIA A100-SXM4-80GB\r\nGPU 2: NVIDIA A100-SXM4-80GB\r\nGPU 3: NVIDIA A100-SXM4-80GB\r\nGPU 4: NVIDIA A100-SXM4-80GB\r\nGPU 5: NVIDIA A100-SXM4-80GB\r\nGPU 6: NVIDIA A100-SXM4-80GB\r\nGPU 7: NVIDIA A100-SXM4-80GB\r\n\r\nNvidia driver version: 535.54.03\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   48 bits physical, 48 bits virtual\r\nCPU(s):                          256\r\nOn-line CPU(s) list:             0-255\r\nThread(s) per core:              2\r\nCore(s) per socket:              64\r\nSocket(s):                       2\r\nNUMA node(s):                    8\r\nVendor ID:                       AuthenticAMD\r\nCPU family:                      25\r\nModel:                           1\r\nModel name:                      AMD EPYC 7763 64-Core Processor\r\nStepping:                        1\r\nCPU MHz:                         2381.263\r\nBogoMIPS:                        4890.70\r\nVirtualization:                  AMD-V\r\nL1d cache:                       4 MiB\r\nL1i cache:                       4 MiB\r\nL2 cache:                        64 MiB\r\nL3 cache:                        512 MiB\r\nNUMA node0 CPU(s):               0-15,128-143\r\nNUMA node1 CPU(s):               16-31,144-159\r\nNUMA node2 CPU(s):               32-47,160-175\r\nNUMA node3 CPU(s):               48-63,176-191\r\nNUMA node4 CPU(s):               64-79,192-207\r\nNUMA node5 CPU(s):               80-95,208-223\r\nNUMA node6 CPU(s):               96-111,224-239\r\nNUMA node7 CPU(s):               112-127,240-255\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Spec store bypass: Vulnerable\r\nVulnerability Spectre v1:        Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\r\nVulnerability Spectre v2:        Vulnerable, IBPB: disabled, STIBP: disabled\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.1.2\r\n[pip3] triton==2.1.0\r\n[conda] Could not collectROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.0.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tNIC1\tNIC2\tNIC3\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tSYS\tPXB\tSYS\tSYS\t48-63,176-191\t3\t\tN/A\r\nGPU1\tNV12\t X \tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tSYS\tPXB\tSYS\tSYS\t48-63,176-191\t3\t\tN/A\r\nGPU2\tNV12\tNV12\t X \tNV12\tNV12\tNV12\tNV12\tNV12\tPXB\tSYS\tSYS\tSYS\t16-31,144-159\t1\t\tN/A\r\nGPU3\tNV12\tNV12\tNV12\t X \tNV12\tNV12\tNV12\tNV12\tPXB\tSYS\tSYS\tSYS\t16-31,144-159\t1\t\tN/A\r\nGPU4\tNV12\tNV12\tNV12\tNV12\t X \tNV12\tNV12\tNV12\tSYS\tSYS\tSYS\tPXB\t112-127,240-255\t7\t\tN/A\r\nGPU5\tNV12\tNV12\tNV12\tNV12\tNV12\t X \tNV12\tNV12\tSYS\tSYS\tSYS\tPXB\t112-127,240-255\t7\t\tN/A\r\nGPU6\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\t X \tNV12\tSYS\tSYS\tPXB\tSYS\t80-95,208-223\t5\t\tN/A\r\nGPU7\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\t X \tSYS\tSYS\tPXB\tSYS\t80-95,208-223\t5\t\tN/A\r\nNIC0\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\t X \tSYS\tSYS\tSYS\t\t\t\t\r\nNIC1\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tSYS\tSYS\t\t\t\t\r\nNIC2\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\t X \tSYS\t\t\t\t\r\nNIC3\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\t X \t\t\t\t\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\n```(vllm-venv) me@mycomputer:~/calvinn/vllm$ python -m vllm.entrypoints.openai.api_server \\\r\n--model facebook/opt-125m\r\nINFO 04-15 07:11:52 pynccl.py:58] Loading nccl from library /home/team/.config/vllm/nccl/cu12/libnccl.so.2.18.1\r\nINFO 04-15 07:11:53 api_server.py:149] vLLM API server version 0.4.0.post1\r\nINFO 04-15 07:11:53 api_server.py:150] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='facebook/opt-125m', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, speculative_model=None, num_speculative_tokens=None, tensorizer_uri=None, verify_hash=False, encryption_keyfile=None, num_readers=1, s3_access_key_id=None, s3_secret_access_key=None, s3_endpoint=None, vllm_tensorized=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)\r\nconfig.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 651/651 [00:00<00:00, 212kB/s]\r\nINFO 04-15 07:11:53 llm_engine.py:82] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, seed=0)\r\ntokenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 685/685 [00:00<00:00, 269kB/s]\r\nvocab.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 899k/899k [00:00<00:00, 3.76MB/s]\r\nmerges.txt: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 456k/456k [00:00<00:00, 954kB/s]\r\nspecial_tokens_map.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 441/441 [00:00<00:00, 564kB/s]\r\nINFO 04-15 07:11:59 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.\r\nINFO 04-15 07:11:59 selector.py:33] Using XFormers backend.\r\nINFO 04-15 07:12:00 weight_utils.py:197] Using model weights format ['*.bin']\r\npytorch_model.bin: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 251M/251M [00:00<00:00, 385MB/s]\r\nINFO 04-15 07:12:02 model_runner.py:169] Loading model weights took 0.2389 GB\r\nINFO 04-15 07:12:02 gpu_executor.py:80] # GPU blocks: 127977, # CPU blocks: 7281\r\nINFO 04-15 07:12:04 model_runner.py:967] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\nINFO 04-15 07:12:04 model_runner.py:971] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/team/calvinn/vllm/vllm/entrypoints/openai/api_server.py\", line 157, in <module>\r\n    engine = AsyncLLMEngine.from_engine_args(\r\n  File \"/home/team/calvinn/vllm/vllm/engine/async_llm_engine.py\", line 347, in from_engine_args\r\n    engine = cls(\r\n  File \"/home/team/calvinn/vllm/vllm/engine/async_llm_engine.py\", line 311, in __init__\r\n    self.engine = self._init_engine(*args, **kwargs)\r\n  File \"/home/team/calvinn/vllm/vllm/engine/async_llm_engine.py\", line 421, in _init_engine\r\n    return engine_class(*args, **kwargs)\r\n  File \"/home/team/calvinn/vllm/vllm/engine/llm_engine.py\", line 133, in __init__\r\n    self._initialize_kv_caches()\r\n  File \"/home/team/calvinn/vllm/vllm/engine/llm_engine.py\", line 204, in _initialize_kv_caches\r\n    self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)\r\n  File \"/home/team/calvinn/vllm/vllm/executor/gpu_executor.py\", line 83, in initialize_cache\r\n    self.driver_worker.initialize_cache(num_gpu_blocks, num_cpu_blocks)\r\n  File \"/home/team/calvinn/vllm/vllm/worker/worker.py\", line 175, in initialize_cache\r\n    self._warm_up_model()\r\n  File \"/home/team/calvinn/vllm/vllm/worker/worker.py\", line 186, in _warm_up_model\r\n    self.model_runner.capture_model(self.gpu_cache)\r\n  File \"/home/team/calvinn/vllm/vllm-venv/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/team/calvinn/vllm/vllm/worker/model_runner.py\", line 1035, in capture_model\r\n    graph_runner.capture(\r\n  File \"/home/team/calvinn/vllm/vllm/worker/model_runner.py\", line 1087, in capture\r\n    self.model(\r\n  File \"/home/team/calvinn/vllm/vllm-venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/team/calvinn/vllm/vllm-venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/team/calvinn/vllm/vllm/model_executor/models/opt.py\", line 300, in forward\r\n    hidden_states = self.model(input_ids, positions, kv_caches,\r\n  File \"/home/team/calvinn/vllm/vllm-venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/team/calvinn/vllm/vllm-venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/team/calvinn/vllm/vllm/model_executor/models/opt.py\", line 275, in forward\r\n    return self.decoder(input_ids, positions, kv_caches, attn_metadata)\r\n  File \"/home/team/calvinn/vllm/vllm-venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/team/calvinn/vllm/vllm-venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/team/calvinn/vllm/vllm/model_executor/models/opt.py\", line 249, in forward\r\n    hidden_states = layer(hidden_states, kv_caches[i], attn_metadata)\r\n  File \"/home/team/calvinn/vllm/vllm-venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/team/calvinn/vllm/vllm-venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/team/calvinn/vllm/vllm/model_executor/models/opt.py\", line 157, in forward\r\n    hidden_states = self.self_attn(hidden_states=hidden_states,\r\n  File \"/home/team/calvinn/vllm/vllm-venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/team/calvinn/vllm/vllm-venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/team/calvinn/vllm/vllm/model_executor/models/opt.py\", line 101, in forward\r\n    attn_output = self.attn(q, k, v, kv_cache, attn_metadata)\r\n  File \"/home/team/calvinn/vllm/vllm-venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/team/calvinn/vllm/vllm-venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/team/calvinn/vllm/vllm/attention/layer.py\", line 48, in forward\r\n    return self.impl.forward(query, key, value, kv_cache, attn_metadata,\r\n  File \"/home/team/calvinn/vllm/vllm/attention/backends/xformers.py\", line 200, in forward\r\n    PagedAttention.write_to_paged_cache(key, value, key_cache,\r\n  File \"/home/team/calvinn/vllm/vllm/attention/ops/paged_attn.py\", line 72, in write_to_paged_cache\r\n    ops.reshape_and_cache(\r\n  File \"/home/team/calvinn/vllm/vllm/_custom_ops.py\", line 175, in reshape_and_cache\r\n    vllm_cache_ops.reshape_and_cache(key, value, key_cache, value_cache,\r\nNameError: name 'vllm_cache_ops' is not defined```",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-04-15T07:15:06+00:00",
    "closed_at": "2024-06-13T09:16:35+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4083/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 2
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/4083"
  }
}