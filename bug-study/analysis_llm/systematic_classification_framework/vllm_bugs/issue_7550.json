{
  "issue_type": "bug",
  "extracted_from": "/root/yunwei37/vllm-exp/bug-study/analysis_llm/sample_results/vllm/label_based/bug/issues.json",
  "issue": {
    "number": 7550,
    "title": "[Bug]:  online fp8 quantization with jais model got assert error due to cutlass_scaled_mm()",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n\r\nNvidia driver version: 555.42.06\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nByte Order:                         Little Endian\r\nAddress sizes:                      52 bits physical, 57 bits virtual\r\nCPU(s):                             384\r\nOn-line CPU(s) list:                0-383\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 96\r\nSocket(s):                          2\r\nNUMA node(s):                       2\r\nVendor ID:                          AuthenticAMD\r\nCPU family:                         25\r\nModel:                              17\r\nModel name:                         AMD EPYC 9654 96-Core Processor\r\nStepping:                           1\r\nFrequency boost:                    enabled\r\nCPU MHz:                            1500.000\r\nCPU max MHz:                        3707.8120\r\nCPU min MHz:                        1500.0000\r\nBogoMIPS:                           4799.99\r\nVirtualization:                     AMD-V\r\nL1d cache:                          6 MiB\r\nL1i cache:                          6 MiB\r\nL2 cache:                           192 MiB\r\nL3 cache:                           768 MiB\r\nNUMA node0 CPU(s):                  0-95,192-287\r\nNUMA node1 CPU(s):                  96-191,288-383\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Mitigation; Safe RET\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced / Automatic IBRS; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good amd_lbr_v2 nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba perfmon_v2 ibrs ibpb stibp ibrs_enhanced vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif x2avic v_spec_ctrl vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid overflow_recov succor smca fsrm flush_l1d\r\n\r\nVersions of relevant libraries:\r\n[pip3] flashinfer==0.1.2+cu121torch2.4\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] pyzmq==26.1.0\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.44.0\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.4@\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\n\r\n\r\n</details>\r\n\n\n### \ud83d\udc1b Describe the bug\n\nrun `throughput benchmark.py`  with jais-13B/30B models with following command:\r\n\r\n```sh\r\n python3 /vllm/benchmarks/benchmark_throughput.py --model core42/jais-13b-chat  --num-prompts $req -tp $tp --distributed-executor-backend mp --input-len $inp --output-len $out --trust-remote-code --dtype auto --enforce-eager   --quantization fp8\r\n```\r\n\r\nerror logs as:\r\n\r\n```yml\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/jais.py\", line 206, in forward\r\n[rank0]:     feed_forward_hidden_states = self.mlp(hidden_states)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/jais.py\", line 162, in forward\r\n[rank0]:     hidden_states2, _ = self.c_fc2(hidden_states)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/linear.py\", line 355, in forward\r\n[rank0]:     output_parallel = self.quant_method.apply(self, input_, bias)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/quantization/fp8.py\", line 240, in apply\r\n[rank0]:     return apply_fp8_linear(\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/quantization/utils/w8a8_utils.py\", line 126, in apply_fp8_linear\r\n[rank0]:     return ops.cutlass_scaled_mm(qinput,\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/_custom_ops.py\", line 32, in wrapper\r\n[rank0]:     return fn(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/_custom_ops.py\", line 242, in cutlass_scaled_mm\r\n[rank0]:     assert (b.shape[0] % 16 == 0 and b.shape[1] % 16 == 0)\r\n``` \r\n",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-15T09:28:39+00:00",
    "closed_at": "2025-04-16T02:20:10+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7550/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7550"
  }
}