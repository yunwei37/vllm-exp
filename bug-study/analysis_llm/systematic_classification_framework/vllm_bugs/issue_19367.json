{
  "issue_type": "bug",
  "extracted_from": "/root/yunwei37/vllm-exp/bug-study/analysis_llm/sample_results/vllm/label_based/bug/issues.json",
  "issue": {
    "number": 19367,
    "title": "[Bug]: Sliding Window Attention not supported in V1 for ROCm",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nINFO 06-09 15:59:09 [__init__.py:248] Automatically detected platform rocm.\nCollecting environment information...\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 22.04.5 LTS (x86_64)\nGCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version                : 18.0.0git (https://github.com/RadeonOpenCompute/llvm-project roc-6.3.1 24491 1e0fda770a2079fbd71e4b70974d74f62fd3af10)\nCMake version                : version 3.31.4\nLibc version                 : glibc-2.35\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.7.0a0+git6c0e746\nIs debug build               : False\nCUDA used to build PyTorch   : N/A\nROCM used to build PyTorch   : 6.3.42133-1b9c17779\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.12.9 (main, Feb  5 2025, 08:49:00) [GCC 11.4.0] (64-bit runtime)\nPython platform              : Linux-6.8.0-52-generic-x86_64-with-glibc2.35\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : Could not collect\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration : AMD Instinct MI250X/MI250 (gfx90a:sramecc+:xnack-)\nNvidia driver version        : Could not collect\ncuDNN version                : Could not collect\nHIP runtime version          : 6.3.42133\nMIOpen runtime version       : 3.3.0\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        48 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               128\nOn-line CPU(s) list:                  0-127\nVendor ID:                            AuthenticAMD\nModel name:                           AMD EPYC 7713 64-Core Processor\nCPU family:                           25\nModel:                                1\nThread(s) per core:                   1\nCore(s) per socket:                   64\nSocket(s):                            2\nStepping:                             1\nFrequency boost:                      enabled\nCPU max MHz:                          3720.7029\nCPU min MHz:                          1500.0000\nBogoMIPS:                             3992.52\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local user_shstk clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin brs arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm debug_swap\nVirtualization:                       AMD-V\nL1d cache:                            4 MiB (128 instances)\nL1i cache:                            4 MiB (128 instances)\nL2 cache:                             64 MiB (128 instances)\nL3 cache:                             512 MiB (16 instances)\nNUMA node(s):                         8\nNUMA node0 CPU(s):                    0-15\nNUMA node1 CPU(s):                    16-31\nNUMA node2 CPU(s):                    32-47\nNUMA node3 CPU(s):                    48-63\nNUMA node4 CPU(s):                    64-79\nNUMA node5 CPU(s):                    80-95\nNUMA node6 CPU(s):                    96-111\nNUMA node7 CPU(s):                    112-127\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Mitigation; Safe RET\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] numpy==1.26.4\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.7.0a0+git6c0e746\n[pip3] torchvision==0.21.0+7af6987\n[pip3] transformers==4.52.3\n[pip3] triton==3.2.0+gite5be006a\n[conda] Could not collect\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : 6.3.42133-1b9c17779\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.8.5.dev681+g964472b96.d20250528 (git sha: 964472b96, date: 20250528)\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n  ============================ ROCm System Management Interface ============================\n================================ Weight between two GPUs =================================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7\nGPU0   0            15           15           30           30           30           15           30\nGPU1   15           0            30           15           30           15           30           45\nGPU2   15           30           0            15           15           30           30           30\nGPU3   30           15           15           0            30           45           30           15\nGPU4   30           30           15           30           0            15           15           30\nGPU5   30           15           30           45           15           0            30           15\nGPU6   15           30           30           30           15           30           0            15\nGPU7   30           45           30           15           30           15           15           0\n\n================================= Hops between two GPUs ==================================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7\nGPU0   0            1            1            1            1            1            1            1\nGPU1   1            0            1            1            1            1            1            1\nGPU2   1            1            0            1            1            1            1            1\nGPU3   1            1            1            0            1            1            1            1\nGPU4   1            1            1            1            0            1            1            1\nGPU5   1            1            1            1            1            0            1            1\nGPU6   1            1            1            1            1            1            0            1\nGPU7   1            1            1            1            1            1            1            0\n\n=============================== Link Type between two GPUs ===============================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7\nGPU0   0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI\nGPU1   XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI\nGPU2   XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI\nGPU3   XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI\nGPU4   XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI\nGPU5   XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI\nGPU6   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI\nGPU7   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0\n\n======================================= Numa Nodes =======================================\nGPU[0]          : (Topology) Numa Node: 3\nGPU[0]          : (Topology) Numa Affinity: 3\nGPU[1]          : (Topology) Numa Node: 3\nGPU[1]          : (Topology) Numa Affinity: 3\nGPU[2]          : (Topology) Numa Node: 2\nGPU[2]          : (Topology) Numa Affinity: 2\nGPU[3]          : (Topology) Numa Node: 2\nGPU[3]          : (Topology) Numa Affinity: 2\nGPU[4]          : (Topology) Numa Node: 7\nGPU[4]          : (Topology) Numa Affinity: 7\nGPU[5]          : (Topology) Numa Node: 7\nGPU[5]          : (Topology) Numa Affinity: 7\nGPU[6]          : (Topology) Numa Node: 6\nGPU[6]          : (Topology) Numa Affinity: 6\nGPU[7]          : (Topology) Numa Node: 6\nGPU[7]          : (Topology) Numa Affinity: 6\n================================== End of ROCm SMI Log ===================================\n\n==============================\n     Environment Variables\n==============================\nPYTORCH_ROCM_ARCH=gfx90a;gfx942\nVLLM_ROCM_CUSTOM_PAGED_ATTN=1\nVLLM_TARGET_DEVICE=rocm\nLD_LIBRARY_PATH=/opt/rocm/lib:/usr/local/lib:\nVLLM_USE_V1=1\nVERBOSE=1\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nTORCHINDUCTOR_CACHE_DIR=/tmp/torchinductor_root\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nMultiple architectures, such as Qwen2, use Sliding Window Attention. However, there is no option in V1 to run Sliding Window Attention on ROCm. Sending a request to the server crashes, for:\n```bash\nMODEL_NAME=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\nexport VLLM_USE_V1=1\npython3 -m vllm.entrypoints.openai.api_server   --port 8080   --model $MODEL_NAME   --served-model-name $MODEL_NAME   --gpu-memory-utilization 0.95   --disable-custom-all-reduce   --tensor-parallel-size 1   --enable-chunked-prefill   --disable-log-requests   --enable-reasoning   --reasoning-parser deepseek_r1\n```\nThis is because it uses Triton Flash Attention, which not support Sliding Window Attention. As a result, sending a request to the server crashes vLLM:\n\n**Request:**\n```bash\ncurl -iX POST \"http://localhost:8080/v1/chat/completions\"         -H \"Content-Type: application/json\"         -d '{\n    \"model\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n    \"messages\": [{ \"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n    \"stream\": false\n  }'\n```\n\n\n**Resulting logs from crash:**\n``` \nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/triton/language/core.py\", line 34, in wrapper\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/triton/language/core.py\", line 1281, in arange\n    return semantic.arange(start, end, _builder)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/triton/language/semantic.py\", line 610, in arange\n    raise ValueError(\"arange's range must be a power of 2\")\nValueError: arange's range must be a power of 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/vllm/vllm/v1/engine/core.py\", line 493, in run_engine_core\n    raise e\n  File \"/vllm/vllm/v1/engine/core.py\", line 482, in run_engine_core\n    engine_core.run_busy_loop()\n  File \"/vllm/vllm/v1/engine/core.py\", line 509, in run_busy_loop\n    self._process_engine_step()\n  File \"/vllm/vllm/v1/engine/core.py\", line 534, in _process_engine_step\n    outputs = self.step_fn()\n              ^^^^^^^^^^^^^^\n  File \"/vllm/vllm/v1/engine/core.py\", line 222, in step\n    model_output = self.execute_model(scheduler_output)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/vllm/vllm/v1/engine/core.py\", line 209, in execute_model\n    raise err\n  File \"/vllm/vllm/v1/engine/core.py\", line 203, in execute_model\n    return self.model_executor.execute_model(scheduler_output)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/vllm/vllm/v1/executor/abstract.py\", line 86, in execute_model\n    output = self.collective_rpc(\"execute_model\",\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/vllm/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n    answer = run_method(self.driver_worker, method, args, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/vllm/vllm/utils.py\", line 2534, in run_method\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/vllm/vllm/v1/worker/gpu_worker.py\", line 276, in execute_model\n    output = self.model_runner.execute_model(scheduler_output,\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/vllm/vllm/v1/worker/gpu_model_runner.py\", line 1156, in execute_model\n    model_output = self.model(\n                   ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/vllm/vllm/model_executor/models/qwen2.py\", line 480, in forward\n    hidden_states = self.model(input_ids, positions, intermediate_tensors,\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/vllm/vllm/compilation/decorators.py\", line 245, in __call__\n    model_output = self.forward(*args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/vllm/vllm/model_executor/models/qwen2.py\", line 339, in forward\n    def forward(\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\", line 764, in _fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py\", line 830, in call_wrapped\n    return self._wrapped_call(self, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py\", line 406, in __call__\n    raise e\n  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py\", line 393, in __call__\n    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<eval_with_key>.58\", line 212, in forward\n    submod_1 = self.submod_1(getitem, s0, getitem_1, getitem_2, getitem_3);  getitem = getitem_1 = getitem_2 = submod_1 = None\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py\", line 830, in call_wrapped\n    return self._wrapped_call(self, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py\", line 406, in __call__\n    raise e\n  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py\", line 393, in __call__\n    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<eval_with_key>.2\", line 5, in forward\n    unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_2, key_2, value, output_1, 'model.layers.0.self_attn.attn');  query_2 = key_2 = value = output_1 = unified_attention_with_output = None\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_ops.py\", line 1156, in __call__\n    return self._op(*args, **(kwargs or {}))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/vllm/vllm/attention/layer.py\", line 425, in unified_attention_with_output\n    self.impl.forward(self,\n  File \"/vllm/vllm/v1/attention/backends/triton_attn.py\", line 201, in forward\n    unified_attention(\n  File \"/vllm/vllm/attention/ops/triton_unified_attention.py\", line 294, in unified_attention\n    kernel_unified_attention_2d[(\n  File \"/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py\", line 330, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py\", line 623, in run\n    kernel = self.compile(\n             ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py\", line 280, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py\", line 85, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntriton.compiler.errors.CompilationError: at 67:13:\n    q_block_local_idx = q_block_global_idx - q_block_start_idx\n\n    cur_batch_in_all_start_index = tl.load(query_start_len_ptr + seq_idx)\n    cur_batch_in_all_stop_index = tl.load(query_start_len_ptr + seq_idx + 1)\n\n    cur_batch_query_len = cur_batch_in_all_stop_index \\\n        - cur_batch_in_all_start_index\n\n    if q_block_local_idx * BLOCK_Q >= cur_batch_query_len:\n        return\n\n    offs_m = tl.arange(0, BLOCK_Q * num_queries_per_kv)\n             ^\n\n```\n\nIn V0, one can use ROCm's Custom Paged Attention, however this is not supported on V1, apparently due to [numerical instabilities on V1](https://github.com/vllm-project/vllm/blob/c1c7dbbeeb6d4f0155d25b673f2063bfb14b37b9/vllm/platforms/rocm.py#L135):\n```python \n   # custom paged attn always supported on V0. On V1, requires sliding window\n   # disabled due to observed numerical discrepancy.\n    if ON_GFX9:\n        return ((not envs.VLLM_USE_V1 or sliding_window == 0\n                 or sliding_window == (-1, -1))\n                and (qtype == torch.half or qtype == torch.bfloat16)\n                and (head_size == 64 or head_size == 128)\n                and (block_size == 16 or block_size == 32)\n                and (gqa_ratio >= 1 and gqa_ratio <= 16)\n                and max_seq_len <= 32768 and (envs.VLLM_ROCM_CUSTOM_PAGED_ATTN)\n                and not (envs.VLLM_ROCM_USE_AITER_PAGED_ATTN\n                         and envs.VLLM_ROCM_USE_AITER))\n\n    else:\n        return (ON_GFX11_GFX12 and (not envs.VLLM_USE_V1 or sliding_window == 0\n                                    or sliding_window == (-1, -1))\n                and (qtype == torch.half or qtype == torch.bfloat16)\n                and head_size == 128 and block_size == 16\n                and (gqa_ratio >= 3 and gqa_ratio <= 16)\n                and max_seq_len <= 32768 and alibi_slopes is None\n                and kv_cache_dtype == \"auto\"\n                and envs.VLLM_ROCM_CUSTOM_PAGED_ATTN)\n\n```\n\nSetting `VLLM_USE_TRITON_FLASH_ATTN=0` does not work on V1, as it will still use Triton Flash Attention despite the flag, and suffer from the resulting crash if one sends a request. E.g. from the logs:\n``` \nINFO 06-09 16:08:58 [rocm.py:184] Using Triton Attention backend on V1 engine.\n```\n\nAs such, there is no way to run Qwen2 architectures, or any architectures that use Sliding Window Attention, on ROCm in V1. [Given the plans to deprecate V0](https://github.com/vllm-project/vllm/issues/18571), this is going to be quite concerning for ROCm.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "rocm"
    ],
    "state": "closed",
    "created_at": "2025-06-09T16:12:29+00:00",
    "closed_at": "2025-06-10T20:28:04+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19367/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/19367"
  }
}