{
  "issue_type": "performance",
  "extracted_from": "/root/yunwei37/vllm-exp/bug-study/analysis_llm/sample_results/vllm/label_based/performance/issues.json",
  "issue": {
    "number": 9474,
    "title": "[Performance]: VLLM \u8bf7\u6c42\u6570\u91cf\u8fc7\u591a\u65f6\u592a\u6162",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\n\n### How would you like to use vllm\n\n\u6211\u6b63\u5728\u4f7f\u7528\u4e00\u5f20A100 \u90e8\u7f72\u768472B\u91cf\u5316\u6a21\u578b \u8fd9\u662f\u542f\u52a8\u811a\u672c \r\npython -m vllm.entrypoints.openai.api_server --host 0.0.0.0  --max-model-len 9000 --served-model-name chat-yzq --model /workspace/chat-v1-Int4 --enforce-eager  --tensor-parallel-size 1 --gpu-memory-utilization 0.85\r\n\r\n\u5f531\u5929\u67091\u4e07\u6b21\u8bf7\u6c42\u65f6 \u56de\u590d\u4f1a\u53d8\u5f97\u975e\u5e38\u7f13\u6162 \u6709\u4ec0\u4e48\u529e\u6cd5\u5417\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "performance",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-10-17T20:29:57+00:00",
    "closed_at": "2025-02-16T02:03:35+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9474/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9474"
  }
}