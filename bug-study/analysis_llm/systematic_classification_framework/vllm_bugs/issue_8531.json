{
  "issue_type": "bug",
  "extracted_from": "/root/yunwei37/vllm-exp/bug-study/analysis_llm/sample_results/vllm/label_based/bug/issues.json",
  "issue": {
    "number": 8531,
    "title": "[Bug]: benchmark_serving.py generates different numbers of tokens at different runs",
    "body": "### Your current environment\n\n4xH100.\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nWhen benchmarking the performance of vllm with `benchmark_serving.py`, it will generate different number of tokens at different runs.\r\n\r\nCode to launch vllm server\r\n```\r\nvllm serve meta-llama/Meta-Llama-3.1-70B-Instruct \\\r\n    --disable-log-requests \\\r\n    --tensor-parallel-size 4\r\n```\r\n\r\nCode to run the benchmark\r\n```\r\npython benchmarks/benchmark_serving.py \\\r\n    --backend vllm \\\r\n    --model meta-llama/Meta-Llama-3.1-70B-Instruct\\\r\n    --dataset-name sharegpt \\\r\n    --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json \\\r\n    --request-rate 1 \\\r\n    --num-prompts 200 \\\r\n    --save-result\r\n```\r\n\r\nIf I run the benchmark_serving.py script twice, the number of generated tokens is different for the two runs.\r\nThe output of the first run:\r\n```\r\n============ Serving Benchmark Result ============                                                                                                                                                                     \r\nSuccessful requests:                     200                                                                                                                                                                           \r\nBenchmark duration (s):                  203.41                                                                                                                                                                        \r\nTotal input tokens:                      42659                                                                                                                                                                         \r\nTotal generated tokens:                  **38614**                                                                                                                                                                         \r\nRequest throughput (req/s):              0.98                                                                                                                                                                          \r\nOutput token throughput (tok/s):         189.84                                                                                                                                                                        \r\nTotal Token throughput (tok/s):          399.56                                                                                                                                                                        \r\n---------------Time to First Token----------------                                                                                                                                                                     \r\nMean TTFT (ms):                          62.95                                                                                                                                                                         \r\nMedian TTFT (ms):                        64.68                                                                                                                                                                         \r\nP99 TTFT (ms):                           141.49                                                                                                                                                                        \r\n-----Time per Output Token (excl. 1st token)------                                                                                                                                                                     \r\nMean TPOT (ms):                          20.10                                                                                                                                                                         \r\nMedian TPOT (ms):                        19.93                                                                                                                                                                         \r\nP99 TPOT (ms):                           24.28                                                                                                                                                                         \r\n---------------Inter-token Latency----------------                                                                                                                                                                     \r\nMean ITL (ms):                           19.98                                                                                                                                                                         \r\nMedian ITL (ms):                         19.60                                                                                                                                                                         \r\nP99 ITL (ms):                            44.31                                                                                                                                                                         \r\n================================================== \r\n```\r\nTotal generated tokens:                  **38614**         \r\nThe output of the second run\r\n```\r\n============ Serving Benchmark Result ============                                                                                                                                                              [3/452]\r\nSuccessful requests:                     200                                                                                                                                                                           \r\nBenchmark duration (s):                  203.40                                                                                                                                                                        \r\nTotal input tokens:                      42659                                                                                                                                                                         \r\nTotal generated tokens:                  **38536**     \r\nRequest throughput (req/s):              0.98      \r\nOutput token throughput (tok/s):         189.46    \r\nTotal Token throughput (tok/s):          399.20    \r\n---------------Time to First Token----------------\r\nMean TTFT (ms):                          60.23     \r\nMedian TTFT (ms):                        64.19     \r\nP99 TTFT (ms):                           127.43    \r\n-----Time per Output Token (excl. 1st token)------\r\nMean TPOT (ms):                          20.01     \r\nMedian TPOT (ms):                        19.87     \r\nP99 TPOT (ms):                           22.67     \r\n---------------Inter-token Latency----------------\r\nMean ITL (ms):                           19.93     \r\nMedian ITL (ms):                         19.57     \r\nP99 ITL (ms):                            43.91     \r\n==================================================\r\n```\r\nTotal generated tokens:                  **38536**  .\r\nEven if I relaunch the server for the second run, the randomness still exists.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-17T06:09:27+00:00",
    "closed_at": "2025-03-01T02:05:45+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8531/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/8531"
  }
}