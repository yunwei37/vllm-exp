{
  "issue_type": "performance",
  "extracted_from": "/root/yunwei37/vllm-exp/bug-study/analysis_llm/sample_results/vllm/label_based/performance/issues.json",
  "issue": {
    "number": 1562,
    "title": "[FEATURE]\u00a0Implement Dynamic SplitFuse",
    "body": "Dear vLLM maintainers @WoosukKwon and @zhuohan123 (@Yard1),\r\n\r\nDeepSpeed has released its serving framework which claims to be faster than vLLM. The main speedup comes from [Dynamic SplitFuse](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen#b-dynamic-splitfuse-) which is a technique that does the following:\r\n\r\n- Long prompts are decomposed into much smaller chunks and scheduled across multiple forward passes (iterations) with only the final pass performing any generation.\r\n\r\n- Short prompts will be composed to exactly fill a target token budget. Even short prompts may be decomposed to ensure the budget is precisely met and the forward sizes are well-aligned.\r\n\r\nCode: https://github.com/microsoft/DeepSpeed-MII\r\nBackground: https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen\r\n\r\nLlama 13B (1x A100-80GB):\r\n![image](https://github.com/vllm-project/vllm/assets/27340033/cc7842b8-e234-482d-8550-d38d39d94473)\r\n\r\nLlama 70B (4x A100x80GB with TP):\r\n![image](https://github.com/vllm-project/vllm/assets/27340033/e035e094-0f10-463c-abf0-aafd67a61fed)\r\n",
    "labels": [
      "performance",
      "feature request"
    ],
    "state": "closed",
    "created_at": "2023-11-04T14:06:52+00:00",
    "closed_at": "2024-07-26T10:25:27+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1562/reactions",
      "total_count": 19,
      "+1": 19,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/1562"
  }
}