{
  "issue_type": "performance",
  "extracted_from": "/root/yunwei37/vllm-exp/bug-study/analysis_llm/sample_results/vllm/label_based/performance/issues.json",
  "issue": {
    "number": 8866,
    "title": "[Performance]: Slowdown compared to Gradio",
    "body": "### Proposal to improve performance\r\n\r\nvLLM is amazingly fast\r\nHowever, when running below prompt, with meta-llama/Meta-Llama-3-8B-Instruct, Gradio takes ~4sec per prompt (one by one) while vLLM takes ~12sec by def. When setting --quantization fp8 times reduced to ~8s\r\nOverall vLLM is much faster since it allows to process in parallel while Gradio doesn't\r\nTested with AWS L4, Gradio 4.43.0\r\nWhat am I missing?\r\n\r\n`prompt = \"\"\"You are a knowledgeable, efficient, and direct Al assistant. Provide concise answers up to 100 words`\r\n`without explainations or extra notes, focusing on the key information needed. Answer in question: answer JSON format`\r\n`**User:**I like the color red. Our website is www.nba.com. My age is 18.`\r\n`**Assistant:**Great. Write 3 things for me to answer.`\r\n`**User:**What is our website? What is my age? What kind of drink do I like to drink?`\r\n`**Assistant**:`\r\n`\"\"\"`\r\n\r\n### Report of performance regression\r\n\r\n_No response_\r\n\r\n### Misc discussion on performance\r\n\r\n_No response_\r\n\r\n### Your current environment (if you think it is necessary)\r\n\r\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "performance",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-26T20:35:42+00:00",
    "closed_at": "2025-01-26T01:59:49+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8866/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8866"
  }
}